{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIP installs\n",
    "!pip install -qU langchain_openai langchain_huggingface \n",
    "!pip install -qU langchain_core==0.2.40 langchain langchain_community langchain-text-splitters\n",
    "!pip install -qU qdrant_client pymupdf tiktoken ragas pandas\n",
    "!pip install -qU python-pptx==1.0.2 nltk==3.9.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain the data from the PDF documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "347"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, tiktoken\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Path to my directory containing PDF files\n",
    "directory = \"References/\"\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "# List to store all the documents\n",
    "all_docs = []\n",
    "\n",
    "# Iterate through all the files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".pdf\"):  # Check if the file is a PDF\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        loader = PyMuPDFLoader(file_path)\n",
    "        docs = loader.load()\n",
    "        all_docs.extend(docs)  # Append the loaded docs to my list\n",
    "\n",
    "\n",
    "one_document = \"\"\n",
    "for doc in all_docs:\n",
    "    one_document += doc.page_content\n",
    "\n",
    "def tiktoken_len(text):\n",
    "    tokens = tiktoken.encoding_for_model(\"gpt-4o\").encode(\n",
    "        text,\n",
    "    )\n",
    "    return len(tokens)\n",
    "\n",
    "## I am splitting with different chunk and overlaps than are used in my app\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 300,\n",
    "    chunk_overlap = 50,\n",
    "    length_function = tiktoken_len,\n",
    ")\n",
    "\n",
    "split_chunks = text_splitter.split_text(one_document)\n",
    "training_documents = text_splitter.create_documents(split_chunks)\n",
    "len(training_documents)\n",
    "\n",
    "# training_vectorstore = Qdrant.from_documents(\n",
    "#     text_splitter.create_documents(split_chunks),\n",
    "#     embedding_model,\n",
    "#     location=\":memory:\",\n",
    "#     collection_name=\"training_collection\",\n",
    "# )\n",
    "\n",
    "# training_retriever = training_vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "id_set = set()\n",
    "\n",
    "for document in training_documents:\n",
    "  id = str(uuid.uuid4())\n",
    "  while id in id_set:\n",
    "    id = uuid.uuid4()\n",
    "  id_set.add(id)\n",
    "  document.metadata[\"id\"] = id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n",
      "50\n",
      "47\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(metadata={'id': '6e590e51-727a-442b-b43b-24e90e0f935c'}, page_content='arXiv. https://arxiv.org/pdf/2309.01219 \\nZhao, X. et al. (2023) Provable Robust Watermarking for AI-Generated Text. Semantic Scholar. \\nhttps://www.semanticscholar.org/paper/Provable-Robust-Watermarking-for-AI-Generated-Text-Zhao-\\nAnanth/75b68d0903af9d9f6e47ce3cf7e1a7d27ec811dc')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_split_documents = training_documents[:250]\n",
    "val_split_documents = training_documents[250:300]\n",
    "test_split_documents = training_documents[300:347]\n",
    "print(len(training_split_documents))\n",
    "print(len(val_split_documents))\n",
    "print(len(test_split_documents))\n",
    "test_split_documents[46]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now build a fine tuning data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "qa_chat_model = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "qa_prompt = \"\"\"\\\n",
    "Given the following context, you must generate questions based on only the provided context.\n",
    "\n",
    "You are to generate {n_questions} questions which should be provided in the following format:\n",
    "\n",
    "1. QUESTION #1\n",
    "2. QUESTION #2\n",
    "...\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "qa_prompt_template = ChatPromptTemplate.from_template(qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_generation_chain = qa_prompt_template | qa_chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "def create_questions(documents, n_questions):\n",
    "  questions = {}\n",
    "  relevant_docs = {}\n",
    "\n",
    "  ### YOUR CODE HERE\n",
    "  for document in tqdm.tqdm(documents):\n",
    "    document_content = {\"context\": document.page_content, \"questions\": []}\n",
    "    questions_generated = question_generation_chain.invoke({\"context\": document_content[\"context\"], \"n_questions\": n_questions})\n",
    "    for question in questions_generated.content.split(\"\\n\"):\n",
    "      question_id = str(uuid.uuid4())\n",
    "      questions[question_id] = \"\".join(question.split(\".\")[1:]).strip()\n",
    "      relevant_docs[question_id] = [document.metadata[\"id\"]]\n",
    "  return questions, relevant_docs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  return questions, relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [04:18<00:00,  1.03s/it]\n",
      "100%|██████████| 50/50 [00:51<00:00,  1.04s/it]\n",
      "100%|██████████| 47/47 [00:52<00:00,  1.12s/it]\n"
     ]
    }
   ],
   "source": [
    "## generate he data with n_questions = 2 for each\n",
    "training_questions, training_relevant_contexts = create_questions(training_split_documents,2)\n",
    "val_questions, val_relevant_contexts = create_questions(val_split_documents, 2) \n",
    "test_questions, test_relevant_contexts = create_questions(test_split_documents, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the datasets so I don't have to recreate them when I move into COLAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "training_corpus = {train_item.metadata[\"id\"] : train_item.page_content for train_item in training_split_documents}\n",
    "\n",
    "train_dataset = {\n",
    "    \"questions\" : training_questions,\n",
    "    \"relevant_contexts\" : training_relevant_contexts,\n",
    "    \"corpus\" : training_corpus\n",
    "}\n",
    "\n",
    "with open(\"training_dataset.jsonl\", \"w\") as f:\n",
    "  json.dump(train_dataset, f)\n",
    "\n",
    "val_corpus = {val_item.metadata[\"id\"] : val_item.page_content for val_item in val_split_documents}\n",
    "\n",
    "val_dataset = {\n",
    "    \"questions\" : val_questions,\n",
    "    \"relevant_contexts\" : val_relevant_contexts,\n",
    "    \"corpus\" : val_corpus\n",
    "}\n",
    "\n",
    "with open(\"val_dataset.jsonl\", \"w\") as f:\n",
    "  json.dump(val_dataset, f)\n",
    "\n",
    "train_corpus = {test_item.metadata[\"id\"] : test_item.page_content for test_item in test_split_documents}\n",
    "\n",
    "test_dataset = {\n",
    "    \"questions\" : test_questions,\n",
    "    \"relevant_contexts\" : test_relevant_contexts,\n",
    "    \"corpus\" : train_corpus\n",
    "}\n",
    "\n",
    "with open(\"test_dataset.jsonl\", \"w\") as f:\n",
    "  json.dump(test_dataset, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we will fine tune snowflake (what else)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU sentence_transformers datasets pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jmichaeldean/anaconda3/envs/AIE4/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f903aee3d454adc952204c390019690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eef7f0db91974f278ce2ea0a19a2cb9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/252 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "005ba3bce5b14bf9a1ed9c878450dd26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/84.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16445880cefd4baeb301430300af8634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/107 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38c26cecb8f942dca6da3be252bf2d2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/738 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc85d28497084888a5b57a1e7cccc137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f332c4993cb4179b9f076688c78db46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37e7492439c541e2989cc888d90a4fa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e266470a6124b12990dbd6212d42223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58a8edeca55a40b3a80d2699868778bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fae99643b4204540a2ca559d9eff2264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_id = \"Snowflake/snowflake-arctic-embed-m\"\n",
    "model = SentenceTransformer(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from sentence_transformers import InputExample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20\n",
    "\n",
    "corpus = train_dataset['corpus']\n",
    "queries = train_dataset['questions']\n",
    "relevant_docs = train_dataset['relevant_contexts']\n",
    "\n",
    "examples = []\n",
    "for query_id, query in queries.items():\n",
    "    doc_id = relevant_docs[query_id][0]\n",
    "    text = corpus[doc_id]\n",
    "    example = InputExample(texts=[query, text])\n",
    "    examples.append(example)\n",
    "\n",
    "loader = DataLoader(\n",
    "    examples, batch_size=BATCH_SIZE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.losses import MatryoshkaLoss, MultipleNegativesRankingLoss\n",
    "\n",
    "matryoshka_dimensions = [768, 512, 256, 128, 64]\n",
    "inner_train_loss = MultipleNegativesRankingLoss(model)\n",
    "train_loss = MatryoshkaLoss(\n",
    "    model, inner_train_loss, matryoshka_dims=matryoshka_dimensions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
    "\n",
    "corpus = val_dataset['corpus']\n",
    "queries = val_dataset['questions']\n",
    "relevant_docs = val_dataset['relevant_contexts']\n",
    "\n",
    "evaluator = InformationRetrievalEvaluator(queries, corpus, relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.21.0`: Please run `pip install transformers[torch]` or `pip install accelerate -U`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m warmup_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(loader) \u001b[38;5;241m*\u001b[39m EPOCHS \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m      4\u001b[0m     train_objectives\u001b[38;5;241m=\u001b[39m[(loader, train_loss)],\n\u001b[1;32m      5\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mEPOCHS,\n\u001b[1;32m      6\u001b[0m     warmup_steps\u001b[38;5;241m=\u001b[39mwarmup_steps,\n\u001b[1;32m      7\u001b[0m     output_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinetuned_arctic\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m     show_progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      9\u001b[0m     evaluator\u001b[38;5;241m=\u001b[39mevaluator,\n\u001b[1;32m     10\u001b[0m     evaluation_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m     11\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/AIE4/lib/python3.11/site-packages/sentence_transformers/fit_mixin.py:307\u001b[0m, in \u001b[0;36mFitMixin.fit\u001b[0;34m(self, train_objectives, evaluator, epochs, steps_per_epoch, scheduler, warmup_steps, optimizer_class, optimizer_params, weight_decay, evaluation_steps, output_path, save_best_model, max_grad_norm, use_amp, callback, show_progress_bar, checkpoint_path, checkpoint_save_steps, checkpoint_save_total_limit)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;66;03m# Transformers renamed `evaluation_strategy` to `eval_strategy` in v4.41.0\u001b[39;00m\n\u001b[1;32m    302\u001b[0m eval_strategy_key \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_strategy\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(transformers\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4.41.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation_strategy\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m )\n\u001b[0;32m--> 307\u001b[0m args \u001b[38;5;241m=\u001b[39m SentenceTransformerTrainingArguments(\n\u001b[1;32m    308\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39mcheckpoint_path \u001b[38;5;129;01mor\u001b[39;00m _default_checkpoint_dir(),\n\u001b[1;32m    309\u001b[0m     batch_sampler\u001b[38;5;241m=\u001b[39mbatch_sampler,\n\u001b[1;32m    310\u001b[0m     multi_dataset_batch_sampler\u001b[38;5;241m=\u001b[39mMultiDatasetBatchSamplers\u001b[38;5;241m.\u001b[39mROUND_ROBIN,\n\u001b[1;32m    311\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m    312\u001b[0m     per_device_eval_batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m    313\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39mepochs,\n\u001b[1;32m    314\u001b[0m     max_steps\u001b[38;5;241m=\u001b[39mmax_steps,\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\n\u001b[1;32m    316\u001b[0m         eval_strategy_key: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m evaluation_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m evaluation_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    317\u001b[0m     },\n\u001b[1;32m    318\u001b[0m     eval_steps\u001b[38;5;241m=\u001b[39mevaluation_steps,\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;66;03m# load_best_model_at_end=save_best_model, # <- TODO: Look into a good solution for save_best_model\u001b[39;00m\n\u001b[1;32m    320\u001b[0m     max_grad_norm\u001b[38;5;241m=\u001b[39mmax_grad_norm,\n\u001b[1;32m    321\u001b[0m     fp16\u001b[38;5;241m=\u001b[39muse_amp,\n\u001b[1;32m    322\u001b[0m     disable_tqdm\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m show_progress_bar,\n\u001b[1;32m    323\u001b[0m     save_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m checkpoint_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    324\u001b[0m     save_steps\u001b[38;5;241m=\u001b[39mcheckpoint_save_steps,\n\u001b[1;32m    325\u001b[0m     save_total_limit\u001b[38;5;241m=\u001b[39mcheckpoint_save_total_limit,\n\u001b[1;32m    326\u001b[0m )\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m steps_per_epoch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m steps_per_epoch \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    329\u001b[0m     steps_per_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m([\u001b[38;5;28mlen\u001b[39m(train_dataset) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m batch_size \u001b[38;5;28;01mfor\u001b[39;00m train_dataset \u001b[38;5;129;01min\u001b[39;00m train_dataset_dict\u001b[38;5;241m.\u001b[39mvalues()])\n",
      "File \u001b[0;32m<string>:133\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, eval_do_concat_batches, fp16_backend, evaluation_strategy, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, split_batches, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, eval_use_gather_object, batch_sampler, multi_dataset_batch_sampler)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/AIE4/lib/python3.11/site-packages/sentence_transformers/training_args.py:168\u001b[0m, in \u001b[0;36mSentenceTransformerTrainingArguments.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__post_init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 168\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m__post_init__()\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_sampler \u001b[38;5;241m=\u001b[39m BatchSamplers(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_sampler)\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_dataset_batch_sampler \u001b[38;5;241m=\u001b[39m MultiDatasetBatchSamplers(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_dataset_batch_sampler)\n",
      "File \u001b[0;32m~/anaconda3/envs/AIE4/lib/python3.11/site-packages/transformers/training_args.py:1730\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1728\u001b[0m \u001b[38;5;66;03m# Initialize device before we proceed\u001b[39;00m\n\u001b[1;32m   1729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_torch_available():\n\u001b[0;32m-> 1730\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m   1732\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtorchdynamo \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1733\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1734\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torchdynamo` is deprecated and will be removed in version 5 of 🤗 Transformers. Use\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1735\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `torch_compile_backend` instead\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m   1737\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/AIE4/lib/python3.11/site-packages/transformers/training_args.py:2227\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2223\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2224\u001b[0m \u001b[38;5;124;03mThe device used by this process.\u001b[39;00m\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2226\u001b[0m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m-> 2227\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_devices\n",
      "File \u001b[0;32m~/anaconda3/envs/AIE4/lib/python3.11/site-packages/transformers/utils/generic.py:60\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, obj, objtype)\u001b[0m\n\u001b[1;32m     58\u001b[0m cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, attr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m     cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfget(obj)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(obj, attr, cached)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cached\n",
      "File \u001b[0;32m~/anaconda3/envs/AIE4/lib/python3.11/site-packages/transformers/training_args.py:2103\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[1;32m   2102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[0;32m-> 2103\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m   2104\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2105\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease run `pip install transformers[torch]` or `pip install accelerate -U`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2106\u001b[0m         )\n\u001b[1;32m   2107\u001b[0m \u001b[38;5;66;03m# We delay the init of `PartialState` to the end for clarity\u001b[39;00m\n\u001b[1;32m   2108\u001b[0m accelerator_state_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menabled\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_configured_state\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n",
      "\u001b[0;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.21.0`: Please run `pip install transformers[torch]` or `pip install accelerate -U`"
     ]
    }
   ],
   "source": [
    "warmup_steps = int(len(loader) * EPOCHS * 0.1)\n",
    "\n",
    "model.fit(\n",
    "    train_objectives=[(loader, train_loss)],\n",
    "    epochs=EPOCHS,\n",
    "    warmup_steps=warmup_steps,\n",
    "    output_path='finetuned_arctic',\n",
    "    show_progress_bar=True,\n",
    "    evaluator=evaluator,\n",
    "    evaluation_steps=50,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIE4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

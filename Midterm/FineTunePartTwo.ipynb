{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I made this notebook to start in the middle because I don't want to keep regenerating the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no such file or directory: 0.1.0\n"
     ]
    }
   ],
   "source": [
    "# PIP installs\n",
    "!pip install -q langchain_openai langchain_huggingface<0.1.0 \n",
    "!pip install -q langchain_core==0.2.40 langchain==0.2.4 langchain_community langchain-text-splitters==0.2.4\n",
    "!pip install -q qdrant_client pymupdf tiktoken ragas pandas\n",
    "!pip install -q python-pptx==1.0.2 nltk==3.9.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter Your OpenAI API Key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the data sets that I already created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read embedding model datasets that have already been created and stored\n",
    "import readDataSets\n",
    "test_dataset, train_dataset, val_dataset = readDataSets.loadDataSets()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jmichaeldean/anaconda3/envs/AIE4a/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_id = \"Snowflake/snowflake-arctic-embed-m\"\n",
    "model = SentenceTransformer(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from sentence_transformers import InputExample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = train_dataset['corpus']\n",
    "queries = train_dataset['questions']\n",
    "relevant_docs = train_dataset['relevant_contexts']\n",
    "\n",
    "examples = []\n",
    "for query_id, query in queries.items():\n",
    "    doc_id = relevant_docs[query_id][0]\n",
    "    text = corpus[doc_id]\n",
    "    example = InputExample(texts=[query, text])\n",
    "    examples.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20\n",
    "loader = DataLoader(\n",
    "    examples, batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.losses import MatryoshkaLoss, MultipleNegativesRankingLoss\n",
    "\n",
    "matryoshka_dimensions = [768, 512, 256, 128, 64]\n",
    "inner_train_loss = MultipleNegativesRankingLoss(model)\n",
    "train_loss = MatryoshkaLoss(\n",
    "    model, inner_train_loss, matryoshka_dims=matryoshka_dimensions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
    "\n",
    "corpus = val_dataset['corpus']\n",
    "queries = val_dataset['questions']\n",
    "relevant_docs = val_dataset['relevant_contexts']\n",
    "\n",
    "evaluator = InformationRetrievalEvaluator(queries, corpus, relevant_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01879bd762b549e7a7a85217936afbeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb4a9b554bec4049a31216461ae2f45d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_cosine_accuracy@1': 0.74, 'eval_cosine_accuracy@3': 0.87, 'eval_cosine_accuracy@5': 0.96, 'eval_cosine_accuracy@10': 0.98, 'eval_cosine_precision@1': 0.74, 'eval_cosine_precision@3': 0.29, 'eval_cosine_precision@5': 0.19199999999999995, 'eval_cosine_precision@10': 0.09799999999999998, 'eval_cosine_recall@1': 0.74, 'eval_cosine_recall@3': 0.87, 'eval_cosine_recall@5': 0.96, 'eval_cosine_recall@10': 0.98, 'eval_cosine_ndcg@10': 0.8615543379824142, 'eval_cosine_mrr@10': 0.8233888888888888, 'eval_cosine_map@100': 0.8249365079365079, 'eval_dot_accuracy@1': 0.74, 'eval_dot_accuracy@3': 0.87, 'eval_dot_accuracy@5': 0.96, 'eval_dot_accuracy@10': 0.98, 'eval_dot_precision@1': 0.74, 'eval_dot_precision@3': 0.29, 'eval_dot_precision@5': 0.19199999999999995, 'eval_dot_precision@10': 0.09799999999999998, 'eval_dot_recall@1': 0.74, 'eval_dot_recall@3': 0.87, 'eval_dot_recall@5': 0.96, 'eval_dot_recall@10': 0.98, 'eval_dot_ndcg@10': 0.8615543379824142, 'eval_dot_mrr@10': 0.8233888888888888, 'eval_dot_map@100': 0.8249365079365079, 'eval_runtime': 0.9122, 'eval_samples_per_second': 0.0, 'eval_steps_per_second': 0.0, 'epoch': 1.92}\n",
      "{'eval_cosine_accuracy@1': 0.75, 'eval_cosine_accuracy@3': 0.9, 'eval_cosine_accuracy@5': 0.96, 'eval_cosine_accuracy@10': 0.97, 'eval_cosine_precision@1': 0.75, 'eval_cosine_precision@3': 0.3, 'eval_cosine_precision@5': 0.19199999999999995, 'eval_cosine_precision@10': 0.09699999999999998, 'eval_cosine_recall@1': 0.75, 'eval_cosine_recall@3': 0.9, 'eval_cosine_recall@5': 0.96, 'eval_cosine_recall@10': 0.97, 'eval_cosine_ndcg@10': 0.8673712763276756, 'eval_cosine_mrr@10': 0.8336111111111113, 'eval_cosine_map@100': 0.8360959595959596, 'eval_dot_accuracy@1': 0.75, 'eval_dot_accuracy@3': 0.9, 'eval_dot_accuracy@5': 0.96, 'eval_dot_accuracy@10': 0.97, 'eval_dot_precision@1': 0.75, 'eval_dot_precision@3': 0.3, 'eval_dot_precision@5': 0.19199999999999995, 'eval_dot_precision@10': 0.09699999999999998, 'eval_dot_recall@1': 0.75, 'eval_dot_recall@3': 0.9, 'eval_dot_recall@5': 0.96, 'eval_dot_recall@10': 0.97, 'eval_dot_ndcg@10': 0.8673712763276756, 'eval_dot_mrr@10': 0.8336111111111113, 'eval_dot_map@100': 0.8360959595959596, 'eval_runtime': 0.9238, 'eval_samples_per_second': 0.0, 'eval_steps_per_second': 0.0, 'epoch': 3.85}\n",
      "{'train_runtime': 323.4273, 'train_samples_per_second': 7.761, 'train_steps_per_second': 0.402, 'train_loss': 1.6381398127629208, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "warmup_steps = int(len(loader) * EPOCHS * 0.1)\n",
    "\n",
    "model.fit(\n",
    "    train_objectives=[(loader, train_loss)],\n",
    "    epochs=EPOCHS,\n",
    "    warmup_steps=warmup_steps,\n",
    "    output_path='finetuned_arctic',\n",
    "    show_progress_bar=True,\n",
    "    evaluator=evaluator,\n",
    "    evaluation_steps=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "# from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "def evaluate_openai(\n",
    "    dataset,\n",
    "    embed_model,\n",
    "    top_k=5,\n",
    "    verbose=False,\n",
    "):\n",
    "  corpus = dataset['corpus']\n",
    "  questions = dataset['questions']\n",
    "  relevant_docs = dataset['relevant_contexts']\n",
    "  documents = [Document(page_content=content, metadata={\"id\": doc_id}) for doc_id, content in corpus.items()]\n",
    "  vectorstore = FAISS.from_documents(documents, embed_model)\n",
    "\n",
    "  retriever = vectorstore.as_retriever(search_kwargs={\"k\": top_k})\n",
    "\n",
    "  eval_results = []\n",
    "  for id, question in tqdm.tqdm(questions.items()):\n",
    "    retrieved_nodes = retriever.invoke(question)\n",
    "    retrieved_ids = [node.metadata[\"id\"] for node in retrieved_nodes]\n",
    "    expected_id = relevant_docs[id][0]\n",
    "    is_hit = expected_id in retrieved_ids\n",
    "    eval_results.append({\"id\": id, \"question\": question, \"expected_id\": expected_id, \"is_hit\": is_hit})\n",
    "\n",
    "  return eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate default embedder (text-embedding-3-small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import defaults\n",
    "te3_openai = defaults.default_embedding_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.8.0.post1-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.0 in /Users/jmichaeldean/anaconda3/envs/AIE4a/lib/python3.11/site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /Users/jmichaeldean/anaconda3/envs/AIE4a/lib/python3.11/site-packages (from faiss-cpu) (24.1)\n",
      "Downloading faiss_cpu-1.8.0.post1-cp311-cp311-macosx_11_0_arm64.whl (6.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.8.0.post1\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/94 [00:00<00:15,  5.98it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 94/94 [00:17<00:00,  5.50it/s]\n"
     ]
    }
   ],
   "source": [
    "te3_results = evaluate_openai(test_dataset, te3_openai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9361702127659575"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te3_results_df = pd.DataFrame(te3_results)\n",
    "te3_hit_rate = te3_results_df[\"is_hit\"].mean()\n",
    "te3_hit_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate base snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zx/1kbq8dj123q1cq4b0tqqc64r0000gn/T/ipykernel_20107/461913052.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  huggingface_embeddings = HuggingFaceEmbeddings(model_name=\"Snowflake/snowflake-arctic-embed-m\")\n",
      "100%|██████████| 94/94 [00:06<00:00, 15.02it/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "huggingface_embeddings = HuggingFaceEmbeddings(model_name=\"Snowflake/snowflake-arctic-embed-m\")\n",
    "arctic_embed_m_results = evaluate_openai(test_dataset, huggingface_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6702127659574468"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arctic_embed_m_results_df = pd.DataFrame(arctic_embed_m_results)\n",
    "arctic_embed_m_hit_rate = arctic_embed_m_results_df[\"is_hit\"].mean()\n",
    "arctic_embed_m_hit_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate fine tuned snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at finetuned_arctic and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 94/94 [00:02<00:00, 33.12it/s]\n"
     ]
    }
   ],
   "source": [
    "finetune_embeddings = HuggingFaceEmbeddings(model_name=\"finetuned_arctic\")\n",
    "finetune_results = evaluate_openai(test_dataset, finetune_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9893617021276596"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetune_results_df = pd.DataFrame(finetune_results)\n",
    "finetune_hit_rate = finetune_results_df[\"is_hit\"].mean()\n",
    "finetune_hit_rate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIE4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

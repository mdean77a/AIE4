{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Midterm Challenge Notebook - Mike Dean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain_openai langchain_huggingface<0.1.0 \n",
    "!pip install -q langchain_core==0.2.40 langchain==0.2.4 langchain_community langchain-text-splitters==0.2.4\n",
    "!pip install -q qdrant_client pymupdf tiktoken ragas pandas\n",
    "!pip install -q python-pptx==1.0.2 nltk==3.9.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU langchain langchain_openai langchain_core==0.2.40 langchain_community\n",
    "!pip install -qU qdrant_client pymupdf tiktoken ragas pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.  Dealing with the Data\n",
    "(Role: AI Solutions Engineer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import defaults\n",
    "llm = defaults.default_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method count of list object at 0x1143159c0>\n"
     ]
    }
   ],
   "source": [
    "# Load PDF documents from a directory\n",
    "import loadReferenceDocuments\n",
    "separate_pages, one_document = loadReferenceDocuments.loadReferenceDocuments(\"References/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking Strategies\n",
    "#### Ingest the PDF by page - page_split\n",
    "#### Ingest the PDF by page and recombine into single file and then chunk - chunk_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import splitAndVectorize\n",
    "\n",
    "page_split_vectorstore = splitAndVectorize.createVectorstore(\n",
    "    separate_pages,\n",
    "    \"separate_page_collection\",\n",
    ")\n",
    "\n",
    "page_split_retriever = page_split_vectorstore.as_retriever()\n",
    "\n",
    "chunk_split_vectorstore = splitAndVectorize.createVectorstore(\n",
    "    one_document,\n",
    "    \"chunk_split_collection\",\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=400,\n",
    ")\n",
    "\n",
    "chunk_split_retriever = chunk_split_vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 Deliverables:\n",
    "1.  Describe the default chunking strategy that I will use:<br>\n",
    "The default strategy will be to load the two PDF files using `PyMuPDFLoader` just as we have previously done.  This results in each PDF page being its own document.  I have checked sample pages with `tiktoken` and the token count per page is <1000, so these are small enough to just embed without further splitting. I saved these embeddings in `page_split_vectorstore`.\n",
    "\n",
    "2.  Articulate a chunking strategy that I will also test:<br>\n",
    "The disadvantage of the default strategy is that there is no chunk overlapping between the pages, and this might worsen the ability connect two pages that are both relevant to a query.  So I  recombine the page_content of all pages into a single string, convert it into a document, and split it with a chunk size of 800 and an overlap of 400 (the default settings used by OpenAI).  This strategy allows chunks to overlap, pehaps adding semantic continuity between adjacent pages.  These were embedded with the same embedding model and saved in `chunk_split_vectorstore`.\n",
    "\n",
    "3.  Describe how and why I made these decisions:<br>\n",
    "The default behavior of `PyMuPDFLoader` is not bad and I have been using it for several months.  However, I was splitting each of the documents created, not thinking through that if I had chunk sizes greater than the page itself, this was meaningless.  I also had chunk overlap, but had not thought through the implications of each page being a separate document.  So I made these decision for this Midterm Challenge so I can later compare the performance using RAGAS in Task 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.  Building a Quick End-to-End Prototype\n",
    "(Role: AI Systems Engineer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prompts\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from operator import itemgetter\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(prompts.rag_prompt_template)\n",
    "\n",
    "page_split_rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | page_split_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | rag_prompt | llm | StrOutputParser()\n",
    ")\n",
    "\n",
    "chunk_split_rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | chunk_split_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | rag_prompt | llm | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The ten major risks of AI, as identified, are:\n",
       "\n",
       "1. **Confabulation**: AI generating false or misleading information.\n",
       "2. **Dangerous or Violent Recommendations**: AI suggesting harmful actions.\n",
       "3. **Data Privacy**: Risks related to unauthorized access and misuse of personal data.\n",
       "4. **Value Chain and Component Integration**: Issues arising from integrating various AI components.\n",
       "5. **Harmful Bias**: AI perpetuating or amplifying biases present in training data.\n",
       "6. **Homogenization**: The risk of creating uniformity that can lead to systemic vulnerabilities.\n",
       "7. **CBRN Information or Capabilities**: Misuse of AI for chemical, biological, radiological, or nuclear purposes.\n",
       "8. **Human-AI Configuration**: Risks from improper interaction between humans and AI systems.\n",
       "9. **Obscene, Degrading, and/or Abusive Content**: AI generating harmful or offensive content.\n",
       "10. **Information Integrity**: Ensuring the accuracy and reliability of information produced by AI systems.\n",
       "\n",
       "These risks can be categorized into technical/model risks, misuse by humans, and ecosystem/societal risks."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Some risks of AI include:\n",
       "\n",
       "1. **Technical / Model Risks**:\n",
       "   - Confabulation: The production of confidently stated but erroneous or false content.\n",
       "   - Dangerous or Violent Recommendations: AI models suggesting harmful actions.\n",
       "   - Data Privacy: Leakage and unauthorized use of sensitive data.\n",
       "   - Value Chain and Component Integration: Issues in integrating various components leading to risks.\n",
       "   - Harmful Bias and Homogenization: Amplification of biases and undesired homogeneity.\n",
       "\n",
       "2. **Misuse by Humans**:\n",
       "   - CBRN Information or Capabilities: Easier access to information related to chemical, biological, radiological, or nuclear weapons.\n",
       "   - Obscene, Degrading, and/or Abusive Content: Generation and distribution of harmful content.\n",
       "   - Information Integrity and Security: Issues with the accuracy and security of information.\n",
       "\n",
       "3. **Ecosystem / Societal Risks**:\n",
       "   - Environmental Impacts: High resource utilization affecting ecosystems.\n",
       "   - Intellectual Property: Risks to proprietary information and innovation.\n",
       "\n",
       "These risks can arise from the design, training, or operation of AI systems and can be exacerbated by human behavior."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "## TEST THE TWO CHAINS\n",
    "page_response = (page_split_rag_chain.invoke({\"question\": \"List the ten major risks of AI?\"}))\n",
    "display(Markdown(page_response))\n",
    "\n",
    "chunk_response = (chunk_split_rag_chain.invoke({\"question\": \"What are some risks of AI?\"}))\n",
    "display(Markdown(chunk_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 Deliverables:\n",
    "1.  Build a live public prototype on Hugging Face, and include the public URL link to my space.<br>\n",
    "\n",
    "Here is a one minute Loom video that demonstrates the prototype running in Hugging Face.\n",
    "https://www.loom.com/share/70b741d3e4e14af792572b3aa9106463?sid=5aeb2e51-0f75-4c74-9f8c-6bc6d14aaa52\n",
    "\n",
    "I used the page split retriever for this prototype, meaning that the documents were broken by page, not recombined, and were chunked as whole pages without overlap.\n",
    "\n",
    "2.  How did I choose my stack, and why did I select each tool the way it Did? <br>\n",
    "\n",
    "My stack consists of the following:\n",
    "- Hardware is Apple Mac Studio\n",
    "- Editor is VSC as recommended, and I have grown to like it very much  because it includes everything.\n",
    "- Qdrant is the vector store.  I have used FAISS and Chroma, but Qdrant is fantastic.  I run it locally as a server, though for this Hugging Face situation, I am using a memory-based implementation.\n",
    "- ChainLit is the interface, as recommended.  Notably, the current version of ChainLit does NOT work on Hugging Face, and the version needs to be locked (chainlit==0.7.700).\n",
    "- RAGAS is part of my stack for purposes of later evaluation.\n",
    "- LangChain is used to help simplify the code.\n",
    "\n",
    "I learned an important lesson about software engineering - notebooks are NOT good ways to organize!  They are great for teaching.  So I reverted to my last 30 years of software development, and started refactoring code OUT OF THE NOTEBOOK, and then calling it inside the notebook.  So you (or others) can use the notebook as a navigation tool, but it doesn't become so unwieldly that you can't figure anything out.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.  Creating a Golden Test Data Set\n",
    "(Role: AI Evaluation and Performance Engineer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "318"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import splitAndVectorize\n",
    "# create a new splitting without embedding\n",
    "\n",
    "eval_documents = splitAndVectorize.split_into_chunks(\n",
    "    one_document,\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "len(eval_documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following code was used to create tests but I have commented out to avoid repetition.\n",
    "## I created 50 pairs instead of 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "import defaults\n",
    "\n",
    "## llm and embedding models were set earlier\n",
    "# generator_llm = llm\n",
    "# critic_llm = llm\n",
    "# embeddings = defaults.default_embedding_model\n",
    "\n",
    "# generator = TestsetGenerator.from_langchain(\n",
    "#     generator_llm,\n",
    "#     critic_llm,\n",
    "#     embeddings\n",
    "# )\n",
    "\n",
    "# distributions = {\n",
    "#     simple: 0.5,\n",
    "#     multi_context: 0.4,\n",
    "#     reasoning: 0.1\n",
    "# }\n",
    "\n",
    "# num_qa_pairs = 50 # I increased this from 20 because I have a surplus of OpenAI credits\n",
    "\n",
    "## I ALREADY RAN THIS SO HAVE COMMENTED IT OUT HERE SO I DON'T DO IT AGAIN\n",
    "## I WILL READ IN THE CSV FILE TO CONTINUE\n",
    "# testset = generator.generate_with_langchain_docs(eval_documents, num_qa_pairs, distributions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the test data from the stored CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ IN THE TESTSET FROM THE CSV FILE\n",
    "import pandas as pd\n",
    "test_df = pd.read_csv(\"testset.csv\")\n",
    "test_questions = test_df[\"question\"].values.tolist()\n",
    "test_groundtruths = test_df[\"ground_truth\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "retrieval_augmented_qa_chain_chunk = (\n",
    "    {\"context\": itemgetter(\"question\") | chunk_split_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | llm, \"context\": itemgetter(\"context\")}\n",
    ")\n",
    "retrieval_augmented_qa_chain_paged = (\n",
    "    {\"context\": itemgetter(\"question\") | page_split_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | llm, \"context\": itemgetter(\"context\")}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a41328725bb485899f0cd0fc9db2452",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No statements were generated from the answer.\n"
     ]
    }
   ],
   "source": [
    "import evaluateRAGAS\n",
    "results = evaluateRAGAS.evaluateRAGAS(retrieval_augmented_qa_chain_chunk,\n",
    "                                                       test_questions, test_groundtruths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'faithfulness': 0.8999, 'answer_relevancy': 0.9475, 'context_recall': 0.9017, 'context_precision': 0.9083}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6660d66f59c145af808e09fab1bfbe66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No statements were generated from the answer.\n"
     ]
    }
   ],
   "source": [
    "import evaluateRAGAS\n",
    "paged_results = evaluateRAGAS.evaluateRAGAS(retrieval_augmented_qa_chain_paged,\n",
    "                                                       test_questions, test_groundtruths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'faithfulness': 0.9146, 'answer_relevancy': 0.9240, 'context_recall': 0.8700, 'context_precision': 0.9006}\n"
     ]
    }
   ],
   "source": [
    "print(paged_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>separate_pages</th>\n",
       "      <th>total_chunked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>faithfulness</td>\n",
       "      <td>0.914559</td>\n",
       "      <td>0.899927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>answer_relevancy</td>\n",
       "      <td>0.923959</td>\n",
       "      <td>0.947485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>context_recall</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.901667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>context_precision</td>\n",
       "      <td>0.900556</td>\n",
       "      <td>0.908333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Metric  separate_pages  total_chunked\n",
       "0       faithfulness        0.914559       0.899927\n",
       "1   answer_relevancy        0.923959       0.947485\n",
       "2     context_recall        0.870000       0.901667\n",
       "3  context_precision        0.900556       0.908333"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chunked = pd.DataFrame(list(results.items()), columns=['Metric', 'total_chunked'])\n",
    "df_paged = pd.DataFrame(list(paged_results.items()), columns=['Metric', 'separate_pages'])\n",
    "df_merged = pd.merge(df_paged, df_chunked, on='Metric')\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 Deliverables:\n",
    "1.  Assess my pipeline using the RAGAS framework including key metrics faithfulness, answer relevancy, context precision, and context recall.  Provide a table of my output results.<br>\n",
    "\n",
    "I did the evaluation of my prototype model which was based on the PDF documents being divided by pages, but while I was here, I compared this with the other strategy, which was to recombine all the text and then split by chunks with overlap.\n",
    "\n",
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Metric</th>\n",
    "      <th>separate_pages</th>\n",
    "      <th>total_chunked</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>faithfulness</td>\n",
    "      <td>0.914559</td>\n",
    "      <td>0.899927</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>answer_relevancy</td>\n",
    "      <td>0.923959</td>\n",
    "      <td>0.947485</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>context_recall</td>\n",
    "      <td>0.870000</td>\n",
    "      <td>0.901667</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>context_precision</td>\n",
    "      <td>0.900556</td>\n",
    "      <td>0.908333</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "When the PDF document is separated by page, and then those pages are INDIVIDUALLY chunked or embedded, the context recall is somewhat less, but other parameters are not really striking.  Both strategies need to be assessed later when we use the finetuned embedding model.\n",
    "\n",
    "2.  What conclusions can I draw about performance and effectiveness of my pipeline with this information? <br>\n",
    "\n",
    "- Faithfulness: Measures whether all claims or statements in the answer can be completely inferred from the context that was provided.  The value is the percentage of claims that can be inferred over the total number of claims.  \n",
    "- Answer relevancy: Measures whether the answer is relevant to the question. It does not matter if the answer is actually correct - only that it directly answers the question without redundancy. \n",
    "- Context recall: This measures whether the facts that are in the ground truth reference answer can be inferred from the context that was provided to the LLM.  In a perfect situation, every statement in the ground truth should be able to be linked to the context.  \n",
    "- Context precision: This measures whether all the elements in the ground truth are in the highest ranked parts of the context.  \n",
    "\n",
    "**OVERALL CONCLUSION:**<br> \n",
    "Not really any significant differences here, though I suspect that letting the pages be kept as separate documents is going to be inferior in the long run.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.  Fine-Tuning Open-Source Embeddings\n",
    "(Role: Machine Learning Engineer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I performed the fine tuning in a separate notebook (FineTunePartTwo.ipynb) that you can find in this location:\n",
    "https://github.com/mdean77a/AIE4/blob/main/Midterm/FineTunePartTwo.ipynb\n",
    "\n",
    "I did this separately because I anticipated needing to use Colab.  To my utter surprise, the training actually worked on my Mac Studio (M1 32 gb, 323 sec) and I could reproduce it on M3 laptop (128 gb, 106 sec).  So I didn't end up having to wrestle with Colab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 Deliverables:\n",
    "1.  Swap out my existing embedding model for the new fine tuned version.  Provide a link to m fine-tuned embedding model on the Hugging Face Hub.<br>\n",
    "\n",
    "https://huggingface.co/Mdean77/finetuned_arctic\n",
    "\n",
    "2.  How did I choose the embedding model for this application?<br>\n",
    "\n",
    "I selected Snowflake/snowflake-arctic-embed-m because it improved dramatically in our previous exercise with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5.  Assessing Performance\n",
    "(Role: AI Evaluation and Performance Engineer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jmichaeldean/anaconda3/envs/AIE4/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e6755d44251439f8d2b56e275db8738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d01a3a1c43e45b38dfff05babd4b885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/271 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "784725ecdef247d3a783e4673ada05d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/49.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4659ff1fa9054630ab817523dd62e76c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "817083151af64c46bd0b895b71c0b615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/657 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea16baad05f54dc1b1d79f7ca828f279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d7ea1ebce6f4c31a420bcf44c5d6203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daaf24d8dee644cc871f03085274862c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e013a37444e64d639717aa8b93ce1a49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2c14d2f918e421f9e06f3128eda375e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0e21eccc7f4482da2879a0536df0482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"Mdean77/finetuned_arctic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_huggingface\n",
      "  Using cached langchain_huggingface-0.1.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /Users/jmichaeldean/anaconda3/envs/AIE4/lib/python3.11/site-packages (from langchain_huggingface) (0.24.6)\n",
      "Requirement already satisfied: langchain-core<0.4,>=0.3.0 in /Users/jmichaeldean/anaconda3/envs/AIE4/lib/python3.11/site-packages (from langchain_huggingface) (0.3.5)\n",
      "Requirement already satisfied: sentence-transformers>=2.6.0 in /Users/jmichaeldean/anaconda3/envs/AIE4/lib/python3.11/site-packages (from langchain_huggingface) (3.1.1)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in /Users/jmichaeldean/anaconda3/envs/AIE4/lib/python3.11/site-packages (from langchain_huggingface) (0.19.1)\n",
      "Requirement already satisfied: transformers>=4.39.0 in /Users/jmichaeldean/anaconda3/envs/AIE4/lib/python3.11/site-packages (from langchain_huggingface) (4.44.2)\n",
      "Requirement already satisfied: filelock in /Users/jmichaeldean/anaconda3/envs/AIE4/lib/python3.11/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/jmichaeldean/anaconda3/envs/AIE4/lib/python3.11/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/jmichaeldean/anaconda3/envs/AIE4/lib/python3.11/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/jmichaeldean/anaconda3/envs/AIE4/lib/python3.11/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (6.0.1)\n",
      "Requirement already satisfied: requests in /Users/jmichaeldean/anaconda3/envs/AIE4/lib/python3.11/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/jmichaeldean/anaconda3/envs/AIE4/lib/python3.11/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/jmichaeldean/anaconda3/envs/AIE4/lib/python3.11/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.11.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/jmichaeldean/anaconda3/envs/AIE4/lib/python3.11/site-packages (from langchain-core<0.4,>=0.3.0->langchain_huggingface) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /Users/jmichaeldean/anaconda3/envs/AIE4/lib/python3.11/site-packages (from langchain-core<0.4,>=0.3.0->langchain_huggingface) (0.1.125)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /Users/jmichaeldean/anaconda3/envs/AIE4/lib/python3.11/site-packages (from langchain-core<0.4,>=0.3.0->langchain_huggingface) (2.9.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /Users/jmichaeldean/anaconda3/envs/AIE4/lib/python3.11/site-packages (from langchain-core<0.4,>=0.3.0->langchain_huggingface) (8.5.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/jmichaeldean/anaconda3/envs/AIE4/lib/python3.11/site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (2.4.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/jmichaeldean/anaconda3/envs/AIE4/lib/python3.11/site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.5.1)\n",
      "Requirement already satisfied: scipy in /Users/jmichaeldean/anaconda3/envs/AIE4/lib/python3.11/site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.14.0)\n",
      "Requirement already satisfied: Pillow in /Users/jmichaeldean/anaconda3/envs/AIE4/lib/python3.11/site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (10.4.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/jmichaeldean/anaconda3/envs/AIE4/lib/python3.11/site-packages (from transformers>=4.39.0->langchain_huggingface) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/jmichaeldean/anaconda3/envs/AIE4/lib/python3.11/site-packages (from transformers>=4.39.0->langchain_huggingface) (2024.7.24)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/jmichaeldean/anaconda3/envs/AIE4/lib/python3.11/site-packages (from transformers>=4.39.0->langchain_huggingface) (0.4.5)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/jmichaeldean/anaconda3/envs/AIE4/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3.0->langchain_huggingface) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/jmichaeldean/anaconda3/envs/AIE4/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain_huggingface) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/jmichaeldean/anaconda3/envs/AIE4/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain_huggingface) (3.10.7)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/jmichaeldean/anaconda3/envs/AIE4/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.3.0->langchain_huggingface) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Users/jmichaeldean/anaconda3/envs/AIE4/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.3.0->langchain_huggingface) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jmichaeldean/anaconda3/envs/AIE4/lib/python3.11/site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jmichaeldean/anaconda3/envs/AIE4/lib/python3.11/site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jmichaeldean/anaconda3/envs/AIE4/lib/python3.11/site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jmichaeldean/anaconda3/envs/AIE4/lib/python3.11/site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (2024.7.4)\n",
      "Requirement already satisfied: sympy in /Users/jmichaeldean/anaconda3/envs/AIE4/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/jmichaeldean/anaconda3/envs/AIE4/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/jmichaeldean/anaconda3/envs/AIE4/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.1.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/jmichaeldean/anaconda3/envs/AIE4/lib/python3.11/site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/jmichaeldean/anaconda3/envs/AIE4/lib/python3.11/site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (3.5.0)\n",
      "Requirement already satisfied: anyio in /Users/jmichaeldean/anaconda3/envs/AIE4/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain_huggingface) (3.7.1)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/jmichaeldean/anaconda3/envs/AIE4/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain_huggingface) (1.0.5)\n",
      "Requirement already satisfied: sniffio in /Users/jmichaeldean/anaconda3/envs/AIE4/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain_huggingface) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/jmichaeldean/anaconda3/envs/AIE4/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain_huggingface) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/jmichaeldean/anaconda3/envs/AIE4/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/jmichaeldean/anaconda3/envs/AIE4/lib/python3.11/site-packages (from sympy->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.3.0)\n",
      "Using cached langchain_huggingface-0.1.0-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: langchain_huggingface\n",
      "Successfully installed langchain_huggingface-0.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "finetune_embeddings = HuggingFaceEmbeddings(model_name=\"Mdean77/finetuned_arctic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jmichaeldean/anaconda3/envs/AIE4/lib/python3.11/site-packages/langchain_openai/chat_models/__init__.py:1: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_openai.chat_models.azure import AzureChatOpenAI\n",
      "/Users/jmichaeldean/anaconda3/envs/AIE4/lib/python3.11/site-packages/pydantic/_internal/_config.py:341: UserWarning: Valid config keys have changed in V2:\n",
      "* 'allow_population_by_field_name' has been renamed to 'populate_by_name'\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    },
    {
     "ename": "PydanticUserError",
     "evalue": "The `__modify_schema__` method is not supported in Pydantic v2. Use `__get_pydantic_json_schema__` instead in class `SecretStr`.\n\nFor further information visit https://errors.pydantic.dev/2.9/u/custom-json-schema",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPydanticUserError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msplitAndVectorize\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# page_split_vectorstore was created earlier, and uses te3 embedder.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# page_split_vectorstore = splitAndVectorize.createVectorstore(\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#     separate_pages,\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#     \"separate_page_collection\",\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m      7\u001b[0m te3_page_split_retriever \u001b[38;5;241m=\u001b[39m page_split_vectorstore\u001b[38;5;241m.\u001b[39mas_retriever()\n",
      "File \u001b[0;32m~/AIE4/Midterm/splitAndVectorize.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtiktoken\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Qdrant\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_splitter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdefaults\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/AIE4/lib/python3.11/site-packages/langchain_openai/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureChatOpenAI, ChatOpenAI\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureOpenAIEmbeddings, OpenAIEmbeddings\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureOpenAI, OpenAI\n",
      "File \u001b[0;32m~/anaconda3/envs/AIE4/lib/python3.11/site-packages/langchain_openai/chat_models/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mazure\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureChatOpenAI\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[1;32m      4\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatOpenAI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAzureChatOpenAI\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/AIE4/lib/python3.11/site-packages/langchain_openai/chat_models/azure.py:41\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunction_calling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert_to_openai_tool\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_basemodel_subclass\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseChatOpenAI\n\u001b[1;32m     43\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     46\u001b[0m _BM \u001b[38;5;241m=\u001b[39m TypeVar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_BM\u001b[39m\u001b[38;5;124m\"\u001b[39m, bound\u001b[38;5;241m=\u001b[39mBaseModel)\n",
      "File \u001b[0;32m~/anaconda3/envs/AIE4/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:379\u001b[0m\n\u001b[1;32m    375\u001b[0m     parsed: Optional[_DictOrPydantic]\n\u001b[1;32m    376\u001b[0m     parsing_error: Optional[\u001b[38;5;167;01mBaseException\u001b[39;00m]\n\u001b[0;32m--> 379\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBaseChatOpenAI\u001b[39;00m(BaseChatModel):\n\u001b[1;32m    380\u001b[0m     client: Any \u001b[38;5;241m=\u001b[39m Field(default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, exclude\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m#: :meta private:\u001b[39;00m\n\u001b[1;32m    381\u001b[0m     async_client: Any \u001b[38;5;241m=\u001b[39m Field(default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, exclude\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m#: :meta private:\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/AIE4/lib/python3.11/site-packages/pydantic/_internal/_model_construction.py:224\u001b[0m, in \u001b[0;36mModelMetaclass.__new__\u001b[0;34m(mcs, cls_name, bases, namespace, __pydantic_generic_metadata__, __pydantic_reset_parent_namespace__, _create_model_module, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_wrapper\u001b[38;5;241m.\u001b[39mfrozen \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__hash__\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m namespace:\n\u001b[1;32m    222\u001b[0m     set_default_hash_func(\u001b[38;5;28mcls\u001b[39m, bases)\n\u001b[0;32m--> 224\u001b[0m complete_model_class(\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m    226\u001b[0m     cls_name,\n\u001b[1;32m    227\u001b[0m     config_wrapper,\n\u001b[1;32m    228\u001b[0m     raise_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    229\u001b[0m     types_namespace\u001b[38;5;241m=\u001b[39mtypes_namespace,\n\u001b[1;32m    230\u001b[0m     create_model_module\u001b[38;5;241m=\u001b[39m_create_model_module,\n\u001b[1;32m    231\u001b[0m )\n\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m# If this is placed before the complete_model_class call above,\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;66;03m# the generic computed fields return type is set to PydanticUndefined\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_computed_fields \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39minfo \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_decorators__\u001b[38;5;241m.\u001b[39mcomputed_fields\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[0;32m~/anaconda3/envs/AIE4/lib/python3.11/site-packages/pydantic/_internal/_model_construction.py:577\u001b[0m, in \u001b[0;36mcomplete_model_class\u001b[0;34m(cls, cls_name, config_wrapper, raise_errors, types_namespace, create_model_module)\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 577\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__get_pydantic_core_schema__(\u001b[38;5;28mcls\u001b[39m, handler)\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m PydanticUndefinedAnnotation \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    579\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raise_errors:\n",
      "File \u001b[0;32m~/anaconda3/envs/AIE4/lib/python3.11/site-packages/pydantic/main.py:671\u001b[0m, in \u001b[0;36mBaseModel.__get_pydantic_core_schema__\u001b[0;34m(cls, source, handler)\u001b[0m\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_generic_metadata__[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morigin\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m    669\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_core_schema__\n\u001b[0;32m--> 671\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m handler(source)\n",
      "File \u001b[0;32m~/anaconda3/envs/AIE4/lib/python3.11/site-packages/pydantic/_internal/_schema_generation_shared.py:83\u001b[0m, in \u001b[0;36mCallbackGetCoreSchemaHandler.__call__\u001b[0;34m(self, source_type)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, source_type: Any, \u001b[38;5;241m/\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m core_schema\u001b[38;5;241m.\u001b[39mCoreSchema:\n\u001b[0;32m---> 83\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handler(source_type)\n\u001b[1;32m     84\u001b[0m     ref \u001b[38;5;241m=\u001b[39m schema\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mref\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ref_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto-def\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/AIE4/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:655\u001b[0m, in \u001b[0;36mGenerateSchema.generate_schema\u001b[0;34m(self, obj, from_dunder_get_core_schema)\u001b[0m\n\u001b[1;32m    652\u001b[0m         schema \u001b[38;5;241m=\u001b[39m from_property\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 655\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_schema_inner(obj)\n\u001b[1;32m    657\u001b[0m metadata_js_function \u001b[38;5;241m=\u001b[39m _extract_get_pydantic_json_schema(obj, schema)\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadata_js_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/AIE4/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:924\u001b[0m, in \u001b[0;36mGenerateSchema._generate_schema_inner\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lenient_issubclass(obj, BaseModel):\n\u001b[1;32m    923\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type_stack\u001b[38;5;241m.\u001b[39mpush(obj):\n\u001b[0;32m--> 924\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_schema(obj)\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, PydanticRecursiveRef):\n\u001b[1;32m    927\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m core_schema\u001b[38;5;241m.\u001b[39mdefinition_reference_schema(schema_ref\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mtype_ref)\n",
      "File \u001b[0;32m~/anaconda3/envs/AIE4/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:739\u001b[0m, in \u001b[0;36mGenerateSchema._model_schema\u001b[0;34m(self, cls)\u001b[0m\n\u001b[1;32m    727\u001b[0m     model_schema \u001b[38;5;241m=\u001b[39m core_schema\u001b[38;5;241m.\u001b[39mmodel_schema(\n\u001b[1;32m    728\u001b[0m         \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m    729\u001b[0m         inner_schema,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    735\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mmetadata,\n\u001b[1;32m    736\u001b[0m     )\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    738\u001b[0m     fields_schema: core_schema\u001b[38;5;241m.\u001b[39mCoreSchema \u001b[38;5;241m=\u001b[39m core_schema\u001b[38;5;241m.\u001b[39mmodel_fields_schema(\n\u001b[0;32m--> 739\u001b[0m         {k: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_md_field_schema(k, v, decorators) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m fields\u001b[38;5;241m.\u001b[39mitems()},\n\u001b[1;32m    740\u001b[0m         computed_fields\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m    741\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_computed_field_schema(d, decorators\u001b[38;5;241m.\u001b[39mfield_serializers)\n\u001b[1;32m    742\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m computed_fields\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[1;32m    743\u001b[0m         ],\n\u001b[1;32m    744\u001b[0m         extras_schema\u001b[38;5;241m=\u001b[39mextras_schema,\n\u001b[1;32m    745\u001b[0m         model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m    746\u001b[0m     )\n\u001b[1;32m    747\u001b[0m     inner_schema \u001b[38;5;241m=\u001b[39m apply_validators(fields_schema, decorators\u001b[38;5;241m.\u001b[39mroot_validators\u001b[38;5;241m.\u001b[39mvalues(), \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    748\u001b[0m     new_inner_schema \u001b[38;5;241m=\u001b[39m define_expected_missing_refs(inner_schema, recursively_defined_type_refs())\n",
      "File \u001b[0;32m~/anaconda3/envs/AIE4/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:739\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    727\u001b[0m     model_schema \u001b[38;5;241m=\u001b[39m core_schema\u001b[38;5;241m.\u001b[39mmodel_schema(\n\u001b[1;32m    728\u001b[0m         \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m    729\u001b[0m         inner_schema,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    735\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mmetadata,\n\u001b[1;32m    736\u001b[0m     )\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    738\u001b[0m     fields_schema: core_schema\u001b[38;5;241m.\u001b[39mCoreSchema \u001b[38;5;241m=\u001b[39m core_schema\u001b[38;5;241m.\u001b[39mmodel_fields_schema(\n\u001b[0;32m--> 739\u001b[0m         {k: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_md_field_schema(k, v, decorators) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m fields\u001b[38;5;241m.\u001b[39mitems()},\n\u001b[1;32m    740\u001b[0m         computed_fields\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m    741\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_computed_field_schema(d, decorators\u001b[38;5;241m.\u001b[39mfield_serializers)\n\u001b[1;32m    742\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m computed_fields\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[1;32m    743\u001b[0m         ],\n\u001b[1;32m    744\u001b[0m         extras_schema\u001b[38;5;241m=\u001b[39mextras_schema,\n\u001b[1;32m    745\u001b[0m         model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m    746\u001b[0m     )\n\u001b[1;32m    747\u001b[0m     inner_schema \u001b[38;5;241m=\u001b[39m apply_validators(fields_schema, decorators\u001b[38;5;241m.\u001b[39mroot_validators\u001b[38;5;241m.\u001b[39mvalues(), \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    748\u001b[0m     new_inner_schema \u001b[38;5;241m=\u001b[39m define_expected_missing_refs(inner_schema, recursively_defined_type_refs())\n",
      "File \u001b[0;32m~/anaconda3/envs/AIE4/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:1115\u001b[0m, in \u001b[0;36mGenerateSchema._generate_md_field_schema\u001b[0;34m(self, name, field_info, decorators)\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_md_field_schema\u001b[39m(\n\u001b[1;32m   1109\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1110\u001b[0m     name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   1111\u001b[0m     field_info: FieldInfo,\n\u001b[1;32m   1112\u001b[0m     decorators: DecoratorInfos,\n\u001b[1;32m   1113\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m core_schema\u001b[38;5;241m.\u001b[39mModelField:\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prepare a ModelField to represent a model field.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1115\u001b[0m     common_field \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_common_field_schema(name, field_info, decorators)\n\u001b[1;32m   1116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m core_schema\u001b[38;5;241m.\u001b[39mmodel_field(\n\u001b[1;32m   1117\u001b[0m         common_field[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mschema\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   1118\u001b[0m         serialization_exclude\u001b[38;5;241m=\u001b[39mcommon_field[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mserialization_exclude\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mcommon_field[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   1123\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/AIE4/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:1308\u001b[0m, in \u001b[0;36mGenerateSchema._common_field_schema\u001b[0;34m(self, name, field_info, decorators)\u001b[0m\n\u001b[1;32m   1304\u001b[0m         schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_annotations(\n\u001b[1;32m   1305\u001b[0m             source_type, annotations \u001b[38;5;241m+\u001b[39m validators_from_decorators, transform_inner_schema\u001b[38;5;241m=\u001b[39mset_discriminator\n\u001b[1;32m   1306\u001b[0m         )\n\u001b[1;32m   1307\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1308\u001b[0m         schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_annotations(\n\u001b[1;32m   1309\u001b[0m             source_type,\n\u001b[1;32m   1310\u001b[0m             annotations \u001b[38;5;241m+\u001b[39m validators_from_decorators,\n\u001b[1;32m   1311\u001b[0m         )\n\u001b[1;32m   1313\u001b[0m \u001b[38;5;66;03m# This V1 compatibility shim should eventually be removed\u001b[39;00m\n\u001b[1;32m   1314\u001b[0m \u001b[38;5;66;03m# push down any `each_item=True` validators\u001b[39;00m\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;66;03m# note that this won't work for any Annotated types that get wrapped by a function validator\u001b[39;00m\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;66;03m# but that's okay because that didn't exist in V1\u001b[39;00m\n\u001b[1;32m   1317\u001b[0m this_field_validators \u001b[38;5;241m=\u001b[39m filter_field_decorator_info_by_field(decorators\u001b[38;5;241m.\u001b[39mvalidators\u001b[38;5;241m.\u001b[39mvalues(), name)\n",
      "File \u001b[0;32m~/anaconda3/envs/AIE4/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2107\u001b[0m, in \u001b[0;36mGenerateSchema._apply_annotations\u001b[0;34m(self, source_type, annotations, transform_inner_schema)\u001b[0m\n\u001b[1;32m   2102\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   2103\u001b[0m     get_inner_schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_wrapped_inner_schema(\n\u001b[1;32m   2104\u001b[0m         get_inner_schema, annotation, pydantic_js_annotation_functions\n\u001b[1;32m   2105\u001b[0m     )\n\u001b[0;32m-> 2107\u001b[0m schema \u001b[38;5;241m=\u001b[39m get_inner_schema(source_type)\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pydantic_js_annotation_functions:\n\u001b[1;32m   2109\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m CoreMetadataHandler(schema)\u001b[38;5;241m.\u001b[39mmetadata\n",
      "File \u001b[0;32m~/anaconda3/envs/AIE4/lib/python3.11/site-packages/pydantic/_internal/_schema_generation_shared.py:83\u001b[0m, in \u001b[0;36mCallbackGetCoreSchemaHandler.__call__\u001b[0;34m(self, source_type)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, source_type: Any, \u001b[38;5;241m/\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m core_schema\u001b[38;5;241m.\u001b[39mCoreSchema:\n\u001b[0;32m---> 83\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handler(source_type)\n\u001b[1;32m     84\u001b[0m     ref \u001b[38;5;241m=\u001b[39m schema\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mref\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ref_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto-def\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/AIE4/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2088\u001b[0m, in \u001b[0;36mGenerateSchema._apply_annotations.<locals>.inner_handler\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2086\u001b[0m from_property \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_schema_from_property(obj, source_type)\n\u001b[1;32m   2087\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_property \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2088\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_schema_inner(obj)\n\u001b[1;32m   2089\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2090\u001b[0m     schema \u001b[38;5;241m=\u001b[39m from_property\n",
      "File \u001b[0;32m~/anaconda3/envs/AIE4/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:929\u001b[0m, in \u001b[0;36mGenerateSchema._generate_schema_inner\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, PydanticRecursiveRef):\n\u001b[1;32m    927\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m core_schema\u001b[38;5;241m.\u001b[39mdefinition_reference_schema(schema_ref\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mtype_ref)\n\u001b[0;32m--> 929\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmatch_type(obj)\n",
      "File \u001b[0;32m~/anaconda3/envs/AIE4/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:1029\u001b[0m, in \u001b[0;36mGenerateSchema.match_type\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m   1027\u001b[0m origin \u001b[38;5;241m=\u001b[39m get_origin(obj)\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m origin \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1029\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_generic_type(obj, origin)\n\u001b[1;32m   1031\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_prepare_pydantic_annotations_for_known_type(obj, ())\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/AIE4/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:1058\u001b[0m, in \u001b[0;36mGenerateSchema._match_generic_type\u001b[0;34m(self, obj, origin)\u001b[0m\n\u001b[1;32m   1055\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m from_property\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _typing_extra\u001b[38;5;241m.\u001b[39morigin_is_union(origin):\n\u001b[0;32m-> 1058\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_union_schema(obj)\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m origin \u001b[38;5;129;01min\u001b[39;00m TUPLE_TYPES:\n\u001b[1;32m   1060\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tuple_schema(obj)\n",
      "File \u001b[0;32m~/anaconda3/envs/AIE4/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:1378\u001b[0m, in \u001b[0;36mGenerateSchema._union_schema\u001b[0;34m(self, union_type)\u001b[0m\n\u001b[1;32m   1376\u001b[0m         nullable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1377\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1378\u001b[0m         choices\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_schema(arg))\n\u001b[1;32m   1380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(choices) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1381\u001b[0m     s \u001b[38;5;241m=\u001b[39m choices[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/AIE4/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:657\u001b[0m, in \u001b[0;36mGenerateSchema.generate_schema\u001b[0;34m(self, obj, from_dunder_get_core_schema)\u001b[0m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    655\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_schema_inner(obj)\n\u001b[0;32m--> 657\u001b[0m metadata_js_function \u001b[38;5;241m=\u001b[39m _extract_get_pydantic_json_schema(obj, schema)\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadata_js_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    659\u001b[0m     metadata_schema \u001b[38;5;241m=\u001b[39m resolve_original_schema(schema, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefs\u001b[38;5;241m.\u001b[39mdefinitions)\n",
      "File \u001b[0;32m~/anaconda3/envs/AIE4/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2447\u001b[0m, in \u001b[0;36m_extract_get_pydantic_json_schema\u001b[0;34m(tp, schema)\u001b[0m\n\u001b[1;32m   2445\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_custom_v2_modify_js_func:\n\u001b[1;32m   2446\u001b[0m         cls_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(tp, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__name__\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 2447\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PydanticUserError(\n\u001b[1;32m   2448\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe `__modify_schema__` method is not supported in Pydantic v2. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2449\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUse `__get_pydantic_json_schema__` instead\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in class `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcls_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mcls_name\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   2450\u001b[0m             code\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustom-json-schema\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   2451\u001b[0m         )\n\u001b[1;32m   2453\u001b[0m \u001b[38;5;66;03m# handle GenericAlias' but ignore Annotated which \"lies\" about its origin (in this case it would be `int`)\u001b[39;00m\n\u001b[1;32m   2454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(tp, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__origin__\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_annotated(tp):\n",
      "\u001b[0;31mPydanticUserError\u001b[0m: The `__modify_schema__` method is not supported in Pydantic v2. Use `__get_pydantic_json_schema__` instead in class `SecretStr`.\n\nFor further information visit https://errors.pydantic.dev/2.9/u/custom-json-schema"
     ]
    }
   ],
   "source": [
    "import splitAndVectorize\n",
    "# page_split_vectorstore was created earlier, and uses te3 embedder.\n",
    "# page_split_vectorstore = splitAndVectorize.createVectorstore(\n",
    "#     separate_pages,\n",
    "#     \"separate_page_collection\",\n",
    "# )\n",
    "te3_page_split_retriever = page_split_vectorstore.as_retriever()\n",
    "\n",
    "arctic_page_split_vectorstore = splitAndVectorize.createVectorstore(\n",
    "    separate_pages,\n",
    "    \"arctic_separate_page_collection\",\n",
    "    embedding_model=finetune_embeddings,\n",
    ")\n",
    "arctic_page_split_retriever = arctic_page_split_vectorstore.as_retriever()\n",
    "\n",
    "#need to RESPLIT the chunk split for testing \n",
    "chunk_split_vectorstore = splitAndVectorize.createVectorstore(\n",
    "    one_document,\n",
    "    \"te3_chunk_split_collection\",\n",
    "    chunk_size=250,\n",
    "    chunk_overlap=50,\n",
    "    embedding_model=finetune_embeddings,\n",
    ")\n",
    "te3_chunk_split_retriever = chunk_split_vectorstore.as_retriever()\n",
    "\n",
    "# now make chunk split with arctic\n",
    "arctic_chunk_split_vectorstore = splitAndVectorize.createVectorstore(\n",
    "    one_document,\n",
    "    \"arctic_chunk_split_collection\",\n",
    "    chunk_size=250,\n",
    "    chunk_overlap=50,\n",
    ")\n",
    "arctic_chunk_split_retriever = arctic_chunk_split_vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "retrieval_augmented_qa_chain_chunk = (\n",
    "    {\"context\": itemgetter(\"question\") | chunk_split_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | llm, \"context\": itemgetter(\"context\")}\n",
    ")\n",
    "retrieval_augmented_qa_chain_paged = (\n",
    "    {\"context\": itemgetter(\"question\") | page_split_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | llm, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5 Deliverables:\n",
    "1.  Test the fine-tuned embedding model using the RAGAS frameworks to quantify any improvements.  Provide results in a table.<br>\n",
    "\n",
    "2.  Test the two chunking strategies using the RAGAS frameworks to quantify any improvements.  Provide results in a table.<br>\n",
    "\n",
    "3.  The AI Solutions Engineer asks me \"Which one is the best to test with internal stakeholders next week, and why?<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6.  Managing Your Boss and User Expectations\n",
    "(Role: SVP of Technology)\n",
    "\n",
    "## Task 6 Deliverables:\n",
    "1.  What is the story that I will give to the CEO to tell the whole company at the launch next month?<br>\n",
    "\n",
    "2.  There appears to be important information not included in our build.  How might we incorporate relevant white-house briefing information in future versions? <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIE4a",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

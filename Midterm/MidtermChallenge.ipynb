{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Midterm Challenge Notebook - Mike Dean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU langchain langchain_openai langchain_core==0.2.40 langchain_community\n",
    "!pip install -qU qdrant_client pymupdf tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.  Dealing with the Data\n",
    "(Role: AI Solutions Engineer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, tiktoken\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Path to my directory containing PDF files\n",
    "directory = \"References/\"\n",
    "\n",
    "# List to store all the documents\n",
    "all_docs = []\n",
    "\n",
    "# Iterate through all the files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".pdf\"):  # Check if the file is a PDF\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        loader = PyMuPDFLoader(file_path)\n",
    "        docs = loader.load()\n",
    "        all_docs.extend(docs)  # Append the loaded docs to my list\n",
    "\n",
    "# Default behavior is to break PDF files into their pages\n",
    "# Using tiktoken, I checked the token lengths of several representative pages and\n",
    "# the lengths were always less than 1000 tokens, so INITIAL STRATEGY is to use\n",
    "# each document as a single chunk and not further split.\n",
    "\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "page_split_vectorstore = Qdrant.from_documents(\n",
    "    all_docs,\n",
    "    embedding_model,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"page_split_collection\",\n",
    ")\n",
    "page_split_retriever = page_split_vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ALTERNATIVE STRATEGY is to recombine all the pages into one string document and then\n",
    "# split it.  The advantage of this approach is to have chunk overlap, which is not\n",
    "# possible with my initial strategy.\n",
    "\n",
    "one_document = \"\"\n",
    "for doc in all_docs:\n",
    "    one_document += doc.page_content\n",
    "\n",
    "def tiktoken_len(text):\n",
    "    tokens = tiktoken.encoding_for_model(\"gpt-4o\").encode(\n",
    "        text,\n",
    "    )\n",
    "    return len(tokens)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 800,\n",
    "    chunk_overlap = 400,\n",
    "    length_function = tiktoken_len,\n",
    ")\n",
    "\n",
    "split_chunks = text_splitter.split_text(one_document)\n",
    "\n",
    "chunk_split_vectorstore = Qdrant.from_documents(\n",
    "    text_splitter.create_documents(split_chunks),\n",
    "    embedding_model,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"chunk_split_collection\",\n",
    ")\n",
    "\n",
    "chunk_split_retriever = chunk_split_vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collections=[CollectionDescription(name='page_split_collection')]\n",
      "collections=[CollectionDescription(name='chunk_split_collection')]\n"
     ]
    }
   ],
   "source": [
    "## Check that I have two vectorstores in memory\n",
    "client = page_split_vectorstore.client\n",
    "print(client.get_collections())\n",
    "client = chunk_split_vectorstore.client\n",
    "print(client.get_collections())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 Deliverables:\n",
    "1.  Describe the default chunking strategy that I will use:<br>\n",
    "The default strategy will be to load the two PDF files using `PyMuPDFLoader` just as we have previously done.  This results in each PDF page being its own document.  I have checked sample pages with `tiktoken` and the token count per page is <1000, so these are small enough to just embed without further splitting. I saved these embeddings in `page_split_vectorstore`.\n",
    "\n",
    "2.  Articulate a chunking strategy that I will also test:<br>\n",
    "The disadvantage of the default strategy is that there is no chunk overlapping between the pages, and this might worsen the ability connect two pages that are both relevant to a query.  So I  recombine the page_content of all pages into a single string, convert it into a document, and split it with a chunk size of 800 and an overlap of 400 (the default settings used by OpenAI).  This strategy allows chunks to overlap, pehaps adding semantic continuity between adjacent pages.  These were embedded with the same embedding model and saved in `chunk_split_vectorstore`.\n",
    "\n",
    "3.  Describe how and why I made these decisions:<br>\n",
    "The default behavior of `PyMuPDFLoader` is not bad and I have been using it for several months.  However, I was splitting each of the documents created, not thinking through that if I had chunk sizes greater than the page itself, this was meaningless.  I also had chunk overlap, but had not thought through the implications of each page being a separate document.  So I made these decision for this Midterm Challenge so I can later compare the performance using RAGAS in Task 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.  Building a Quick End-to-End Prototype\n",
    "(Role: AI Systems Engineer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "rag_prompt_template = \"\"\"\\\n",
    "You are a helpful and polite and cheerful assistant who answers questions based solely on the provided context. \n",
    "Use the context to answer the question and provide a  clear answer. Do not mention the document in your\n",
    "response.\n",
    "If there is no specific information\n",
    "relevant to the question, then tell the user that you can't answer based on the context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(rag_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "## CREATE MY TWO RAG CHAINS\n",
    "\n",
    "page_split_rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | page_split_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | rag_prompt | llm | StrOutputParser()\n",
    ")\n",
    "\n",
    "chunk_split_rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | chunk_split_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | rag_prompt | llm | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on the provided context, the ten major risks of AI, particularly Generative AI (GAI), can be categorized as follows:\n",
       "\n",
       "1. **Confabulation**\n",
       "2. **Dangerous or Violent Recommendations**\n",
       "3. **Data Privacy**\n",
       "4. **Value Chain and Component Integration**\n",
       "5. **Harmful Bias**\n",
       "6. **Homogenization**\n",
       "7. **CBRN Information or Capabilities**\n",
       "8. **Human-AI Configuration**\n",
       "9. **Obscene, Degrading, and/or Abusive Content**\n",
       "10. **Information Integrity**\n",
       "\n",
       "These risks can be further categorized into technical/model risks, misuse by humans, and ecosystem/societal risks."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Some risks of AI include:\n",
       "\n",
       "1. **Confabulation**: The production of confidently stated but erroneous or false content, which can mislead or deceive users.\n",
       "2. **Dangerous, Violent, or Hateful Content**: Eased production of and access to harmful content, including violent, inciting, radicalizing, or threatening material.\n",
       "3. **Data Privacy**: Leakage and unauthorized use or disclosure of sensitive data like biometric, health, or location information.\n",
       "4. **Environmental Impacts**: High compute resource utilization in training or operating AI models, which can adversely impact ecosystems.\n",
       "5. **Harmful Bias or Homogenization**: Amplification and exacerbation of historical, societal, and systemic biases, and performance disparities between sub-groups or languages.\n",
       "6. **Algorithmic Monocultures**: Increased susceptibility to correlated failures due to the repeated use of the same model in consequential decision-making settings.\n",
       "\n",
       "These risks can arise during various stages of the AI lifecycle, from design and development to deployment and operation, and can have impacts at individual, application, and ecosystem levels."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "## TEST THE TWO CHAINS\n",
    "page_response = (page_split_rag_chain.invoke({\"question\": \"List the ten major risks of AI?\"}))\n",
    "display(Markdown(page_response))\n",
    "\n",
    "chunk_response = (chunk_split_rag_chain.invoke({\"question\": \"What are some risks of AI?\"}))\n",
    "display(Markdown(chunk_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 Deliverables:\n",
    "1.  Build a live public prototype on Hugging Face, and include the public URL link to my space.<br>\n",
    "\n",
    "2.  How did I choose my stack, and why did I select each tool the way it Did? <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.  Creating a Golden Test Data Set\n",
    "(Role: AI Evaluation and Performance Engineer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 Deliverables:\n",
    "1.  Assess my pipeline using the RAGAS framework including key metrics faithfulness, answer relevancy, context precision, and context recall.  Provide a table of my output results.<br>\n",
    "\n",
    "2.  What conclusions can I draw about performance and effectiveness of my pipeline with this information? <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.  Fine-Tuning Open-Source Embeddings\n",
    "(Role: Machine Learning Engineer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 Deliverables:\n",
    "1.  Swap out my existing embedding model for the new fine tuned version.  Provide a link to m fine-tuned embedding model on the Hugging Face Hub.<br>\n",
    "\n",
    "2.  How did I choose the embedding model for this application?<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5.  Assessing Performance\n",
    "(Role: AI Evaluation and Performance Engineer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5 Deliverables:\n",
    "1.  Test the fine-tuned embedding model using the RAGAS frameworks to quantify any improvements.  Provide results in a table.<br>\n",
    "\n",
    "2.  Test the two chunking strategies using the RAGAS frameworks to quantify any improvements.  Provide results in a table.<br>\n",
    "\n",
    "3.  The AI Solutions Engineer asks me \"Which one is the best to test with internal stakeholders next week, and why?<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6.  Managing Your Boss and User Expectations\n",
    "(Role: SVP of Technology)\n",
    "\n",
    "## Task 6 Deliverables:\n",
    "1.  What is the story that I will give to the CEO to tell the whole company at the launch next month?<br>\n",
    "\n",
    "2.  There appears to be important information not included in our build.  How might we incorporate relevant white-house briefing information in future versions? <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIE4a",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

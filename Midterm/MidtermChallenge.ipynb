{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Midterm Challenge Notebook - Mike Dean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU langchain langchain_openai langchain_core==0.2.40 langchain_community\n",
    "!pip install -qU qdrant_client pymupdf tiktoken ragas pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.  Dealing with the Data\n",
    "(Role: AI Solutions Engineer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import defaults\n",
    "llm = defaults.default_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method count of list object at 0x1143159c0>\n"
     ]
    }
   ],
   "source": [
    "# Load PDF documents from a directory\n",
    "import loadReferenceDocuments\n",
    "separate_pages, one_document = loadReferenceDocuments.loadReferenceDocuments(\"References/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking Strategies\n",
    "#### Ingest the PDF by page - page_split\n",
    "#### Ingest the PDF by page and recombine into single file and then chunk - chunk_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import splitAndVectorize\n",
    "\n",
    "page_split_vectorstore = splitAndVectorize.createVectorstore(\n",
    "    separate_pages,\n",
    "    \"separate_page_collection\",\n",
    ")\n",
    "\n",
    "page_split_retriever = page_split_vectorstore.as_retriever()\n",
    "\n",
    "chunk_split_vectorstore = splitAndVectorize.createVectorstore(\n",
    "    one_document,\n",
    "    \"chunk_split_collection\",\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=400,\n",
    ")\n",
    "\n",
    "chunk_split_retriever = chunk_split_vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 Deliverables:\n",
    "1.  Describe the default chunking strategy that I will use:<br>\n",
    "The default strategy will be to load the two PDF files using `PyMuPDFLoader` just as we have previously done.  This results in each PDF page being its own document.  I have checked sample pages with `tiktoken` and the token count per page is <1000, so these are small enough to just embed without further splitting. I saved these embeddings in `page_split_vectorstore`.\n",
    "\n",
    "2.  Articulate a chunking strategy that I will also test:<br>\n",
    "The disadvantage of the default strategy is that there is no chunk overlapping between the pages, and this might worsen the ability connect two pages that are both relevant to a query.  So I  recombine the page_content of all pages into a single string, convert it into a document, and split it with a chunk size of 800 and an overlap of 400 (the default settings used by OpenAI).  This strategy allows chunks to overlap, pehaps adding semantic continuity between adjacent pages.  These were embedded with the same embedding model and saved in `chunk_split_vectorstore`.\n",
    "\n",
    "3.  Describe how and why I made these decisions:<br>\n",
    "The default behavior of `PyMuPDFLoader` is not bad and I have been using it for several months.  However, I was splitting each of the documents created, not thinking through that if I had chunk sizes greater than the page itself, this was meaningless.  I also had chunk overlap, but had not thought through the implications of each page being a separate document.  So I made these decision for this Midterm Challenge so I can later compare the performance using RAGAS in Task 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.  Building a Quick End-to-End Prototype\n",
    "(Role: AI Systems Engineer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prompts\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from operator import itemgetter\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(prompts.rag_prompt_template)\n",
    "\n",
    "page_split_rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | page_split_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | rag_prompt | llm | StrOutputParser()\n",
    ")\n",
    "\n",
    "chunk_split_rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | chunk_split_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | rag_prompt | llm | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The ten major risks of AI, as identified, are:\n",
       "\n",
       "1. **Confabulation**: AI generating false or misleading information.\n",
       "2. **Dangerous or Violent Recommendations**: AI suggesting harmful actions.\n",
       "3. **Data Privacy**: Risks related to unauthorized access and misuse of personal data.\n",
       "4. **Value Chain and Component Integration**: Issues arising from integrating various AI components.\n",
       "5. **Harmful Bias**: AI perpetuating or amplifying biases present in training data.\n",
       "6. **Homogenization**: The risk of creating uniformity that can lead to systemic vulnerabilities.\n",
       "7. **CBRN Information or Capabilities**: Misuse of AI for chemical, biological, radiological, or nuclear purposes.\n",
       "8. **Human-AI Configuration**: Risks from improper interaction between humans and AI systems.\n",
       "9. **Obscene, Degrading, and/or Abusive Content**: AI generating harmful or offensive content.\n",
       "10. **Information Integrity**: Ensuring the accuracy and reliability of information produced by AI systems.\n",
       "\n",
       "These risks can be categorized into technical/model risks, misuse by humans, and ecosystem/societal risks."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Some risks of AI include:\n",
       "\n",
       "1. **Technical / Model Risks**:\n",
       "   - Confabulation: The production of confidently stated but erroneous or false content.\n",
       "   - Dangerous or Violent Recommendations: AI models suggesting harmful actions.\n",
       "   - Data Privacy: Leakage and unauthorized use of sensitive data.\n",
       "   - Value Chain and Component Integration: Issues in integrating various components leading to risks.\n",
       "   - Harmful Bias and Homogenization: Amplification of biases and undesired homogeneity.\n",
       "\n",
       "2. **Misuse by Humans**:\n",
       "   - CBRN Information or Capabilities: Easier access to information related to chemical, biological, radiological, or nuclear weapons.\n",
       "   - Obscene, Degrading, and/or Abusive Content: Generation and distribution of harmful content.\n",
       "   - Information Integrity and Security: Issues with the accuracy and security of information.\n",
       "\n",
       "3. **Ecosystem / Societal Risks**:\n",
       "   - Environmental Impacts: High resource utilization affecting ecosystems.\n",
       "   - Intellectual Property: Risks to proprietary information and innovation.\n",
       "\n",
       "These risks can arise from the design, training, or operation of AI systems and can be exacerbated by human behavior."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "## TEST THE TWO CHAINS\n",
    "page_response = (page_split_rag_chain.invoke({\"question\": \"List the ten major risks of AI?\"}))\n",
    "display(Markdown(page_response))\n",
    "\n",
    "chunk_response = (chunk_split_rag_chain.invoke({\"question\": \"What are some risks of AI?\"}))\n",
    "display(Markdown(chunk_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 Deliverables:\n",
    "1.  Build a live public prototype on Hugging Face, and include the public URL link to my space.<br>\n",
    "\n",
    "Here is a one minute Loom video that demonstrates the prototype running in Hugging Face.\n",
    "https://www.loom.com/share/70b741d3e4e14af792572b3aa9106463?sid=5aeb2e51-0f75-4c74-9f8c-6bc6d14aaa52\n",
    "\n",
    "I used the page split retriever for this prototype, meaning that the documents were broken by page, not recombined, and were chunked as whole pages without overlap.\n",
    "\n",
    "2.  How did I choose my stack, and why did I select each tool the way it Did? <br>\n",
    "\n",
    "My stack consists of the following:\n",
    "- Hardware is Apple Mac Studio\n",
    "- Editor is VSC as recommended, and I have grown to like it very much  because it includes everything.\n",
    "- Qdrant is the vector store.  I have used FAISS and Chroma, but Qdrant is fantastic.  I run it locally as a server, though for this Hugging Face situation, I am using a memory-based implementation.\n",
    "- ChainLit is the interface, as recommended.  Notably, the current version of ChainLit does NOT work on Hugging Face, and the version needs to be locked (chainlit==0.7.700).\n",
    "- RAGAS is part of my stack for purposes of later evaluation.\n",
    "- LangChain is used to help simplify the code.\n",
    "\n",
    "I didn't choose this stack by accident, as all of it was suggested in the course.  However, I have resisted moving from one framework to another, etc., because I think important to stop wandering in the AIE jungle and learn one set of tools well.  This will prepare me to switch to other frameworks, if desired, but I think the key is to learn a set of reliable tools well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.  Creating a Golden Test Data Set\n",
    "(Role: AI Evaluation and Performance Engineer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "318"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import splitAndVectorize\n",
    "# create a new splitting without embedding\n",
    "\n",
    "eval_documents = splitAndVectorize.split_into_chunks(\n",
    "    one_document,\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "len(eval_documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following code was used to create tests but I have commented out to avoid repetition.\n",
    "## I created 50 pairs instead of 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "import defaults\n",
    "\n",
    "## llm and embedding models were set earlier\n",
    "# generator_llm = llm\n",
    "# critic_llm = llm\n",
    "# embeddings = defaults.default_embedding_model\n",
    "\n",
    "# generator = TestsetGenerator.from_langchain(\n",
    "#     generator_llm,\n",
    "#     critic_llm,\n",
    "#     embeddings\n",
    "# )\n",
    "\n",
    "# distributions = {\n",
    "#     simple: 0.5,\n",
    "#     multi_context: 0.4,\n",
    "#     reasoning: 0.1\n",
    "# }\n",
    "\n",
    "# num_qa_pairs = 50 # I increased this from 20 because I have a surplus of OpenAI credits\n",
    "\n",
    "## I ALREADY RAN THIS SO HAVE COMMENTED IT OUT HERE SO I DON'T DO IT AGAIN\n",
    "## I WILL READ IN THE CSV FILE TO CONTINUE\n",
    "# testset = generator.generate_with_langchain_docs(eval_documents, num_qa_pairs, distributions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the test data from the stored CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ IN THE TESTSET FROM THE CSV FILE\n",
    "import pandas as pd\n",
    "test_df = pd.read_csv(\"testset.csv\")\n",
    "test_questions = test_df[\"question\"].values.tolist()\n",
    "test_groundtruths = test_df[\"ground_truth\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "retrieval_augmented_qa_chain_chunk = (\n",
    "    {\"context\": itemgetter(\"question\") | chunk_split_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | llm, \"context\": itemgetter(\"context\")}\n",
    ")\n",
    "retrieval_augmented_qa_chain_paged = (\n",
    "    {\"context\": itemgetter(\"question\") | page_split_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | llm, \"context\": itemgetter(\"context\")}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a41328725bb485899f0cd0fc9db2452",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No statements were generated from the answer.\n"
     ]
    }
   ],
   "source": [
    "import evaluateRAGAS\n",
    "results = evaluateRAGAS.evaluateRAGAS(retrieval_augmented_qa_chain_chunk,\n",
    "                                                       test_questions, test_groundtruths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'faithfulness': 0.8999, 'answer_relevancy': 0.9475, 'context_recall': 0.9017, 'context_precision': 0.9083}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6660d66f59c145af808e09fab1bfbe66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No statements were generated from the answer.\n"
     ]
    }
   ],
   "source": [
    "import evaluateRAGAS\n",
    "paged_results = evaluateRAGAS.evaluateRAGAS(retrieval_augmented_qa_chain_paged,\n",
    "                                                       test_questions, test_groundtruths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'faithfulness': 0.9146, 'answer_relevancy': 0.9240, 'context_recall': 0.8700, 'context_precision': 0.9006}\n"
     ]
    }
   ],
   "source": [
    "print(paged_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>paged</th>\n",
       "      <th>chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>faithfulness</td>\n",
       "      <td>0.914559</td>\n",
       "      <td>0.899927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>answer_relevancy</td>\n",
       "      <td>0.923959</td>\n",
       "      <td>0.947485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>context_recall</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.901667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>context_precision</td>\n",
       "      <td>0.900556</td>\n",
       "      <td>0.908333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Metric     paged    chunks\n",
       "0       faithfulness  0.914559  0.899927\n",
       "1   answer_relevancy  0.923959  0.947485\n",
       "2     context_recall  0.870000  0.901667\n",
       "3  context_precision  0.900556  0.908333"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chunked = pd.DataFrame(list(results.items()), columns=['Metric', 'chunks'])\n",
    "df_paged = pd.DataFrame(list(paged_results.items()), columns=['Metric', 'paged'])\n",
    "df_merged = pd.merge(df_paged, df_chunked, on='Metric')\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 Deliverables:\n",
    "1.  Assess my pipeline using the RAGAS framework including key metrics faithfulness, answer relevancy, context precision, and context recall.  Provide a table of my output results.<br>\n",
    "\n",
    "I did the evaluation of my prototype model which was based on the PDF documents being divided by pages, but while I was here, I compared this with the other strategy, which was to recombine all the text and then split by chunks with overlap.\n",
    "\n",
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Metric</th>\n",
    "      <th>paged</th>\n",
    "      <th>chunks</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>faithfulness</td>\n",
    "      <td>0.873630</td>\n",
    "      <td>0.907450</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>answer_relevancy</td>\n",
    "      <td>0.944051</td>\n",
    "      <td>0.963378</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>context_recall</td>\n",
    "      <td>0.850000</td>\n",
    "      <td>0.926667</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>context_precision</td>\n",
    "      <td>0.905000</td>\n",
    "      <td>0.906111</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "It is obvious that the paged approach, using the default for the PDF loader, is INFERIOR and I should alter my app to use the chunking strategy.  In Task 5, I will compare the fine tuned embedding model with the OpenAI embedding model, but with the chunking strategy in both instances.  There is no point in re-studying the paged strategy.\n",
    "\n",
    "2.  What conclusions can I draw about performance and effectiveness of my pipeline with this information? <br>\n",
    "\n",
    "First, the comparison of chunking strategies is clearcut - dividing PDF documents by pages and embedding those pages is very inferior to combining all the PDF text into one document and then splitting that with chunk overlap.\n",
    "\n",
    "- Faithfulness: Measures whether all claims or statements in the answer can be completely inferred from the context that was provided.  The value is the percentage of claims that can be inferred over the total number of claims.  This metric is improved with a chunking overlap strategy.\n",
    "- Answer relevancy: Measures whether the answer is relevant to the question. It does not matter if the answer is actually correct - only that it directly answers the question without redundancy. This metric was not affected by the chunking strategy.\n",
    "- Context recall: This measures whether the facts that are in the ground truth reference answer can be inferred from the context that was provided to the LLM.  In a perfect situation, every statement in the ground truth should be able to be linked to the context.  This metric was improved with the chunking overlap strategy.\n",
    "- Context precision: This measures whether all the elements in the ground truth are in the highest ranked parts of the context.  This metric was not affected by the chunking strategy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.  Fine-Tuning Open-Source Embeddings\n",
    "(Role: Machine Learning Engineer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I performed the training in a different notebook because I needed to run in Colab.\n",
    "### The notebook is in this GitHub repository and is called FineTuneEmbed.ipynb \n",
    "### The model was then placed in Hugging Face and the link is provided as part of the deliverable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 Deliverables:\n",
    "1.  Swap out my existing embedding model for the new fine tuned version.  Provide a link to m fine-tuned embedding model on the Hugging Face Hub.<br>\n",
    "\n",
    "2.  How did I choose the embedding model for this application?<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5.  Assessing Performance\n",
    "(Role: AI Evaluation and Performance Engineer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5 Deliverables:\n",
    "1.  Test the fine-tuned embedding model using the RAGAS frameworks to quantify any improvements.  Provide results in a table.<br>\n",
    "\n",
    "2.  Test the two chunking strategies using the RAGAS frameworks to quantify any improvements.  Provide results in a table.<br>\n",
    "\n",
    "3.  The AI Solutions Engineer asks me \"Which one is the best to test with internal stakeholders next week, and why?<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6.  Managing Your Boss and User Expectations\n",
    "(Role: SVP of Technology)\n",
    "\n",
    "## Task 6 Deliverables:\n",
    "1.  What is the story that I will give to the CEO to tell the whole company at the launch next month?<br>\n",
    "\n",
    "2.  There appears to be important information not included in our build.  How might we incorporate relevant white-house briefing information in future versions? <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIE4a",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

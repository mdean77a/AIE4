{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Midterm Challenge Notebook - Mike Dean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU langchain langchain_openai langchain_core==0.2.40 langchain_community\n",
    "!pip install -qU qdrant_client pymupdf tiktoken ragas pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.  Dealing with the Data\n",
    "(Role: AI Solutions Engineer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import defaults\n",
    "llm = defaults.default_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method count of list object at 0x336d94740>\n"
     ]
    }
   ],
   "source": [
    "# Load PDF documents from a directory\n",
    "import loadReferenceDocuments\n",
    "separate_pages, one_document = loadReferenceDocuments.loadReferenceDocuments(\"References/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking Strategies\n",
    "#### Ingest the PDF by page - page_split\n",
    "#### Ingest the PDF by page and recombine into single file and then chunk - chunk_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import splitAndVectorize\n",
    "\n",
    "page_split_vectorstore = splitAndVectorize.createVectorstore(\n",
    "    separate_pages,\n",
    "    \"separate_page_collection\",\n",
    ")\n",
    "\n",
    "page_split_retriever = page_split_vectorstore.as_retriever()\n",
    "\n",
    "chunk_split_vectorstore = splitAndVectorize.createVectorstore(\n",
    "    one_document,\n",
    "    \"chunk_split_collection\",\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=400,\n",
    ")\n",
    "\n",
    "chunk_split_retriever = chunk_split_vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 Deliverables:\n",
    "1.  Describe the default chunking strategy that I will use:<br>\n",
    "The default strategy will be to load the two PDF files using `PyMuPDFLoader` just as we have previously done.  This results in each PDF page being its own document.  I have checked sample pages with `tiktoken` and the token count per page is <1000, so these are small enough to just embed without further splitting. I saved these embeddings in `page_split_vectorstore`.\n",
    "\n",
    "2.  Articulate a chunking strategy that I will also test:<br>\n",
    "The disadvantage of the default strategy is that there is no chunk overlapping between the pages, and this might worsen the ability connect two pages that are both relevant to a query.  So I  recombine the page_content of all pages into a single string, convert it into a document, and split it with a chunk size of 800 and an overlap of 400 (the default settings used by OpenAI).  This strategy allows chunks to overlap, pehaps adding semantic continuity between adjacent pages.  These were embedded with the same embedding model and saved in `chunk_split_vectorstore`.\n",
    "\n",
    "3.  Describe how and why I made these decisions:<br>\n",
    "The default behavior of `PyMuPDFLoader` is not bad and I have been using it for several months.  However, I was splitting each of the documents created, not thinking through that if I had chunk sizes greater than the page itself, this was meaningless.  I also had chunk overlap, but had not thought through the implications of each page being a separate document.  So I made these decision for this Midterm Challenge so I can later compare the performance using RAGAS in Task 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.  Building a Quick End-to-End Prototype\n",
    "(Role: AI Systems Engineer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prompts\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from operator import itemgetter\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(prompts.rag_prompt_template)\n",
    "\n",
    "page_split_rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | page_split_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | rag_prompt | llm | StrOutputParser()\n",
    ")\n",
    "\n",
    "chunk_split_rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | chunk_split_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | rag_prompt | llm | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on the provided context, the major risks unique to or exacerbated by Generative AI (GAI) include:\n",
       "\n",
       "1. Confabulation\n",
       "2. Dangerous or Violent Recommendations\n",
       "3. Data Privacy\n",
       "4. Value Chain and Component Integration\n",
       "5. Harmful Bias\n",
       "6. Homogenization\n",
       "7. CBRN Information or Capabilities (Chemical, Biological, Radiological, and Nuclear)\n",
       "8. Human-AI Configuration\n",
       "9. Obscene, Degrading, and/or Abusive Content\n",
       "10. Information Integrity"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Some risks of AI include:\n",
       "\n",
       "1. **Confabulation:** The production of confidently stated but erroneous or false content that can mislead or deceive users.\n",
       "2. **Dangerous, Violent, or Hateful Content:** Eased production and access to violent, inciting, radicalizing, or threatening content, including recommendations to carry out self-harm or illegal activities.\n",
       "3. **Data Privacy:** Leakage and unauthorized use, disclosure, or de-anonymization of personally identifiable information or sensitive data.\n",
       "4. **Environmental Impacts:** High compute resource utilization in training or operating AI models that may adversely impact ecosystems.\n",
       "5. **Harmful Bias or Homogenization:** Amplification and exacerbation of historical, societal, and systemic biases; performance disparities between sub-groups or languages due to non-representative training data.\n",
       "6. **Malicious Use:** Eased access to or synthesis of nefarious information or design capabilities related to dangerous materials or agents, such as chemical, biological, radiological, or nuclear weapons.\n",
       "7. **Human-AI Configuration:** Risks arising from the interaction between humans and AI systems, including abuse, misuse, and unsafe repurposing by humans."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "## TEST THE TWO CHAINS\n",
    "page_response = (page_split_rag_chain.invoke({\"question\": \"List the ten major risks of AI?\"}))\n",
    "display(Markdown(page_response))\n",
    "\n",
    "chunk_response = (chunk_split_rag_chain.invoke({\"question\": \"What are some risks of AI?\"}))\n",
    "display(Markdown(chunk_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 Deliverables:\n",
    "1.  Build a live public prototype on Hugging Face, and include the public URL link to my space.<br>\n",
    "\n",
    "Here is a one minute Loom video that demonstrates the prototype running in Hugging Face.\n",
    "https://www.loom.com/share/70b741d3e4e14af792572b3aa9106463?sid=5aeb2e51-0f75-4c74-9f8c-6bc6d14aaa52\n",
    "\n",
    "I used the page split retriever for this prototype, meaning that the documents were broken by page, not recombined, and were chunked as whole pages without overlap.\n",
    "\n",
    "2.  How did I choose my stack, and why did I select each tool the way it Did? <br>\n",
    "\n",
    "My stack consists of the following:\n",
    "- Hardware is Apple Mac Studio\n",
    "- Editor is VSC as recommended, and I have grown to like it very much  because it includes everything.\n",
    "- Qdrant is the vector store.  I have used FAISS and Chroma, but Qdrant is fantastic.  I run it locally as a server, though for this Hugging Face situation, I am using a memory-based implementation.\n",
    "- ChainLit is the interface, as recommended.  Notably, the current version of ChainLit does NOT work on Hugging Face, and the version needs to be locked (chainlit==0.7.700).\n",
    "- RAGAS is part of my stack for purposes of later evaluation.\n",
    "- LangChain is used to help simplify the code.\n",
    "\n",
    "I learned an important lesson about software engineering - notebooks are NOT good ways to organize!  They are great for teaching.  So I reverted to my last 30 years of software development, and started refactoring code OUT OF THE NOTEBOOK, and then calling it inside the notebook.  So you (or others) can use the notebook as a navigation tool, but it doesn't become so unwieldly that you can't figure anything out.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.  Creating a Golden Test Data Set\n",
    "(Role: AI Evaluation and Performance Engineer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "318"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import splitAndVectorize\n",
    "# create a new splitting without embedding\n",
    "\n",
    "eval_documents = splitAndVectorize.split_into_chunks(\n",
    "    one_document,\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "len(eval_documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following code was used to create tests but I have commented out to avoid repetition.\n",
    "## I created 50 pairs instead of 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "import defaults\n",
    "\n",
    "## llm and embedding models were set earlier\n",
    "# generator_llm = llm\n",
    "# critic_llm = llm\n",
    "# embeddings = defaults.default_embedding_model\n",
    "\n",
    "# generator = TestsetGenerator.from_langchain(\n",
    "#     generator_llm,\n",
    "#     critic_llm,\n",
    "#     embeddings\n",
    "# )\n",
    "\n",
    "# distributions = {\n",
    "#     simple: 0.5,\n",
    "#     multi_context: 0.4,\n",
    "#     reasoning: 0.1\n",
    "# }\n",
    "\n",
    "# num_qa_pairs = 50 # I increased this from 20 because I have a surplus of OpenAI credits\n",
    "\n",
    "## I ALREADY RAN THIS SO HAVE COMMENTED IT OUT HERE SO I DON'T DO IT AGAIN\n",
    "## I WILL READ IN THE CSV FILE TO CONTINUE\n",
    "# testset = generator.generate_with_langchain_docs(eval_documents, num_qa_pairs, distributions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the test data from the stored CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ IN THE TESTSET FROM THE CSV FILE\n",
    "import pandas as pd\n",
    "test_df = pd.read_csv(\"testset.csv\")\n",
    "test_questions = test_df[\"question\"].values.tolist()\n",
    "test_groundtruths = test_df[\"ground_truth\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "retrieval_augmented_qa_chain_chunk = (\n",
    "    {\"context\": itemgetter(\"question\") | chunk_split_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | llm, \"context\": itemgetter(\"context\")}\n",
    ")\n",
    "retrieval_augmented_qa_chain_paged = (\n",
    "    {\"context\": itemgetter(\"question\") | page_split_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | llm, \"context\": itemgetter(\"context\")}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7aa5f57ae67445b93ba7a9d3e472d7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No statements were generated from the answer.\n"
     ]
    }
   ],
   "source": [
    "import evaluateRAGAS\n",
    "results = evaluateRAGAS.evaluateRAGAS(retrieval_augmented_qa_chain_chunk,\n",
    "                                                       test_questions, test_groundtruths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'faithfulness': 0.8993, 'answer_relevancy': 0.9060, 'context_recall': 0.9150, 'context_precision': 0.8983}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d268e8adf7e1497a9cf7da9d42b0d1ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No statements were generated from the answer.\n"
     ]
    }
   ],
   "source": [
    "import evaluateRAGAS\n",
    "paged_results = evaluateRAGAS.evaluateRAGAS(retrieval_augmented_qa_chain_paged,\n",
    "                                                       test_questions, test_groundtruths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'faithfulness': 0.8705, 'answer_relevancy': 0.9228, 'context_recall': 0.8600, 'context_precision': 0.8933}\n"
     ]
    }
   ],
   "source": [
    "print(paged_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>separate_pages</th>\n",
       "      <th>total_chunked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>faithfulness</td>\n",
       "      <td>0.870491</td>\n",
       "      <td>0.899317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>answer_relevancy</td>\n",
       "      <td>0.922842</td>\n",
       "      <td>0.905958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>context_recall</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.915000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>context_precision</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.898333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Metric  separate_pages  total_chunked\n",
       "0       faithfulness        0.870491       0.899317\n",
       "1   answer_relevancy        0.922842       0.905958\n",
       "2     context_recall        0.860000       0.915000\n",
       "3  context_precision        0.893333       0.898333"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chunked = pd.DataFrame(list(results.items()), columns=['Metric', 'total_chunked'])\n",
    "df_paged = pd.DataFrame(list(paged_results.items()), columns=['Metric', 'separate_pages'])\n",
    "df_merged = pd.merge(df_paged, df_chunked, on='Metric')\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 Deliverables:\n",
    "1.  Assess my pipeline using the RAGAS framework including key metrics faithfulness, answer relevancy, context precision, and context recall.  Provide a table of my output results.<br>\n",
    "\n",
    "I did the evaluation of my prototype model which was based on the PDF documents being divided by pages, but while I was here, I compared this with the other strategy, which was to recombine all the text and then split by chunks with overlap.\n",
    "\n",
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Metric</th>\n",
    "      <th>separate_pages</th>\n",
    "      <th>total_chunked</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>faithfulness</td>\n",
    "      <td>0.870491</td>\n",
    "      <td>0.899317</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>answer_relevancy</td>\n",
    "      <td>0.922842</td>\n",
    "      <td>0.905958</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>context_recall</td>\n",
    "      <td>0.860000</td>\n",
    "      <td>0.915000</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>context_precision</td>\n",
    "      <td>0.893333</td>\n",
    "      <td>0.898333</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "When the PDF document is separated by page, and then those pages are INDIVIDUALLY chunked or embedded, the context recall is somewhat less, but other parameters are not really striking.  Both strategies need to be assessed later when we use the finetuned embedding model.\n",
    "\n",
    "2.  What conclusions can I draw about performance and effectiveness of my pipeline with this information? <br>\n",
    "\n",
    "- Faithfulness: Measures whether all claims or statements in the answer can be completely inferred from the context that was provided.  The value is the percentage of claims that can be inferred over the total number of claims.  \n",
    "- Answer relevancy: Measures whether the answer is relevant to the question. It does not matter if the answer is actually correct - only that it directly answers the question without redundancy. \n",
    "- Context recall: This measures whether the facts that are in the ground truth reference answer can be inferred from the context that was provided to the LLM.  In a perfect situation, every statement in the ground truth should be able to be linked to the context.  \n",
    "- Context precision: This measures whether all the elements in the ground truth are in the highest ranked parts of the context.  \n",
    "\n",
    "**OVERALL CONCLUSION:**<br> \n",
    "Not really any significant differences here, though I suspect that letting the pages be kept as separate documents is going to be inferior in the long run.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.  Fine-Tuning Open-Source Embeddings\n",
    "(Role: Machine Learning Engineer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I performed the fine tuning in a separate notebook (FineTunePartTwo.ipynb) that you can find in this location:\n",
    "https://github.com/mdean77a/AIE4/blob/main/Midterm/FineTunePartTwo.ipynb\n",
    "\n",
    "I did this separately because I anticipated needing to use Colab.  To my utter surprise, the training actually worked on my Mac Studio (M1 32 gb, 323 sec) and I could reproduce it on M3 laptop (128 gb, 106 sec).  So I didn't end up having to wrestle with Colab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 Deliverables:\n",
    "1.  Swap out my existing embedding model for the new fine tuned version.  Provide a link to m fine-tuned embedding model on the Hugging Face Hub.<br>\n",
    "\n",
    "https://huggingface.co/Mdean77/finetuned_arctic\n",
    "\n",
    "2.  How did I choose the embedding model for this application?<br>\n",
    "\n",
    "I selected Snowflake/snowflake-arctic-embed-m because it improved dramatically in our previous exercise with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5.  Assessing Performance\n",
    "(Role: AI Evaluation and Performance Engineer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"Mdean77/finetuned_arctic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "finetune_embeddings = HuggingFaceEmbeddings(model_name=\"Mdean77/finetuned_arctic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import splitAndVectorize\n",
    "\n",
    "# page_split_vectorstore was created earlier, and uses te3 embedder.\n",
    "page_split_vectorstore = splitAndVectorize.createVectorstore(\n",
    "    separate_pages,\n",
    "    \"separate_page_collection\",\n",
    ")\n",
    "\n",
    "arctic_page_split_vectorstore = splitAndVectorize.createVectorstore(\n",
    "    separate_pages,\n",
    "    \"arctic_separate_page_collection\",\n",
    "    embedding_model=finetune_embeddings,\n",
    ")\n",
    "\n",
    "#need to RESPLIT the chunk split for testing \n",
    "chunk_split_vectorstore = splitAndVectorize.createVectorstore(\n",
    "    one_document,\n",
    "    \"te3_chunk_split_collection\",\n",
    "    chunk_size=250,\n",
    "    chunk_overlap=50,\n",
    "    embedding_model=finetune_embeddings,\n",
    ")\n",
    "\n",
    "# now make chunk split with arctic\n",
    "arctic_chunk_split_vectorstore = splitAndVectorize.createVectorstore(\n",
    "    one_document,\n",
    "    \"arctic_chunk_split_collection\",\n",
    "    chunk_size=250,\n",
    "    chunk_overlap=50,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HERE ARE OUR RETRIEVERS\n",
    "\n",
    "te3_page_split_retriever = page_split_vectorstore.as_retriever()\n",
    "arctic_page_split_retriever = arctic_page_split_vectorstore.as_retriever()\n",
    "te3_chunk_split_retriever = chunk_split_vectorstore.as_retriever()\n",
    "arctic_chunk_split_retriever = arctic_chunk_split_vectorstore.as_retriever()\n",
    "\n",
    "## HERE ARE OUR RETRIEVAL CHAINS\n",
    "\n",
    "te3_chunk_split_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | te3_chunk_split_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | llm, \"context\": itemgetter(\"context\")}\n",
    ")\n",
    "te3_page_split_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | te3_page_split_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | llm, \"context\": itemgetter(\"context\")}\n",
    ")\n",
    "arctic_chunk_split_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | arctic_chunk_split_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | llm, \"context\": itemgetter(\"context\")}\n",
    ")\n",
    "arctic_page_split_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | arctic_page_split_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | llm, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91bb883c43a0472f82465f10759d753c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No statements were generated from the answer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46f6f9dd8a4c4e1397ed2c4877f682b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No statements were generated from the answer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a729737bd9642318083bbd8d704adac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No statements were generated from the answer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "961cdc8463d740c4828f57b3b99163a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No statements were generated from the answer.\n"
     ]
    }
   ],
   "source": [
    "### NOW RUN OUR EVALUATIONS!\n",
    "te3_chunk_results = evaluateRAGAS.evaluateRAGAS(te3_chunk_split_chain, test_questions, test_groundtruths)\n",
    "te3_page_results = evaluateRAGAS.evaluateRAGAS(te3_page_split_chain, test_questions, test_groundtruths)\n",
    "arctic_chunk_results = evaluateRAGAS.evaluateRAGAS(arctic_chunk_split_chain, test_questions, test_groundtruths)\n",
    "arctic_page_results = evaluateRAGAS.evaluateRAGAS(arctic_page_split_chain, test_questions, test_groundtruths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>TE3 paged</th>\n",
       "      <th>TE3 chunked</th>\n",
       "      <th>ARCTIC paged</th>\n",
       "      <th>ARCTIC chunked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>faithfulness</td>\n",
       "      <td>0.885625</td>\n",
       "      <td>0.870754</td>\n",
       "      <td>0.900033</td>\n",
       "      <td>0.900109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>answer_relevancy</td>\n",
       "      <td>0.961167</td>\n",
       "      <td>0.909941</td>\n",
       "      <td>0.909930</td>\n",
       "      <td>0.947806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>context_recall</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.831667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>context_precision</td>\n",
       "      <td>0.898889</td>\n",
       "      <td>0.874444</td>\n",
       "      <td>0.875556</td>\n",
       "      <td>0.862222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Metric  TE3 paged  TE3 chunked  ARCTIC paged  ARCTIC chunked\n",
       "0       faithfulness   0.885625     0.870754      0.900033        0.900109\n",
       "1   answer_relevancy   0.961167     0.909941      0.909930        0.947806\n",
       "2     context_recall   0.850000     0.825000      0.870000        0.831667\n",
       "3  context_precision   0.898889     0.874444      0.875556        0.862222"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a table of results!\n",
    "df_te3_paged = pd.DataFrame(list(te3_page_results.items()), columns=['Metric', \"TE3 paged\"])\n",
    "df_te3_chunked = pd.DataFrame(list(te3_chunk_results.items()), columns=['Metric', 'TE3 chunked'])\n",
    "df_arctic_paged = pd.DataFrame(list(arctic_page_results.items()), columns=['Metric', 'ARCTIC paged'])\n",
    "df_arctic_chunked = pd.DataFrame(list(arctic_chunk_results.items()), columns=['Metric', 'ARCTIC chunked'])\n",
    "df_merged = df_te3_paged.merge(df_te3_chunked, on='Metric').merge(df_arctic_paged, on='Metric').merge(df_arctic_chunked, on='Metric')\n",
    "\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5 Deliverables:\n",
    "1.  Test the fine-tuned embedding model using the RAGAS frameworks to quantify any improvements.  Provide results in a table.<br>\n",
    "2.  Test the two chunking strategies using the RAGAS frameworks to quantify any improvements.  Provide results in a table.<br>\n",
    "The tables are combined and shown here:\n",
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Metric</th>\n",
    "      <th>TE3 paged</th>\n",
    "      <th>TE3 chunked</th>\n",
    "      <th>ARCTIC paged</th>\n",
    "      <th>ARCTIC chunked</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>faithfulness</td>\n",
    "      <td>0.885625</td>\n",
    "      <td>0.870754</td>\n",
    "      <td>0.900033</td>\n",
    "      <td>0.900109</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>answer_relevancy</td>\n",
    "      <td>0.961167</td>\n",
    "      <td>0.909941</td>\n",
    "      <td>0.909930</td>\n",
    "      <td>0.947806</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>context_recall</td>\n",
    "      <td>0.850000</td>\n",
    "      <td>0.825000</td>\n",
    "      <td>0.870000</td>\n",
    "      <td>0.831667</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>context_precision</td>\n",
    "      <td>0.898889</td>\n",
    "      <td>0.874444</td>\n",
    "      <td>0.875556</td>\n",
    "      <td>0.862222</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous results earlier in notebook with different chunking (using the TE3):\n",
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Metric</th>\n",
    "      <th>separate_pages</th>\n",
    "      <th>total_chunked</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>faithfulness</td>\n",
    "      <td>0.870491</td>\n",
    "      <td>0.899317</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>answer_relevancy</td>\n",
    "      <td>0.922842</td>\n",
    "      <td>0.905958</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>context_recall</td>\n",
    "      <td>0.860000</td>\n",
    "      <td>0.915000</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>context_precision</td>\n",
    "      <td>0.893333</td>\n",
    "      <td>0.898333</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.  The AI Solutions Engineer asks me \"Which one is the best to test with internal stakeholders next week, and why?<br>\n",
    "\n",
    "I have included the results from the four way comparison, but also brought down the chunking comparison that I did earlier for Task 3.  It is striking that there is variability.  Of these four metrics, the faithfulness metric is probably the most important, and the finetuned Snowflake outperforms TE3.  My gestalt from looking at these tables, and also common sense, suggests that combining the PDF pages and THEN chunking them is sensible.  I may have compromised performance here compared with the separate pages because I made the chunks small - the separate pages were embedded together and have more content.  \n",
    "\n",
    "If you compare the TE3 chunked with the ARCTIC chunked, the ARCITC model is clearly better.  So my recommendation is that we use the finetuned model, but that we experiment with chunk sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecab3084cd424b9f9628c8768d41ad3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No statements were generated from the answer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da9a2116c93b4f1d8796f31923ae9e34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No statements were generated from the answer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "accccd2120f84b44af285533c9904822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No statements were generated from the answer.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>600/100</th>\n",
       "      <th>800/400</th>\n",
       "      <th>1200/400</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>faithfulness</td>\n",
       "      <td>0.892048</td>\n",
       "      <td>0.931814</td>\n",
       "      <td>0.883423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>answer_relevancy</td>\n",
       "      <td>0.935133</td>\n",
       "      <td>0.931497</td>\n",
       "      <td>0.951927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>context_recall</td>\n",
       "      <td>0.915000</td>\n",
       "      <td>0.915000</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>context_precision</td>\n",
       "      <td>0.882778</td>\n",
       "      <td>0.908333</td>\n",
       "      <td>0.901111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Metric   600/100   800/400  1200/400\n",
       "0       faithfulness  0.892048  0.931814  0.883423\n",
       "1   answer_relevancy  0.935133  0.931497  0.951927\n",
       "2     context_recall  0.915000  0.915000  0.920000\n",
       "3  context_precision  0.882778  0.908333  0.901111"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 800 with 400 overlap\n",
    "arctic_800_400_chunks_vectorstore = splitAndVectorize.createVectorstore(\n",
    "    one_document,\n",
    "    \"arctic_800_400_chunk_collection\",\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=400,\n",
    ")\n",
    "arctic_800_400_chunk_retriever = arctic_800_400_chunks_vectorstore.as_retriever()\n",
    "\n",
    "arctic_800_400_chunk_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | arctic_800_400_chunk_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | llm, \"context\": itemgetter(\"context\")}\n",
    ")\n",
    "arctic_800_400_chunk_results = evaluateRAGAS.evaluateRAGAS(arctic_800_400_chunk_chain, test_questions, test_groundtruths)\n",
    "\n",
    "## 1200 with 400 overlap\n",
    "arctic_1200_400_chunks_vectorstore = splitAndVectorize.createVectorstore(\n",
    "    one_document,\n",
    "    \"arctic_1200_400_chunk_collection\",\n",
    "    chunk_size=1200,\n",
    "    chunk_overlap=400,\n",
    ")\n",
    "arctic_1200_400_chunk_retriever = arctic_1200_400_chunks_vectorstore.as_retriever()\n",
    "\n",
    "arctic_1200_400_chunk_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | arctic_1200_400_chunk_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | llm, \"context\": itemgetter(\"context\")}\n",
    ")\n",
    "arctic_1200_400_chunk_results = evaluateRAGAS.evaluateRAGAS(arctic_1200_400_chunk_chain, test_questions, test_groundtruths)\n",
    "\n",
    "# 600 with 100 overlap\n",
    "arctic_600_100_chunks_vectorstore = splitAndVectorize.createVectorstore(\n",
    "    one_document,\n",
    "    \"arctic_600_100_chunk_collection\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "arctic_600_100_chunk_retriever = arctic_600_100_chunks_vectorstore.as_retriever()\n",
    "\n",
    "arctic_600_100_chunk_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | arctic_600_100_chunk_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | llm, \"context\": itemgetter(\"context\")}\n",
    ")\n",
    "arctic_600_100_chunk_results = evaluateRAGAS.evaluateRAGAS(arctic_600_100_chunk_chain, test_questions, test_groundtruths)\n",
    "\n",
    "# Make a table of results!\n",
    "df_600_100 = pd.DataFrame(list(arctic_600_100_chunk_results.items()), columns=['Metric', \"600/100\"])\n",
    "df_800_400 = pd.DataFrame(list(arctic_800_400_chunk_results.items()), columns=['Metric', '800/400'])\n",
    "df_1200_400 = pd.DataFrame(list(arctic_1200_400_chunk_results.items()), columns=['Metric', '1200/400'])\n",
    "\n",
    "df_merged = df_600_100.merge(df_800_400, on='Metric').merge(df_1200_400, on='Metric')\n",
    "\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Larger Chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>faithfulness</td>\n",
       "      <td>0.872457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>answer_relevancy</td>\n",
       "      <td>0.908182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>context_recall</td>\n",
       "      <td>0.845000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>context_precision</td>\n",
       "      <td>0.863333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Metric  Larger Chunks\n",
       "0       faithfulness       0.872457\n",
       "1   answer_relevancy       0.908182\n",
       "2     context_recall       0.845000\n",
       "3  context_precision       0.863333"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6.  Managing Your Boss and User Expectations\n",
    "(Role: SVP of Technology)\n",
    "\n",
    "## Task 6 Deliverables:\n",
    "1.  What is the story that I will give to the CEO to tell the whole company at the launch next month?<br>\n",
    "\n",
    "2.  There appears to be important information not included in our build.  How might we incorporate relevant white-house briefing information in future versions? <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIE4a",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{"questions": {"8b7a302c-a4ec-4f28-bae6-499a5b2a6b11": "What processes are involved in the training of the pre-trained model for the GAI application, including details on hyperparameters and training duration?", "86d2dfda-e7e8-4bb8-9dfa-9f1639486e82": "How does the GAI application implement content filters to prevent the generation of harmful or inappropriate content?", "42bb3e02-fc47-42c8-9dd4-08c3bca810fb": "What processes should be implemented for real-time monitoring of generated content performance and trustworthiness characteristics?", "fb771c2f-465d-40c6-96fc-e68e113fb208": "How should organizations leverage feedback from boards or committees when deploying GAI applications and using third-party pre-trained models?", "cb8754db-bfa4-4ab7-b476-f6ad54d81657": "What are the key components included in the post-deployment AI system monitoring plans as outlined in the context?", "a3d8f573-a448-4db4-98e3-fc36d565bab2": "How can organizations evaluate the effectiveness of their processes for monitoring GAI systems, particularly concerning confabulation and cyber risks?", "69bc039a-e03b-4002-84f2-63db5745962d": "How can sentiment analysis be utilized to assess user sentiment regarding the performance and impact of GAI content?", "1923141e-14f3-4371-b76b-c094a77b58ee": "What steps should be taken to ensure transparency and accountability in updating the GAI system, as outlined in the context?", "11dfeee4-1a76-4573-9e5f-3fbc5d68e1ab": "What are the suggested actions for managing GAI risks according to the context provided?", "fd024be9-d709-4c3f-8ca7-d3eb952d05ef": "How can regular engagement with interested parties contribute to the continual improvement of AI systems?", "a7071a77-1c6d-4ddf-bd83-85a0a561f8a8": "What methods can be used to help non-technical stakeholders understand GAI system functionality?", "84cc79b6-97de-4925-bee5-eca03c76b370": "How should incidents and errors related to GAI systems be communicated to relevant AI Actors and affected communities?", "5714fbc0-ec3e-40d8-9a1b-0f4d5e852fbb": "What are the primary considerations derived from the GAI PWG consultation process?", "467f9fd0-ff2a-418e-8a9f-97654c33824f": "What legal and regulatory requirements must be followed when reporting GAI incidents?", "1b6e7cbc-4fcc-481a-92df-95ae0449bfd1": "What are some of the key considerations for governance in generative AI systems as mentioned in the context?", "5360730b-ec0d-435f-b729-5ec6f500a5bc": "How might organizations choose to manage risks associated with generative AI models according to the provided information?", "220b3fa9-853e-46f3-975e-554663872deb": "What are some activities that AI Actors may engage in when interacting with GAI systems?", "38ee11b6-296d-4700-bd96-1c575379b248": "How can organizations manage the risks associated with the use of GAI systems?", "69f57ccb-9c79-46c5-bcdd-de025b65e7a4": "What are some key considerations for establishing acceptable use policies for GAI in human-AI teaming settings?", "a098d2a4-1f2f-418e-bebb-b3a618f5b449": "How can organizations ensure compliance with data protection and retention practices when using third-party AI solutions?", "09348bea-d067-4e9c-8a66-a36f27264767": "What are some implications of using third-party GAI models and systems for an organization's functions?", "fea7861d-329d-4c3e-8b80-cf7470be56ff": "Why is it important for organizations to establish clear guidelines for transparency and risk management when integrating third-party GAI tools?", "78b5e899-7e0e-4e73-b100-46a664c2a5d4": "What are the key components involved in the pre-deployment testing of GAI systems as mentioned in the context?", "100c4276-c5ae-42d8-8cdd-ec00e1f3de54": "How do current pre-deployment TEVV processes for GAI applications fall short according to the provided context?", "de745b4d-87cb-4200-90b2-f9172ee60bbf": "What are some limitations of current pre-deployment testing methodologies for GAI applications?", "3c5dbedf-9414-4d83-9fe3-ecd77fee7b91": "How do measurement gaps between laboratory conditions and real-world settings affect the assessment of GAI systems?", "72a710fb-7cea-4954-8dd2-7926cc0b5a2f": "What are some examples of structured public feedback methods used to evaluate GAI systems?", "21a01829-aa88-4c61-a4ac-49e210875d9f": "How can information gathered from structured public feedback influence the design and deployment of GAI systems?", "dc742e0f-251e-42f0-836b-ca3a21e5abf1": "What are some purposes served by the results and insights from approval, maintenance, or decommissioning decisions?", "5427cd7c-5b45-406c-914d-226b971bd0fe": "How do participatory engagement methods differ from field testing in the context of product development?", "67bc043e-8565-4a86-a888-a3d17fc1b265": "What are the key practices organizations should follow when collecting user feedback on AI models in production environments?", "aabcb88e-c6ad-4a69-8a57-edcedc7676b9": "How does the diversity of an AI red team contribute to the effectiveness of identifying flaws in AI models?", "bbc65ec7-0c2d-4d4e-a1be-92b6ddb9bd6b": "What are the key characteristics that AI red teams should possess to effectively identify flaws in generative AI (GAI) contexts?", "1e35699e-fa2f-4a13-8649-2aacd0821519": "How do the different types of AI red-teaming, such as general public, expert, and combination, vary in their approach and effectiveness?", "2479b546-9fcd-4bc6-8131-843ead2935b6": "What are the potential benefits of combining expert and general public participants in AI red-teaming exercises?", "605b536f-89b7-42dc-a5f0-1bd876d3f36e": "How can provenance data tracking assist in managing the risks associated with AI-generated synthetic content?", "15385ecf-488d-46ce-b39a-5eabebda88d8": "What are some techniques used for provenance data tracking in GAI systems?", "90717170-aa44-4153-b0dd-02c25ae552cf": "How can provenance data tracking and synthetic content detection enhance trust in AI systems?", "77f86fb3-90a1-4a0e-83d1-d83cdae43915": "What are the key techniques used in provenance data tracking for GAI systems?", "a9f08f99-399b-45cc-bc65-96f7dbe0b8e3": "How can provenance data tracking assist AI actors in managing the trade-offs of model decisions?", "809405ab-ace2-4131-9f1b-b9598e323025": "What are the limitations of provenance data in GAI systems as mentioned in the context?", "80e9333f-3c32-4bc6-afd7-fa21b6486af4": "How can organizations enhance content provenance through structured public feedback according to the provided information?", "8789e17e-e9f3-4c94-ba8a-76afe1aaa841": "What are some methods organizations can use to capture user feedback before and after the deployment of GAI systems?", "c0179d93-e957-414f-a6bb-8ae5c034cf56": "How can tracking the provenance of datasets help organizations identify performance issues in GAI systems?", "2aef81c2-be34-49ce-869e-3b9226c49770": "What types of harms can result from the malfunction of AI systems according to the provided context?", "263840a8-62cf-4c03-9be4-bc2fee654b6b": "Why is it important to document and report AI incidents, as mentioned in the context?", "3a45412a-fe9d-47d3-a8f9-4871822d2208": "What role do AI Actors play in the reporting of GAI incidents according to the provided context?", "9a41939c-268f-409a-880f-801339ba208e": "How can documentation practices improve the management of GAI incidents among AI Actors?", "706ccd53-05b5-4e1d-a495-97cce8e317e1": "What is the focus of Acemoglu's 2024 paper titled \"The Simple Macroeconomics of AI\"?", "ee5a81de-fa7a-4645-9386-db2a0bb08f74": "How does the AI Incident Database contribute to understanding incidents related to deepfakes and child safety, as discussed in Atherton's 2024 analysis?", "590735cb-281f-426a-ba7a-3f0cb0457fc4": "What are the main challenges discussed by Boyarskaya et al (2020) in the development and deployment of AI-infused systems?", "9ec6183a-d4b1-47e2-b629-76d6bb13721a": "According to Burgess (2024), what is identified as the biggest security flaw in generative AI, and why is it difficult to address?", "f06b2b76-3d79-408b-a5ef-cb7b8d32bcbe": "What are the main findings of Carlini et al (2021) regarding the extraction of training data from large language models?", "4d95595d-e53b-4384-96d6-1e63da7f8265": "How do Carlini et al (2023) quantify memorization in neural language models?", "63d3720d-4e34-441e-b600-21c79bd30fd7": "What are the ethical tensions discussed in the context of human-AI companionship as per the research by Dahl et al (2024)?", "9e17b619-cfc6-4047-8017-3a95d1c9b543": "How do De Freitas et al (2023) address the safety of generative AI in relation to mental health?", "69b0de33-2521-4b41-8be1-ad75e33daa71": "What is the main focus of the study conducted by Dietvorst et al (2014) regarding algorithm aversion?", "2725c395-0037-46b7-8091-527b3816e228": "How do altered images influence human perception according to the research by Elsayed et al (2024)?", "8879c012-51b3-4ab1-a2f2-145383aa42e8": "What are the main themes discussed in the paper \"Red-Teaming for Generative AI: Silver Bullet or Security Theater?\" by Feffer et al (2024)?", "afde52d1-549b-48ac-8c80-5fc819754835": "How does the Project Naptime report evaluate the offensive security capabilities of large language models?", "415d1501-145e-4dae-a213-83d03a24ef7a": "What are the main concerns addressed in the blog post by NVIDIA regarding securing LLM systems against prompt injection?", "6e160efe-c04c-4078-8be0-91de25313248": "How does the Information Technology Industry Council propose to authenticate AI-generated content in their 2024 policy document?", "0e23b6fa-7ea6-4c35-85aa-88e69cb5f72c": "What are the main themes discussed in the paper \"Algorithm Aversion\" presented at ECIS 2020?", "fb3fc295-84d0-4ac4-afc9-6c6f2024f951": "How do the findings of Khan et al (2024) contribute to understanding the value chain of generative AI?", "d8ea3204-cdc4-4863-8993-bfeb4956944e": "What is the main focus of the paper by Kirchenbauer et al (2023) regarding large language models?", "6561fe5c-0d02-4100-939e-f00710feaf7b": "How do the findings of Liang et al (2023) suggest bias in GPT detectors?", "6c3be403-6fa2-402f-9cbf-482ff04519da": "What are the main findings of Liang et al (2023) regarding GPT detectors and their bias against non-native English writers?", "c2baf46f-8e72-4fda-a2c9-aad063dc6fc8": "How does the National Institute of Standards and Technology's AI Risk Management Framework address the challenges of AI deployment?", "33346723-8749-4ab9-acde-d717188e2dda": "What are the key characteristics of AI risks as outlined in Chapter 3 of the AI Risk Management Framework by the National Institute of Standards and Technology?", "ffad3886-4e45-4d44-83cd-6040bba628e6": "How do AI risks differ from traditional software risks according to Appendix B of the AI Risk Management Framework?", "b434a8fc-9c04-4af1-be6d-b4d7043b7409": "What are the key differences between AI risks and traditional software risks as outlined by the National Institute of Standards and Technology?", "74bdbc52-a5c7-48e8-8d29-15db8bfed68a": "How does the National Institute of Standards and Technology propose to identify and manage bias in artificial intelligence?", "209de317-ce91-408c-8bce-3da0cb10c1f6": "What are the main concerns highlighted by Northcutt et al (2021) regarding label errors in test sets for machine learning benchmarks?", "46805613-0a5c-4c98-82c4-440e38352f2a": "How does the OECD (2023) report address the governance and management of risks associated with AI throughout its lifecycle?", "33e8d810-1601-4229-a958-1a3db89ecfec": "What are the main risks associated with AI deception as discussed in the survey by Park et al (2024)?", "c6110c9f-e22a-48a1-a269-f28560aae985": "How does the Partnership on AI's glossary address the issue of synthetic media transparency methods?", "0235c187-686c-43fb-a140-25b8ae8b80c6": "What are the main themes explored in the study by Said et al (2022) regarding the nonconsensual distribution of intimate images?", "73f1445e-b3e1-4001-8a0d-7108d17a194f": "How do the risks associated with language models differ from those of biological design tools as discussed by Sandbrink (2023)?", "2f96b1c8-897b-4637-960a-59af7d2cc053": "What are the potential risks associated with large language models as discussed in Scheurer et al (2023)?", "7c9ddbb1-b8d7-47ec-8659-4678d9b8a39e": "How do Shelby et al (2023) propose to address the sociotechnical harms of algorithmic systems?", "32845fca-228a-4b4c-8e20-951cd1583e36": "What is the main focus of the paper by Soice et al (2023) regarding large language models and biotechnology?", "d7a49868-33ed-40af-9a78-aba3cf2a3c55": "How do the authors of the paper by Staab et al (2023) address the issue of privacy in relation to large language models?", "09bfed17-991c-444c-a601-60df3a687e10": "What are the main objectives outlined in the White House's Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence?", "84f6b80d-8c94-4690-b2ec-552f825e58dd": "How does the 2022 Roadmap for Researchers address priorities related to information integrity in research and development?", "c39f7648-f06f-43af-9342-3d62a130759b": "What are the emergent challenges of computational agency discussed by Tufekci in the context of algorithmic harms?", "d7da73bf-edac-4f4a-a622-6780976984ae": "How does the paper by Turri et al contribute to our understanding of AI incident documentation practices?", "403e6728-dc6e-4301-a28c-79ca20cefabd": "What is the focus of the dataset created by Wang, Y et al in their 2023 paper titled \"Do-Not-Answer\"?", "cd74dbb2-f275-4536-9222-0273495a8466": "How do Weidinger, L et al (2021) address the ethical and social risks associated with language models in their research?", "09c4cf89-6512-439c-b05f-1d81304dc128": "What are the main risks posed by language models as discussed in the taxonomy provided by Weidinger et al (2022)?", "395abbe5-3265-4978-b1a7-452cdc13572d": "How does AI disproportionately affect women according to the findings presented by West (2023)?", "23dd80fa-c71d-4fa0-a135-4d9354fb4814": "What is the main focus of the research conducted by Yu, Z et al in their March 2024 paper on jailbreak prompts of large language models?", "a5991001-e9eb-434e-843c-0685152d793d": "How do Zhang, Y et al (2023) differentiate between human favoritism and AI aversion in their study on perceptions of generative AI?", "da05bfdf-8fca-4319-b3fe-1135c5f3ed94": "What is the main focus of the paper by Zhao et al (2023) regarding AI-generated text?", "02ed0683-a436-4ada-9e29-a07f42f98335": "Where can the full text of the paper \"Provable Robust Watermarking for AI-Generated Text\" be accessed?"}, "relevant_contexts": {"8b7a302c-a4ec-4f28-bae6-499a5b2a6b11": ["2e23e5b8-c0cb-44bd-aa80-7897cc35b7d6"], "86d2dfda-e7e8-4bb8-9dfa-9f1639486e82": ["2e23e5b8-c0cb-44bd-aa80-7897cc35b7d6"], "42bb3e02-fc47-42c8-9dd4-08c3bca810fb": ["20259b03-a5ea-4d46-8544-11b8628bccc5"], "fb771c2f-465d-40c6-96fc-e68e113fb208": ["20259b03-a5ea-4d46-8544-11b8628bccc5"], "cb8754db-bfa4-4ab7-b476-f6ad54d81657": ["64b83d40-e568-4d71-a887-24b63f12b7f4"], "a3d8f573-a448-4db4-98e3-fc36d565bab2": ["64b83d40-e568-4d71-a887-24b63f12b7f4"], "69bc039a-e03b-4002-84f2-63db5745962d": ["3d830de4-ebb2-49bb-afa2-ff9d785cfd1a"], "1923141e-14f3-4371-b76b-c094a77b58ee": ["3d830de4-ebb2-49bb-afa2-ff9d785cfd1a"], "11dfeee4-1a76-4573-9e5f-3fbc5d68e1ab": ["d0bfebca-7579-4909-8a60-571dbb1e741a"], "fd024be9-d709-4c3f-8ca7-d3eb952d05ef": ["d0bfebca-7579-4909-8a60-571dbb1e741a"], "a7071a77-1c6d-4ddf-bd83-85a0a561f8a8": ["57e512e2-9422-4d02-82cf-2443f78f04b2"], "84cc79b6-97de-4925-bee5-eca03c76b370": ["57e512e2-9422-4d02-82cf-2443f78f04b2"], "5714fbc0-ec3e-40d8-9a1b-0f4d5e852fbb": ["fbc38d20-ea5c-4cf3-8f08-88796c4ebddd"], "467f9fd0-ff2a-418e-8a9f-97654c33824f": ["fbc38d20-ea5c-4cf3-8f08-88796c4ebddd"], "1b6e7cbc-4fcc-481a-92df-95ae0449bfd1": ["7681bd3d-3a51-4e35-aedc-2f3b0a65dd47"], "5360730b-ec0d-435f-b729-5ec6f500a5bc": ["7681bd3d-3a51-4e35-aedc-2f3b0a65dd47"], "220b3fa9-853e-46f3-975e-554663872deb": ["5fd41c5b-b7de-4b29-a9c3-f63c394444bd"], "38ee11b6-296d-4700-bd96-1c575379b248": ["5fd41c5b-b7de-4b29-a9c3-f63c394444bd"], "69f57ccb-9c79-46c5-bcdd-de025b65e7a4": ["9ab3a6d1-1c71-488f-924b-03251264e34c"], "a098d2a4-1f2f-418e-bebb-b3a618f5b449": ["9ab3a6d1-1c71-488f-924b-03251264e34c"], "09348bea-d067-4e9c-8a66-a36f27264767": ["6d6c85c4-1459-42aa-b43b-47e9101702fa"], "fea7861d-329d-4c3e-8b80-cf7470be56ff": ["6d6c85c4-1459-42aa-b43b-47e9101702fa"], "78b5e899-7e0e-4e73-b100-46a664c2a5d4": ["27e40573-8209-4af0-bbb3-6f28c1763720"], "100c4276-c5ae-42d8-8cdd-ec00e1f3de54": ["27e40573-8209-4af0-bbb3-6f28c1763720"], "de745b4d-87cb-4200-90b2-f9172ee60bbf": ["4b118253-73f5-4d2f-a5bb-20007a180f20"], "3c5dbedf-9414-4d83-9fe3-ecd77fee7b91": ["4b118253-73f5-4d2f-a5bb-20007a180f20"], "72a710fb-7cea-4954-8dd2-7926cc0b5a2f": ["1f752613-9ea7-4a38-be21-8629358f58d6"], "21a01829-aa88-4c61-a4ac-49e210875d9f": ["1f752613-9ea7-4a38-be21-8629358f58d6"], "dc742e0f-251e-42f0-836b-ca3a21e5abf1": ["267bc70c-af60-4130-ae3c-eff8c62f021f"], "5427cd7c-5b45-406c-914d-226b971bd0fe": ["267bc70c-af60-4130-ae3c-eff8c62f021f"], "67bc043e-8565-4a86-a888-a3d17fc1b265": ["2cd3e491-f364-42ec-bbb9-d9e1ed272109"], "aabcb88e-c6ad-4a69-8a57-edcedc7676b9": ["2cd3e491-f364-42ec-bbb9-d9e1ed272109"], "bbc65ec7-0c2d-4d4e-a1be-92b6ddb9bd6b": ["1fda80cf-5a80-4496-9648-f7090f004699"], "1e35699e-fa2f-4a13-8649-2aacd0821519": ["1fda80cf-5a80-4496-9648-f7090f004699"], "2479b546-9fcd-4bc6-8131-843ead2935b6": ["aa86012a-fbee-4da1-80cc-81e15992b96d"], "605b536f-89b7-42dc-a5f0-1bd876d3f36e": ["aa86012a-fbee-4da1-80cc-81e15992b96d"], "15385ecf-488d-46ce-b39a-5eabebda88d8": ["ac8b459b-790a-4866-9841-6257ef21d53f"], "90717170-aa44-4153-b0dd-02c25ae552cf": ["ac8b459b-790a-4866-9841-6257ef21d53f"], "77f86fb3-90a1-4a0e-83d1-d83cdae43915": ["ca4f07d3-f982-440f-9cc9-715913922f15"], "a9f08f99-399b-45cc-bc65-96f7dbe0b8e3": ["ca4f07d3-f982-440f-9cc9-715913922f15"], "809405ab-ace2-4131-9f1b-b9598e323025": ["695b2a1b-d6c1-4b2e-b603-97768a59456f"], "80e9333f-3c32-4bc6-afd7-fa21b6486af4": ["695b2a1b-d6c1-4b2e-b603-97768a59456f"], "8789e17e-e9f3-4c94-ba8a-76afe1aaa841": ["402533d2-b3a1-428f-8d88-08c5334920e3"], "c0179d93-e957-414f-a6bb-8ae5c034cf56": ["402533d2-b3a1-428f-8d88-08c5334920e3"], "2aef81c2-be34-49ce-869e-3b9226c49770": ["ff743264-0a73-45b5-b5b0-cd06ccfa72a1"], "263840a8-62cf-4c03-9be4-bc2fee654b6b": ["ff743264-0a73-45b5-b5b0-cd06ccfa72a1"], "3a45412a-fe9d-47d3-a8f9-4871822d2208": ["59c84d6b-2880-4af7-86b9-c2e44718dbeb"], "9a41939c-268f-409a-880f-801339ba208e": ["59c84d6b-2880-4af7-86b9-c2e44718dbeb"], "706ccd53-05b5-4e1d-a495-97cce8e317e1": ["79b77812-212e-49d8-8e86-2d9d61d08cd5"], "ee5a81de-fa7a-4645-9386-db2a0bb08f74": ["79b77812-212e-49d8-8e86-2d9d61d08cd5"], "590735cb-281f-426a-ba7a-3f0cb0457fc4": ["bf5f4bcf-4b7b-4d76-a357-fd487466bfbc"], "9ec6183a-d4b1-47e2-b629-76d6bb13721a": ["bf5f4bcf-4b7b-4d76-a357-fd487466bfbc"], "f06b2b76-3d79-408b-a5ef-cb7b8d32bcbe": ["3122138e-7412-4f4b-bc71-d88ebaa4ae84"], "4d95595d-e53b-4384-96d6-1e63da7f8265": ["3122138e-7412-4f4b-bc71-d88ebaa4ae84"], "63d3720d-4e34-441e-b600-21c79bd30fd7": ["8678e58a-2d82-4434-a790-3e6c9f7ab599"], "9e17b619-cfc6-4047-8017-3a95d1c9b543": ["8678e58a-2d82-4434-a790-3e6c9f7ab599"], "69b0de33-2521-4b41-8be1-ad75e33daa71": ["b8c918f9-c2f1-4631-bb7f-3efc794468c4"], "2725c395-0037-46b7-8091-527b3816e228": ["b8c918f9-c2f1-4631-bb7f-3efc794468c4"], "8879c012-51b3-4ab1-a2f2-145383aa42e8": ["ebe445a1-c461-4db4-95fb-103687c90730"], "afde52d1-549b-48ac-8c80-5fc819754835": ["ebe445a1-c461-4db4-95fb-103687c90730"], "415d1501-145e-4dae-a213-83d03a24ef7a": ["6ef6fb19-5f2f-468e-a721-a5f2846ea0f3"], "6e160efe-c04c-4078-8be0-91de25313248": ["6ef6fb19-5f2f-468e-a721-a5f2846ea0f3"], "0e23b6fa-7ea6-4c35-85aa-88e69cb5f72c": ["0f0292e9-abfc-47dd-8a66-98bb29cfbd3b"], "fb3fc295-84d0-4ac4-afc9-6c6f2024f951": ["0f0292e9-abfc-47dd-8a66-98bb29cfbd3b"], "d8ea3204-cdc4-4863-8993-bfeb4956944e": ["571bf188-4c14-4019-86c2-da21cf200763"], "6561fe5c-0d02-4100-939e-f00710feaf7b": ["571bf188-4c14-4019-86c2-da21cf200763"], "6c3be403-6fa2-402f-9cbf-482ff04519da": ["5764cfeb-0fd1-4adc-acca-9dd8b9830c07"], "c2baf46f-8e72-4fda-a2c9-aad063dc6fc8": ["5764cfeb-0fd1-4adc-acca-9dd8b9830c07"], "33346723-8749-4ab9-acde-d717188e2dda": ["e2bea80e-e224-4dfe-83b7-f02d97917dec"], "ffad3886-4e45-4d44-83cd-6040bba628e6": ["e2bea80e-e224-4dfe-83b7-f02d97917dec"], "b434a8fc-9c04-4af1-be6d-b4d7043b7409": ["1feb87fb-b3c1-4234-8c6d-3ef1256b1213"], "74bdbc52-a5c7-48e8-8d29-15db8bfed68a": ["1feb87fb-b3c1-4234-8c6d-3ef1256b1213"], "209de317-ce91-408c-8bce-3da0cb10c1f6": ["ee942749-b2cf-4fcd-a510-cb6fc2d45e70"], "46805613-0a5c-4c98-82c4-440e38352f2a": ["ee942749-b2cf-4fcd-a510-cb6fc2d45e70"], "33e8d810-1601-4229-a958-1a3db89ecfec": ["04dc30c4-6b82-4fd5-bdd3-f7a253749f3e"], "c6110c9f-e22a-48a1-a269-f28560aae985": ["04dc30c4-6b82-4fd5-bdd3-f7a253749f3e"], "0235c187-686c-43fb-a140-25b8ae8b80c6": ["d81b8671-d240-414b-be39-66ea385ad81f"], "73f1445e-b3e1-4001-8a0d-7108d17a194f": ["d81b8671-d240-414b-be39-66ea385ad81f"], "2f96b1c8-897b-4637-960a-59af7d2cc053": ["63506495-db20-438a-a6c0-2ce4d838dd50"], "7c9ddbb1-b8d7-47ec-8659-4678d9b8a39e": ["63506495-db20-438a-a6c0-2ce4d838dd50"], "32845fca-228a-4b4c-8e20-951cd1583e36": ["7fc9ff90-d884-47ef-a410-9f541792c7dc"], "d7a49868-33ed-40af-9a78-aba3cf2a3c55": ["7fc9ff90-d884-47ef-a410-9f541792c7dc"], "09bfed17-991c-444c-a601-60df3a687e10": ["1b911521-8bfa-4444-b7df-d32ff043bd68"], "84f6b80d-8c94-4690-b2ec-552f825e58dd": ["1b911521-8bfa-4444-b7df-d32ff043bd68"], "c39f7648-f06f-43af-9342-3d62a130759b": ["49d93069-13cf-4f24-b8fc-f79478c143de"], "d7da73bf-edac-4f4a-a622-6780976984ae": ["49d93069-13cf-4f24-b8fc-f79478c143de"], "403e6728-dc6e-4301-a28c-79ca20cefabd": ["4d57797c-0be6-4155-92cd-e7c0a320b899"], "cd74dbb2-f275-4536-9222-0273495a8466": ["4d57797c-0be6-4155-92cd-e7c0a320b899"], "09c4cf89-6512-439c-b05f-1d81304dc128": ["b0244231-06bf-4a3e-8c72-7d8599241910"], "395abbe5-3265-4978-b1a7-452cdc13572d": ["b0244231-06bf-4a3e-8c72-7d8599241910"], "23dd80fa-c71d-4fa0-a135-4d9354fb4814": ["78026411-a3c4-411d-83ca-d9889dfb7adb"], "a5991001-e9eb-434e-843c-0685152d793d": ["78026411-a3c4-411d-83ca-d9889dfb7adb"], "da05bfdf-8fca-4319-b3fe-1135c5f3ed94": ["6e590e51-727a-442b-b43b-24e90e0f935c"], "02ed0683-a436-4ada-9e29-a07f42f98335": ["6e590e51-727a-442b-b43b-24e90e0f935c"]}, "corpus": {"2e23e5b8-c0cb-44bd-aa80-7897cc35b7d6": "present in the data related to the GAI application and its content provenance, \narchitecture, training process of the pre-trained model including information on \nhyperparameters, training duration, and any \ufb01ne-tuning or retrieval-augmented \ngeneration processes applied. \nInformation Integrity; Harmful Bias \nand Homogenization; Intellectual \nProperty \nMG-3.2-004 Evaluate user reported problematic content and integrate feedback into system \nupdates. \nHuman-AI Con\ufb01guration, \nDangerous, Violent, or Hateful \nContent \nMG-3.2-005 \nImplement content \ufb01lters to prevent the generation of inappropriate, harmful, \nfalse, illegal, or violent content related to the GAI application, including for CSAM \nand NCII. These \ufb01lters can be rule-based or leverage additional machine learning \nmodels to \ufb02ag problematic inputs and outputs. \nInformation Integrity; Harmful Bias \nand Homogenization; Dangerous, \nViolent, or Hateful Content; \nObscene, Degrading, and/or \nAbusive Content \nMG-3.2-006 \nImplement real-time monitoring processes for analyzing generated content \nperformance and trustworthiness characteristics related to content provenance \nto identify deviations from the desired standards and trigger alerts for human \nintervention. \nInformation Integrity \n \n44", "20259b03-a5ea-4d46-8544-11b8628bccc5": "Implement real-time monitoring processes for analyzing generated content \nperformance and trustworthiness characteristics related to content provenance \nto identify deviations from the desired standards and trigger alerts for human \nintervention. \nInformation Integrity \n \n44 \nMG-3.2-007 \nLeverage feedback and recommendations from organizational boards or \ncommittees related to the deployment of GAI applications and content \nprovenance when using third-party pre-trained models. \nInformation Integrity; Value Chain \nand Component Integration \nMG-3.2-008 \nUse human moderation systems where appropriate to review generated content \nin accordance with human-AI con\ufb01guration policies established in the Govern \nfunction, aligned with socio-cultural norms in the context of use, and for settings \nwhere AI models are demonstrated to perform poorly. \nHuman-AI Con\ufb01guration \nMG-3.2-009 \nUse organizational risk tolerance to evaluate acceptable risks and performance \nmetrics and decommission or retrain pre-trained models that perform outside of \nde\ufb01ned limits. \nCBRN Information or Capabilities; \nConfabulation \nAI Actor Tasks: AI Deployment, Operation and Monitoring, Third-party entities \n \nMANAGE 4.1: Post-deployment AI system monitoring plans are implemented, including mechanisms for capturing and evaluating", "64b83d40-e568-4d71-a887-24b63f12b7f4": "Confabulation \nAI Actor Tasks: AI Deployment, Operation and Monitoring, Third-party entities \n \nMANAGE 4.1: Post-deployment AI system monitoring plans are implemented, including mechanisms for capturing and evaluating \ninput from users and other relevant AI Actors, appeal and override, decommissioning, incident response, recovery, and change \nmanagement. \nAction ID \nSuggested Action \nGAI Risks \nMG-4.1-001 \nCollaborate with external researchers, industry experts, and community \nrepresentatives to maintain awareness of emerging best practices and \ntechnologies in measuring and managing identi\ufb01ed risks. \nInformation Integrity; Harmful Bias \nand Homogenization \nMG-4.1-002 \nEstablish, maintain, and evaluate e\ufb00ectiveness of organizational processes and \nprocedures for post-deployment monitoring of GAI systems, particularly for \npotential confabulation, CBRN, or cyber risks. \nCBRN Information or Capabilities; \nConfabulation; Information \nSecurity \nMG-4.1-003 \nEvaluate the use of sentiment analysis to gauge user sentiment regarding GAI \ncontent performance and impact, and work in collaboration with AI Actors \nexperienced in user research and experience. \nHuman-AI Con\ufb01guration", "3d830de4-ebb2-49bb-afa2-ff9d785cfd1a": "Evaluate the use of sentiment analysis to gauge user sentiment regarding GAI \ncontent performance and impact, and work in collaboration with AI Actors \nexperienced in user research and experience. \nHuman-AI Con\ufb01guration \nMG-4.1-004 Implement active learning techniques to identify instances where the model fails \nor produces unexpected outputs. \nConfabulation \nMG-4.1-005 \nShare transparency reports with internal and external stakeholders that detail \nsteps taken to update the GAI system to enhance transparency and \naccountability. \nHuman-AI Con\ufb01guration; Harmful \nBias and Homogenization \nMG-4.1-006 \nTrack dataset modi\ufb01cations for provenance by monitoring data deletions, \nrecti\ufb01cation requests, and other changes that may impact the veri\ufb01ability of \ncontent origins. \nInformation Integrity \n \n45 \nMG-4.1-007 \nVerify that AI Actors responsible for monitoring reported issues can e\ufb00ectively \nevaluate GAI system performance including the application of content \nprovenance data tracking techniques, and promptly escalate issues for response. \nHuman-AI Con\ufb01guration; \nInformation Integrity \nAI Actor Tasks: AI Deployment, A\ufb00ected Individuals and Communities, Domain Experts, End-Users, Human Factors, Operation and \nMonitoring", "d0bfebca-7579-4909-8a60-571dbb1e741a": "Human-AI Con\ufb01guration; \nInformation Integrity \nAI Actor Tasks: AI Deployment, A\ufb00ected Individuals and Communities, Domain Experts, End-Users, Human Factors, Operation and \nMonitoring \n \nMANAGE 4.2: Measurable activities for continual improvements are integrated into AI system updates and include regular \nengagement with interested parties, including relevant AI Actors. \nAction ID \nSuggested Action \nGAI Risks \nMG-4.2-001 Conduct regular monitoring of GAI systems and publish reports detailing the \nperformance, feedback received, and improvements made. \nHarmful Bias and Homogenization \nMG-4.2-002 \nPractice and follow incident response plans for addressing the generation of \ninappropriate or harmful content and adapt processes based on \ufb01ndings to \nprevent future occurrences. Conduct post-mortem analyses of incidents with \nrelevant AI Actors, to understand the root causes and implement preventive \nmeasures. \nHuman-AI Con\ufb01guration; \nDangerous, Violent, or Hateful \nContent \nMG-4.2-003 Use visualizations or other methods to represent GAI model behavior to ease \nnon-technical stakeholders understanding of GAI system functionality. \nHuman-AI Con\ufb01guration", "57e512e2-9422-4d02-82cf-2443f78f04b2": "Content \nMG-4.2-003 Use visualizations or other methods to represent GAI model behavior to ease \nnon-technical stakeholders understanding of GAI system functionality. \nHuman-AI Con\ufb01guration \nAI Actor Tasks: AI Deployment, AI Design, AI Development, A\ufb00ected Individuals and Communities, End-Users, Operation and \nMonitoring, TEVV \n \nMANAGE 4.3: Incidents and errors are communicated to relevant AI Actors, including a\ufb00ected communities. Processes for tracking, \nresponding to, and recovering from incidents and errors are followed and documented. \nAction ID \nSuggested Action \nGAI Risks \nMG-4.3-001 \nConduct after-action assessments for GAI system incidents to verify incident \nresponse and recovery processes are followed and e\ufb00ective, including to follow \nprocedures for communicating incidents to relevant AI Actors and where \napplicable, relevant legal and regulatory bodies.  \nInformation Security \nMG-4.3-002 Establish and maintain policies and procedures to record and track GAI system \nreported errors, near-misses, and negative impacts. \nConfabulation; Information \nIntegrity \n \n46 \nMG-4.3-003 \nReport GAI incidents in compliance with legal and regulatory requirements (e.g.,", "fbc38d20-ea5c-4cf3-8f08-88796c4ebddd": "Confabulation; Information \nIntegrity \n \n46 \nMG-4.3-003 \nReport GAI incidents in compliance with legal and regulatory requirements (e.g., \nHIPAA breach reporting, e.g., OCR (2023) or NHTSA (2022) autonomous vehicle \ncrash reporting requirements. \nInformation Security; Data Privacy \nAI Actor Tasks: AI Deployment, A\ufb00ected Individuals and Communities, Domain Experts, End-Users, Human Factors, Operation and \nMonitoring \n \n \n \n \n47 \nAppendix A. Primary GAI Considerations \nThe following primary considerations were derived as overarching themes from the GAI PWG \nconsultation process. These considerations (Governance, Pre-Deployment Testing, Content Provenance, \nand Incident Disclosure) are relevant for voluntary use by any organization designing, developing, and \nusing GAI and also inform the Actions to Manage GAI risks. Information included about the primary \nconsiderations is not exhaustive, but highlights the most relevant topics derived from the GAI PWG.  \nAcknowledgments: These considerations could not have been surfaced without the helpful analysis and \ncontributions from the community and NIST sta\ufb00 GAI PWG leads: George Awad, Luca Belli, Harold Booth,", "7681bd3d-3a51-4e35-aedc-2f3b0a65dd47": "Acknowledgments: These considerations could not have been surfaced without the helpful analysis and \ncontributions from the community and NIST sta\ufb00 GAI PWG leads: George Awad, Luca Belli, Harold Booth, \nMat Heyman, Yooyoung Lee, Mark Pryzbocki, Reva Schwartz, Martin Stanley, and Kyra Yee. \nA.1. Governance \nA.1.1. Overview \nLike any other technology system, governance principles and techniques can be used to manage risks \nrelated to generative AI models, capabilities, and applications. Organizations may choose to apply their \nexisting risk tiering to GAI systems, or they may opt to revise or update AI system risk levels to address \nthese unique GAI risks. This section describes how organizational governance regimes may be re-\nevaluated and adjusted for GAI contexts. It also addresses third-party considerations for governing across \nthe AI value chain.  \nA.1.2. Organizational Governance \nGAI opportunities, risks and long-term performance characteristics are typically less well-understood \nthan non-generative AI tools and may be perceived and acted upon by humans in ways that vary greatly. \nAccordingly, GAI may call for di\ufb00erent levels of oversight from AI Actors or di\ufb00erent human-AI", "5fd41c5b-b7de-4b29-a9c3-f63c394444bd": "Accordingly, GAI may call for di\ufb00erent levels of oversight from AI Actors or di\ufb00erent human-AI \ncon\ufb01gurations in order to manage their risks e\ufb00ectively. Organizations\u2019 use of GAI systems may also \nwarrant additional human review, tracking and documentation, and greater management oversight.  \nAI technology can produce varied outputs in multiple modalities and present many classes of user \ninterfaces. This leads to a broader set of AI Actors interacting with GAI systems for widely di\ufb00ering \napplications and contexts of use. These can include data labeling and preparation, development of GAI \nmodels, content moderation, code generation and review, text generation and editing, image and video \ngeneration, summarization, search, and chat. These activities can take place within organizational \nsettings or in the public domain. \nOrganizations can restrict AI applications that cause harm, exceed stated risk tolerances, or that con\ufb02ict \nwith their tolerances or values. Governance tools and protocols that are applied to other types of AI \nsystems can be applied to GAI systems. These plans and actions include: \n\u2022 Accessibility and reasonable \naccommodations \n\u2022 AI actor credentials and quali\ufb01cations  \n\u2022 Alignment to organizational values \n\u2022 Auditing and assessment \n\u2022 Change-management controls \n\u2022 Commercial use \n\u2022 Data provenance", "9ab3a6d1-1c71-488f-924b-03251264e34c": "accommodations \n\u2022 AI actor credentials and quali\ufb01cations  \n\u2022 Alignment to organizational values \n\u2022 Auditing and assessment \n\u2022 Change-management controls \n\u2022 Commercial use \n\u2022 Data provenance \n \n48 \n\u2022 Data protection \n\u2022 Data retention  \n\u2022 Consistency in use of de\ufb01ning key terms \n\u2022 Decommissioning \n\u2022 Discouraging anonymous use \n\u2022 Education  \n\u2022 Impact assessments  \n\u2022 Incident response \n\u2022 Monitoring \n\u2022 Opt-outs  \n\u2022 Risk-based controls \n\u2022 Risk mapping and measurement \n\u2022 Science-backed TEVV practices \n\u2022 Secure software development practices \n\u2022 Stakeholder engagement \n\u2022 Synthetic content detection and \nlabeling tools and techniques \n\u2022 Whistleblower protections \n\u2022 Workforce diversity and \ninterdisciplinary teams\nEstablishing acceptable use policies and guidance for the use of GAI in formal human-AI teaming settings \nas well as di\ufb00erent levels of human-AI con\ufb01gurations can help to decrease risks arising from misuse, \nabuse, inappropriate repurpose, and misalignment between systems and users. These practices are just \none example of adapting existing governance protocols for GAI contexts.  \nA.1.3. Third-Party Considerations \nOrganizations may seek to acquire, embed, incorporate, or use open-source or proprietary third-party", "6d6c85c4-1459-42aa-b43b-47e9101702fa": "one example of adapting existing governance protocols for GAI contexts.  \nA.1.3. Third-Party Considerations \nOrganizations may seek to acquire, embed, incorporate, or use open-source or proprietary third-party \nGAI models, systems, or generated data for various applications across an enterprise. Use of these GAI \ntools and inputs has implications for all functions of the organization \u2013 including but not limited to \nacquisition, human resources, legal, compliance, and IT services \u2013 regardless of whether they are carried \nout by employees or third parties. Many of the actions cited above are relevant and options for \naddressing third-party considerations. \nThird party GAI integrations may give rise to increased intellectual property, data privacy, or information \nsecurity risks, pointing to the need for clear guidelines for transparency and risk management regarding \nthe collection and use of third-party data for model inputs. Organizations may consider varying risk \ncontrols for foundation models, \ufb01ne-tuned models, and embedded tools, enhanced processes for \ninteracting with external GAI technologies or service providers. Organizations can apply standard or \nexisting risk controls and processes to proprietary or open-source GAI technologies, data, and third-party \nservice providers, including acquisition and procurement due diligence, requests for software bills of \nmaterials (SBOMs), application of service level agreements (SLAs), and statement on standards for", "27e40573-8209-4af0-bbb3-6f28c1763720": "service providers, including acquisition and procurement due diligence, requests for software bills of \nmaterials (SBOMs), application of service level agreements (SLAs), and statement on standards for \nattestation engagement (SSAE) reports to help with third-party transparency and risk management for \nGAI systems. \nA.1.4. Pre-Deployment Testing \nOverview \nThe diverse ways and contexts in which GAI systems may be developed, used, and repurposed \ncomplicates risk mapping and pre-deployment measurement e\ufb00orts. Robust test, evaluation, validation, \nand veri\ufb01cation (TEVV) processes can be iteratively applied \u2013 and documented \u2013 in early stages of the AI \nlifecycle and informed by representative AI Actors (see Figure 3 of the AI RMF). Until new and rigorous \n \n49 \nearly lifecycle TEVV approaches are developed and matured for GAI, organizations may use \nrecommended \u201cpre-deployment testing\u201d practices to measure performance, capabilities, limits, risks, \nand impacts. This section describes risk measurement and estimation as part of pre-deployment TEVV, \nand examines the state of play for pre-deployment testing methodologies.  \nLimitations of Current Pre-deployment Test Approaches \nCurrently available pre-deployment TEVV processes used for GAI applications may be inadequate, non-", "4b118253-73f5-4d2f-a5bb-20007a180f20": "and examines the state of play for pre-deployment testing methodologies.  \nLimitations of Current Pre-deployment Test Approaches \nCurrently available pre-deployment TEVV processes used for GAI applications may be inadequate, non-\nsystematically applied, or fail to re\ufb02ect or mismatched to deployment contexts. For example, the \nanecdotal testing of GAI system capabilities through video games or standardized tests designed for \nhumans (e.g., intelligence tests, professional licensing exams) does not guarantee GAI system validity or \nreliability in those domains. Similarly, jailbreaking or prompt engineering tests may not systematically \nassess validity or reliability risks.  \nMeasurement gaps can arise from mismatches between laboratory and real-world settings. Current \ntesting approaches often remain focused on laboratory conditions or restricted to benchmark test \ndatasets and in silico techniques that may not extrapolate well to\u2014or directly assess GAI impacts in real-\nworld conditions. For example, current measurement gaps for GAI make it di\ufb03cult to precisely estimate \nits potential ecosystem-level or longitudinal risks and related political, social, and economic impacts. \nGaps between benchmarks and real-world use of GAI systems may likely be exacerbated due to prompt \nsensitivity and broad heterogeneity of contexts of use. \nA.1.5. Structured Public Feedback", "1f752613-9ea7-4a38-be21-8629358f58d6": "Gaps between benchmarks and real-world use of GAI systems may likely be exacerbated due to prompt \nsensitivity and broad heterogeneity of contexts of use. \nA.1.5. Structured Public Feedback \nStructured public feedback can be used to evaluate whether GAI systems are performing as intended \nand to calibrate and verify traditional measurement methods. Examples of structured feedback include, \nbut are not limited to: \n\u2022 \nParticipatory Engagement Methods: Methods used to solicit feedback from civil society groups, \na\ufb00ected communities, and users, including focus groups, small user studies, and surveys. \n\u2022 \nField Testing: Methods used to determine how people interact with, consume, use, and make \nsense of AI-generated information, and subsequent actions and e\ufb00ects, including UX, usability, \nand other structured, randomized experiments.  \n\u2022 \nAI Red-teaming: A structured testing exercise used to probe an AI system to \ufb01nd \ufb02aws and \nvulnerabilities such as inaccurate, harmful, or discriminatory outputs, often in a controlled \nenvironment and in collaboration with system developers. \nInformation gathered from structured public feedback can inform design, implementation, deployment \napproval, maintenance, or decommissioning decisions. Results and insights gleaned from these exercises \ncan serve multiple purposes, including improving data quality and preprocessing, bolstering governance", "267bc70c-af60-4130-ae3c-eff8c62f021f": "approval, maintenance, or decommissioning decisions. Results and insights gleaned from these exercises \ncan serve multiple purposes, including improving data quality and preprocessing, bolstering governance \ndecision making, and enhancing system documentation and debugging practices. When implementing \nfeedback activities, organizations should follow human subjects research requirements and best \npractices such as informed consent and subject compensation. \n \n50 \nParticipatory Engagement Methods \nOn an ad hoc or more structured basis, organizations can design and use a variety of channels to engage \nexternal stakeholders in product development or review. Focus groups with select experts can provide \nfeedback on a range of issues. Small user studies can provide feedback from representative groups or \npopulations. Anonymous surveys can be used to poll or gauge reactions to speci\ufb01c features. Participatory \nengagement methods are often less structured than \ufb01eld testing or red teaming, and are more \ncommonly used in early stages of AI or product development.  \nField Testing \nField testing involves structured settings to evaluate risks and impacts and to simulate the conditions \nunder which the GAI system will be deployed. Field style tests can be adapted from a focus on user \npreferences and experiences towards AI risks and impacts \u2013 both negative and positive. When carried \nout with large groups of users, these tests can provide estimations of the likelihood of risks and impacts \nin real world interactions.", "2cd3e491-f364-42ec-bbb9-d9e1ed272109": "preferences and experiences towards AI risks and impacts \u2013 both negative and positive. When carried \nout with large groups of users, these tests can provide estimations of the likelihood of risks and impacts \nin real world interactions. \nOrganizations may also collect feedback on outcomes, harms, and user experience directly from users in \nthe production environment after a model has been released, in accordance with human subject \nstandards such as informed consent and compensation. Organizations should follow applicable human \nsubjects research requirements, and best practices such as informed consent and subject compensation, \nwhen implementing feedback activities. \nAI Red-teaming \nAI red-teaming is an evolving practice that references exercises often conducted in a controlled \nenvironment and in collaboration with AI developers building AI models to identify potential adverse \nbehavior or outcomes of a GAI model or system, how they could occur, and stress test safeguards\u201d. AI \nred-teaming can be performed before or after AI models or systems are made available to the broader \npublic; this section focuses on red-teaming in pre-deployment contexts.  \nThe quality of AI red-teaming outputs is related to the background and expertise of the AI red team \nitself. Demographically and interdisciplinarily diverse AI red teams can be used to identify \ufb02aws in the \nvarying contexts where GAI will be used. For best results, AI red teams should demonstrate domain", "1fda80cf-5a80-4496-9648-f7090f004699": "itself. Demographically and interdisciplinarily diverse AI red teams can be used to identify \ufb02aws in the \nvarying contexts where GAI will be used. For best results, AI red teams should demonstrate domain \nexpertise, and awareness of socio-cultural aspects within the deployment context. AI red-teaming results \nshould be given additional analysis before they are incorporated into organizational governance and \ndecision making, policy and procedural updates, and AI risk management e\ufb00orts. \nVarious types of AI red-teaming may be appropriate, depending on the use case: \n\u2022 \nGeneral Public: Performed by general users (not necessarily AI or technical experts) who are \nexpected to use the model or interact with its outputs, and who bring their own lived \nexperiences and perspectives to the task of AI red-teaming. These individuals may have been \nprovided instructions and material to complete tasks which may elicit harmful model behaviors. \nThis type of exercise can be more e\ufb00ective with large groups of AI red-teamers. \n\u2022 \nExpert: Performed by specialists with expertise in the domain or speci\ufb01c AI red-teaming context \nof use (e.g., medicine, biotech, cybersecurity).  \n\u2022 \nCombination: In scenarios when it is di\ufb03cult to identify and recruit specialists with su\ufb03cient", "aa86012a-fbee-4da1-80cc-81e15992b96d": "of use (e.g., medicine, biotech, cybersecurity).  \n\u2022 \nCombination: In scenarios when it is di\ufb03cult to identify and recruit specialists with su\ufb03cient \ndomain and contextual expertise, AI red-teaming exercises may leverage both expert and \n \n51 \ngeneral public participants. For example, expert AI red-teamers could modify or verify the \nprompts written by general public AI red-teamers. These approaches may also expand coverage \nof the AI risk attack surface.  \n\u2022 \nHuman / AI: Performed by GAI in combination with specialist or non-specialist human teams. \nGAI-led red-teaming can be more cost e\ufb00ective than human red-teamers alone. Human or GAI-\nled AI red-teaming may be better suited for eliciting di\ufb00erent types of harms. \n \nA.1.6. Content Provenance \nOverview \nGAI technologies can be leveraged for many applications such as content generation and synthetic data. \nSome aspects of GAI outputs, such as the production of deepfake content, can challenge our ability to \ndistinguish human-generated content from AI-generated synthetic content. To help manage and mitigate \nthese risks, digital transparency mechanisms like provenance data tracking can trace the origin and \nhistory of content. Provenance data tracking and synthetic content detection can help facilitate greater", "ac8b459b-790a-4866-9841-6257ef21d53f": "these risks, digital transparency mechanisms like provenance data tracking can trace the origin and \nhistory of content. Provenance data tracking and synthetic content detection can help facilitate greater \ninformation access about both authentic and synthetic content to users, enabling better knowledge of \ntrustworthiness in AI systems. When combined with other organizational accountability mechanisms, \ndigital content transparency approaches can enable processes to trace negative outcomes back to their \nsource, improve information integrity, and uphold public trust. Provenance data tracking and synthetic \ncontent detection mechanisms provide information about the origin and history of content to assist in \nGAI risk management e\ufb00orts. \nProvenance metadata can include information about GAI model developers or creators of GAI content, \ndate/time of creation, location, modi\ufb01cations, and sources. Metadata can be tracked for text, images, \nvideos, audio, and underlying datasets. The implementation of provenance data tracking techniques can \nhelp assess the authenticity, integrity, intellectual property rights, and potential manipulations in digital \ncontent. Some well-known techniques for provenance data tracking include digital watermarking, \nmetadata recording, digital \ufb01ngerprinting, and human authentication, among others. \nProvenance Data Tracking Approaches \nProvenance data tracking techniques for GAI systems can be used to track the history and origin of data", "ca4f07d3-f982-440f-9cc9-715913922f15": "metadata recording, digital \ufb01ngerprinting, and human authentication, among others. \nProvenance Data Tracking Approaches \nProvenance data tracking techniques for GAI systems can be used to track the history and origin of data \ninputs, metadata, and synthetic content. Provenance data tracking records the origin and history for \ndigital content, allowing its authenticity to be determined. It consists of techniques to record metadata \nas well as overt and covert digital watermarks on content. Data provenance refers to tracking the origin \nand history of input data through metadata and digital watermarking techniques. Provenance data \ntracking processes can include and assist AI Actors across the lifecycle who may not have full visibility or \ncontrol over the various trade-o\ufb00s and cascading impacts of early-stage model decisions on downstream \nperformance and synthetic outputs. For example, by selecting a watermarking model to prioritize \nrobustness (the durability of a watermark), an AI actor may inadvertently diminish computational \ncomplexity (the resources required to implement watermarking). Organizational risk management \ne\ufb00orts for enhancing content provenance include:  \n\u2022 \nTracking provenance of training data and metadata for GAI systems; \n\u2022 \nDocumenting provenance data limitations within GAI systems; \n \n52 \n\u2022 \nMonitoring system capabilities and limitations in deployment through rigorous TEVV processes; \n\u2022", "695b2a1b-d6c1-4b2e-b603-97768a59456f": "\u2022 \nDocumenting provenance data limitations within GAI systems; \n \n52 \n\u2022 \nMonitoring system capabilities and limitations in deployment through rigorous TEVV processes; \n\u2022 \nEvaluating how humans engage, interact with, or adapt to GAI content (especially in decision \nmaking tasks informed by GAI content), and how they react to applied provenance techniques \nsuch as overt disclosures. \nOrganizations can document and delineate GAI system objectives and limitations to identify gaps where \nprovenance data may be most useful. For instance, GAI systems used for content creation may require \nrobust watermarking techniques and corresponding detectors to identify the source of content or \nmetadata recording techniques and metadata management tools and repositories to trace content \norigins and modi\ufb01cations. Further narrowing of GAI task de\ufb01nitions to include provenance data can \nenable organizations to maximize the utility of provenance data and risk management e\ufb00orts. \nA.1.7. Enhancing Content Provenance through Structured Public Feedback \nWhile indirect feedback methods such as automated error collection systems are useful, they often lack \nthe context and depth that direct input from end users can provide. Organizations can leverage feedback \napproaches described in the Pre-Deployment Testing section to capture input from external sources such \nas through AI red-teaming.", "402533d2-b3a1-428f-8d88-08c5334920e3": "the context and depth that direct input from end users can provide. Organizations can leverage feedback \napproaches described in the Pre-Deployment Testing section to capture input from external sources such \nas through AI red-teaming.  \nIntegrating pre- and post-deployment external feedback into the monitoring process for GAI models and \ncorresponding applications can help enhance awareness of performance changes and mitigate potential \nrisks and harms from outputs. There are many ways to capture and make use of user feedback \u2013 before \nand after GAI systems and digital content transparency approaches are deployed \u2013 to gain insights about \nauthentication e\ufb03cacy and vulnerabilities, impacts of adversarial threats on techniques, and unintended \nconsequences resulting from the utilization of content provenance approaches on users and \ncommunities. Furthermore, organizations can track and document the provenance of datasets to identify \ninstances in which AI-generated data is a potential root cause of performance issues with the GAI \nsystem. \nA.1.8. Incident Disclosure \nOverview \nAI incidents can be de\ufb01ned as an \u201cevent, circumstance, or series of events where the development, use, \nor malfunction of one or more AI systems directly or indirectly contributes to one of the following harms: \ninjury or harm to the health of a person or groups of people (including psychological harms and harms to", "ff743264-0a73-45b5-b5b0-cd06ccfa72a1": "or malfunction of one or more AI systems directly or indirectly contributes to one of the following harms: \ninjury or harm to the health of a person or groups of people (including psychological harms and harms to \nmental health); disruption of the management and operation of critical infrastructure; violations of \nhuman rights or a breach of obligations under applicable law intended to protect fundamental, labor, \nand intellectual property rights; or harm to property, communities, or the environment.\u201d AI incidents can \noccur in the aggregate (i.e., for systemic discrimination) or acutely (i.e., for one individual). \nState of AI Incident Tracking and Disclosure \nFormal channels do not currently exist to report and document AI incidents. However, a number of \npublicly available databases have been created to document their occurrence. These reporting channels \nmake decisions on an ad hoc basis about what kinds of incidents to track. Some, for example, track by \namount of media coverage.  \n \n53 \nDocumenting, reporting, and sharing information about GAI incidents can help mitigate and prevent \nharmful outcomes by assisting relevant AI Actors in tracing impacts to their source. Greater awareness \nand standardization of GAI incident reporting could promote this transparency and improve GAI risk \nmanagement across the AI ecosystem.  \nDocumentation and Involvement of AI Actors", "59c84d6b-2880-4af7-86b9-c2e44718dbeb": "and standardization of GAI incident reporting could promote this transparency and improve GAI risk \nmanagement across the AI ecosystem.  \nDocumentation and Involvement of AI Actors \nAI Actors should be aware of their roles in reporting AI incidents. To better understand previous incidents \nand implement measures to prevent similar ones in the future, organizations could consider developing \nguidelines for publicly available incident reporting which include information about AI actor \nresponsibilities. These guidelines would help AI system operators identify GAI incidents across the AI \nlifecycle and with AI Actors regardless of role. Documentation and review of third-party inputs and \nplugins for GAI systems is especially important for AI Actors in the context of incident disclosure; LLM \ninputs and content delivered through these plugins is often distributed, with inconsistent or insu\ufb03cient \naccess control. \nDocumentation practices including logging, recording, and analyzing GAI incidents can facilitate \nsmoother sharing of information with relevant AI Actors. Regular information sharing, change \nmanagement records, version history and metadata can also empower AI Actors responding to and \nmanaging AI incidents.  \n \n \n54 \nAppendix B. References \nAcemoglu, D. (2024) The Simple Macroeconomics of AI https://www.nber.org/papers/w32487 \nAI Incident Database. https://incidentdatabase.ai/", "79b77812-212e-49d8-8e86-2d9d61d08cd5": "Acemoglu, D. (2024) The Simple Macroeconomics of AI https://www.nber.org/papers/w32487 \nAI Incident Database. https://incidentdatabase.ai/ \nAtherton, D. (2024) Deepfakes and Child Safety: A Survey and Analysis of 2023 Incidents and Responses. \nAI Incident Database. https://incidentdatabase.ai/blog/deepfakes-and-child-safety/ \nBadyal, N. et al. (2023) Intentional Biases in LLM Responses. arXiv. https://arxiv.org/pdf/2311.07611 \nBing Chat: Data Ex\ufb01ltration Exploit Explained. Embrace The Red. \nhttps://embracethered.com/blog/posts/2023/bing-chat-data-ex\ufb01ltration-poc-and-\ufb01x/ \nBommasani, R. et al. (2022) Picking on the Same Person: Does Algorithmic Monoculture lead to Outcome \nHomogenization? arXiv. https://arxiv.org/pdf/2211.13972 \nBoyarskaya, M. et al. (2020) Overcoming Failures of Imagination in AI Infused System Development and \nDeployment. arXiv. https://arxiv.org/pdf/2011.13416", "bf5f4bcf-4b7b-4d76-a357-fd487466bfbc": "Boyarskaya, M. et al. (2020) Overcoming Failures of Imagination in AI Infused System Development and \nDeployment. arXiv. https://arxiv.org/pdf/2011.13416 \nBrowne, D. et al. (2023) Securing the AI Pipeline. Mandiant. \nhttps://www.mandiant.com/resources/blog/securing-ai-pipeline \nBurgess, M. (2024) Generative AI\u2019s Biggest Security Flaw Is Not Easy to Fix. WIRED. \nhttps://www.wired.com/story/generative-ai-prompt-injection-hacking/ \nBurtell, M. et al. (2024) The Surprising Power of Next Word Prediction: Large Language Models \nExplained, Part 1. Georgetown Center for Security and Emerging Technology. \nhttps://cset.georgetown.edu/article/the-surprising-power-of-next-word-prediction-large-language-\nmodels-explained-part-1/ \nCanadian Centre for Cyber Security (2023) Generative arti\ufb01cial intelligence (AI) - ITSAP.00.041. \nhttps://www.cyber.gc.ca/en/guidance/generative-arti\ufb01cial-intelligence-ai-itsap00041 \nCarlini, N., et al. (2021) Extracting Training Data from Large Language Models. Usenix.", "3122138e-7412-4f4b-bc71-d88ebaa4ae84": "Carlini, N., et al. (2021) Extracting Training Data from Large Language Models. Usenix. \nhttps://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting \nCarlini, N. et al. (2023) Quantifying Memorization Across Neural Language Models. ICLR 2023. \nhttps://arxiv.org/pdf/2202.07646 \nCarlini, N. et al. (2024) Stealing Part of a Production Language Model. arXiv. \nhttps://arxiv.org/abs/2403.06634 \nChandra, B. et al. (2023) Dismantling the Disinformation Business of Chinese In\ufb02uence Operations. \nRAND. https://www.rand.org/pubs/commentary/2023/10/dismantling-the-disinformation-business-of-\nchinese.html \nCiriello, R. et al. (2024) Ethical Tensions in Human-AI Companionship: A Dialectical Inquiry into Replika. \nResearchGate. https://www.researchgate.net/publication/374505266_Ethical_Tensions_in_Human-\nAI_Companionship_A_Dialectical_Inquiry_into_Replika", "8678e58a-2d82-4434-a790-3e6c9f7ab599": "ResearchGate. https://www.researchgate.net/publication/374505266_Ethical_Tensions_in_Human-\nAI_Companionship_A_Dialectical_Inquiry_into_Replika \nDahl, M. et al. (2024) Large Legal Fictions: Pro\ufb01ling Legal Hallucinations in Large Language Models. arXiv. \nhttps://arxiv.org/abs/2401.01301 \n \n55 \nDe Angelo, D. (2024) Short, Mid and Long-Term Impacts of AI in Cybersecurity. Palo Alto Networks. \nhttps://www.paloaltonetworks.com/blog/2024/02/impacts-of-ai-in-cybersecurity/ \nDe Freitas, J. et al. (2023) Chatbots and Mental Health: Insights into the Safety of Generative AI. Harvard \nBusiness School. https://www.hbs.edu/ris/Publication%20Files/23-011_c1bdd417-f717-47b6-bccb-\n5438c6e65c1a_f6fd9798-3c2d-4932-b222-056231fe69d7.pdf \nDietvorst, B. et al. (2014) Algorithm Aversion: People Erroneously Avoid Algorithms After Seeing Them", "b8c918f9-c2f1-4631-bb7f-3efc794468c4": "Dietvorst, B. et al. (2014) Algorithm Aversion: People Erroneously Avoid Algorithms After Seeing Them \nErr. Journal of Experimental Psychology. https://marketing.wharton.upenn.edu/wp-\ncontent/uploads/2016/10/Dietvorst-Simmons-Massey-2014.pdf \nDuhigg, C. (2012) How Companies Learn Your Secrets. New York Times. \nhttps://www.nytimes.com/2012/02/19/magazine/shopping-habits.html \nElsayed, G. et al. (2024) Images altered to trick machine vision can in\ufb02uence humans too. Google \nDeepMind. https://deepmind.google/discover/blog/images-altered-to-trick-machine-vision-can-\nin\ufb02uence-humans-too/ \nEpstein, Z. et al. (2023). Art and the science of generative AI. Science. \nhttps://www.science.org/doi/10.1126/science.adh4451 \nFe\ufb00er, M. et al. (2024) Red-Teaming for Generative AI: Silver Bullet or Security Theater? arXiv. \nhttps://arxiv.org/pdf/2401.15897", "ebe445a1-c461-4db4-95fb-103687c90730": "Fe\ufb00er, M. et al. (2024) Red-Teaming for Generative AI: Silver Bullet or Security Theater? arXiv. \nhttps://arxiv.org/pdf/2401.15897 \nGlazunov, S. et al. (2024) Project Naptime: Evaluating O\ufb00ensive Security Capabilities of Large Language \nModels. Project Zero. https://googleprojectzero.blogspot.com/2024/06/project-naptime.html \nGreshake, K. et al. (2023) Not what you've signed up for: Compromising Real-World LLM-Integrated \nApplications with Indirect Prompt Injection. arXiv. https://arxiv.org/abs/2302.12173 \nHagan, M. (2024) Good AI Legal Help, Bad AI Legal Help: Establishing quality standards for responses to \npeople\u2019s legal problem stories. SSRN. https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4696936 \nHaran, R. (2023) Securing LLM Systems Against Prompt Injection. NVIDIA. \nhttps://developer.nvidia.com/blog/securing-llm-systems-against-prompt-injection/ \nInformation Technology Industry Council (2024) Authenticating AI-Generated Content.", "6ef6fb19-5f2f-468e-a721-a5f2846ea0f3": "https://developer.nvidia.com/blog/securing-llm-systems-against-prompt-injection/ \nInformation Technology Industry Council (2024) Authenticating AI-Generated Content. \nhttps://www.itic.org/policy/ITI_AIContentAuthorizationPolicy_122123.pdf \nJain, S. et al. (2023) Algorithmic Pluralism: A Structural Approach To Equal Opportunity. arXiv. \nhttps://arxiv.org/pdf/2305.08157 \nJi, Z. et al (2023) Survey of Hallucination in Natural Language Generation. ACM Comput. Surv. 55, 12, \nArticle 248. https://doi.org/10.1145/3571730 \nJones-Jang, S. et al. (2022) How do people react to AI failure? Automation bias, algorithmic aversion, and \nperceived controllability. Oxford. https://academic.oup.com/jcmc/article/28/1/zmac029/6827859] \nJussupow, E. et al. (2020) Why Are We Averse Towards Algorithms? A Comprehensive Literature Review \non Algorithm Aversion. ECIS 2020. https://aisel.aisnet.org/ecis2020_rp/168/", "0f0292e9-abfc-47dd-8a66-98bb29cfbd3b": "on Algorithm Aversion. ECIS 2020. https://aisel.aisnet.org/ecis2020_rp/168/ \nKalai, A., et al. (2024) Calibrated Language Models Must Hallucinate. arXiv. \nhttps://arxiv.org/pdf/2311.14648 \n \n56 \nKarasavva, V. et al. (2021) Personality, Attitudinal, and Demographic Predictors of Non-consensual \nDissemination of Intimate Images. NIH. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9554400/ \nKatzman, J., et al. (2023) Taxonomizing and measuring representational harms: a look at image tagging. \nAAAI. https://dl.acm.org/doi/10.1609/aaai.v37i12.26670 \nKhan, T. et al. (2024) From Code to Consumer: PAI\u2019s Value Chain Analysis Illuminates Generative AI\u2019s Key \nPlayers. AI. https://partnershiponai.org/from-code-to-consumer-pais-value-chain-analysis-illuminates-\ngenerative-ais-key-players/ \nKirchenbauer, J. et al. (2023) A Watermark for Large Language Models. OpenReview.", "571bf188-4c14-4019-86c2-da21cf200763": "generative-ais-key-players/ \nKirchenbauer, J. et al. (2023) A Watermark for Large Language Models. OpenReview. \nhttps://openreview.net/forum?id=aX8ig9X2a7 \nKleinberg, J. et al. (May 2021) Algorithmic monoculture and social welfare. PNAS. \nhttps://www.pnas.org/doi/10.1073/pnas.2018340118 \nLakatos, S. (2023) A Revealing Picture. Graphika. https://graphika.com/reports/a-revealing-picture \nLee, H. et al. (2024) Deepfakes, Phrenology, Surveillance, and More! A Taxonomy of AI Privacy Risks. \narXiv. https://arxiv.org/pdf/2310.07879 \nLenaerts-Bergmans, B. (2024) Data Poisoning: The Exploitation of Generative AI. Crowdstrike. \nhttps://www.crowdstrike.com/cybersecurity-101/cyberattacks/data-poisoning/ \nLiang, W. et al. (2023) GPT detectors are biased against non-native English writers. arXiv. \nhttps://arxiv.org/abs/2304.02819", "5764cfeb-0fd1-4adc-acca-9dd8b9830c07": "Liang, W. et al. (2023) GPT detectors are biased against non-native English writers. arXiv. \nhttps://arxiv.org/abs/2304.02819 \nLuccioni, A. et al. (2023) Power Hungry Processing: Watts Driving the Cost of AI Deployment? arXiv. \nhttps://arxiv.org/pdf/2311.16863 \nMouton, C. et al. (2024) The Operational Risks of AI in Large-Scale Biological Attacks. RAND. \nhttps://www.rand.org/pubs/research_reports/RRA2977-2.html. \nNicoletti, L. et al. (2023) Humans Are Biased. Generative Ai Is Even Worse. Bloomberg. \nhttps://www.bloomberg.com/graphics/2023-generative-ai-bias/. \nNational Institute of Standards and Technology (2024) Adversarial Machine Learning: A Taxonomy and \nTerminology of Attacks and Mitigations https://csrc.nist.gov/pubs/ai/100/2/e2023/\ufb01nal \nNational Institute of Standards and Technology (2023) AI Risk Management Framework. \nhttps://www.nist.gov/itl/ai-risk-management-framework \nNational Institute of Standards and Technology (2023) AI Risk Management Framework, Chapter 3: AI", "e2bea80e-e224-4dfe-83b7-f02d97917dec": "https://www.nist.gov/itl/ai-risk-management-framework \nNational Institute of Standards and Technology (2023) AI Risk Management Framework, Chapter 3: AI \nRisks and Trustworthiness. \nhttps://airc.nist.gov/AI_RMF_Knowledge_Base/AI_RMF/Foundational_Information/3-sec-characteristics \nNational Institute of Standards and Technology (2023) AI Risk Management Framework, Chapter 6: AI \nRMF Pro\ufb01les. https://airc.nist.gov/AI_RMF_Knowledge_Base/AI_RMF/Core_And_Pro\ufb01les/6-sec-pro\ufb01le \nNational Institute of Standards and Technology (2023) AI Risk Management Framework, Appendix A: \nDescriptions of AI Actor Tasks. \nhttps://airc.nist.gov/AI_RMF_Knowledge_Base/AI_RMF/Appendices/Appendix_A#:~:text=AI%20actors%\n20in%20this%20category,data%20providers%2C%20system%20funders%2C%20product \n \n57 \nNational Institute of Standards and Technology (2023) AI Risk Management Framework, Appendix B: \nHow AI Risks Di\ufb00er from Traditional Software Risks. \nhttps://airc.nist.gov/AI_RMF_Knowledge_Base/AI_RMF/Appendices/Appendix_B", "1feb87fb-b3c1-4234-8c6d-3ef1256b1213": "How AI Risks Di\ufb00er from Traditional Software Risks. \nhttps://airc.nist.gov/AI_RMF_Knowledge_Base/AI_RMF/Appendices/Appendix_B \nNational Institute of Standards and Technology (2023) AI RMF Playbook. \nhttps://airc.nist.gov/AI_RMF_Knowledge_Base/Playbook \nNational Institue of Standards and Technology (2023) Framing Risk \nhttps://airc.nist.gov/AI_RMF_Knowledge_Base/AI_RMF/Foundational_Information/1-sec-risk \nNational Institute of Standards and Technology (2023) The Language of Trustworthy AI: An In-Depth \nGlossary of Terms https://airc.nist.gov/AI_RMF_Knowledge_Base/Glossary \nNational Institue of Standards and Technology (2022) Towards a Standard for Identifying and Managing \nBias in Arti\ufb01cial Intelligence https://www.nist.gov/publications/towards-standard-identifying-and-\nmanaging-bias-arti\ufb01cial-intelligence \nNorthcutt, C. et al. (2021) Pervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks. \narXiv. https://arxiv.org/pdf/2103.14749", "ee942749-b2cf-4fcd-a510-cb6fc2d45e70": "Northcutt, C. et al. (2021) Pervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks. \narXiv. https://arxiv.org/pdf/2103.14749 \nOECD (2023) \"Advancing accountability in AI: Governing and managing risks throughout the lifecycle for \ntrustworthy AI\", OECD Digital Economy Papers, No. 349, OECD Publishing, Paris. \nhttps://doi.org/10.1787/2448f04b-en \nOECD (2024) \"De\ufb01ning AI incidents and related terms\" OECD Arti\ufb01cial Intelligence Papers, No. 16, OECD \nPublishing, Paris. https://doi.org/10.1787/d1a8d965-en \nOpenAI (2023) GPT-4 System Card. https://cdn.openai.com/papers/gpt-4-system-card.pdf \nOpenAI (2024) GPT-4 Technical Report. https://arxiv.org/pdf/2303.08774 \nPadmakumar, V. et al. (2024) Does writing with language models reduce content diversity? ICLR. \nhttps://arxiv.org/pdf/2309.05196 \nPark, P. et. al. (2024) AI deception: A survey of examples, risks, and potential solutions. Patterns, 5(5).", "04dc30c4-6b82-4fd5-bdd3-f7a253749f3e": "https://arxiv.org/pdf/2309.05196 \nPark, P. et. al. (2024) AI deception: A survey of examples, risks, and potential solutions. Patterns, 5(5). \narXiv. https://arxiv.org/pdf/2308.14752 \nPartnership on AI (2023) Building a Glossary for Synthetic Media Transparency Methods, Part 1: Indirect \nDisclosure. https://partnershiponai.org/glossary-for-synthetic-media-transparency-methods-part-1-\nindirect-disclosure/ \nQu, Y. et al. (2023) Unsafe Di\ufb00usion: On the Generation of Unsafe Images and Hateful Memes From Text-\nTo-Image Models. arXiv. https://arxiv.org/pdf/2305.13873 \nRafat, K. et al. (2023) Mitigating carbon footprint for knowledge distillation based deep learning model \ncompression. PLOS One. https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0285668 \nSaid, I. et al. (2022) Nonconsensual Distribution of Intimate Images: Exploring the Role of Legal Attitudes \nin Victimization and Perpetration. Sage.", "d81b8671-d240-414b-be39-66ea385ad81f": "Said, I. et al. (2022) Nonconsensual Distribution of Intimate Images: Exploring the Role of Legal Attitudes \nin Victimization and Perpetration. Sage. \nhttps://journals.sagepub.com/doi/full/10.1177/08862605221122834#bibr47-08862605221122834 \nSandbrink, J. (2023) Arti\ufb01cial intelligence and biological misuse: Di\ufb00erentiating risks of language models \nand biological design tools. arXiv. https://arxiv.org/pdf/2306.13952 \n \n58 \nSatariano, A. et al. (2023) The People Onscreen Are Fake. The Disinformation Is Real. New York Times. \nhttps://www.nytimes.com/2023/02/07/technology/arti\ufb01cial-intelligence-training-deepfake.html \nSchaul, K. et al. (2024) Inside the secret list of websites that make AI like ChatGPT sound smart. \nWashington Post. https://www.washingtonpost.com/technology/interactive/2023/ai-chatbot-learning/ \nScheurer, J. et al. (2023) Technical report: Large language models can strategically deceive their users", "63506495-db20-438a-a6c0-2ce4d838dd50": "Scheurer, J. et al. (2023) Technical report: Large language models can strategically deceive their users \nwhen put under pressure. arXiv. https://arxiv.org/abs/2311.07590 \nShelby, R. et al. (2023) Sociotechnical Harms of Algorithmic Systems: Scoping a Taxonomy for Harm \nReduction. arXiv. https://arxiv.org/pdf/2210.05791 \nShevlane, T. et al. (2023) Model evaluation for extreme risks. arXiv. https://arxiv.org/pdf/2305.15324 \nShumailov, I. et al. (2023) The curse of recursion: training on generated data makes models forget. arXiv. \nhttps://arxiv.org/pdf/2305.17493v2 \nSmith, A. et al. (2023) Hallucination or Confabulation? Neuroanatomy as metaphor in Large Language \nModels. PLOS Digital Health. \nhttps://journals.plos.org/digitalhealth/article?id=10.1371/journal.pdig.0000388 \nSoice, E. et al. (2023) Can large language models democratize access to dual-use biotechnology? arXiv. \nhttps://arxiv.org/abs/2306.03809", "7fc9ff90-d884-47ef-a410-9f541792c7dc": "Soice, E. et al. (2023) Can large language models democratize access to dual-use biotechnology? arXiv. \nhttps://arxiv.org/abs/2306.03809 \nSolaiman, I. et al. (2023) The Gradient of Generative AI Release: Methods and Considerations. arXiv. \nhttps://arxiv.org/abs/2302.04844 \nStaab, R. et al. (2023) Beyond Memorization: Violating Privacy via Inference With Large Language \nModels. arXiv. https://arxiv.org/pdf/2310.07298 \nStanford, S. et al. (2023) Whose Opinions Do Language Models Re\ufb02ect? arXiv. \nhttps://arxiv.org/pdf/2303.17548 \nStrubell, E. et al. (2019) Energy and Policy Considerations for Deep Learning in NLP. arXiv. \nhttps://arxiv.org/pdf/1906.02243 \nThe White House (2016) Circular No. A-130, Managing Information as a Strategic Resource. \nhttps://www.whitehouse.gov/wp-\ncontent/uploads/legacy_drupal_\ufb01les/omb/circulars/A130/a130revised.pdf", "1b911521-8bfa-4444-b7df-d32ff043bd68": "https://www.whitehouse.gov/wp-\ncontent/uploads/legacy_drupal_\ufb01les/omb/circulars/A130/a130revised.pdf \nThe White House (2023) Executive Order on the Safe, Secure, and Trustworthy Development and Use of \nArti\ufb01cial Intelligence. https://www.whitehouse.gov/brie\ufb01ng-room/presidential-\nactions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-\narti\ufb01cial-intelligence/ \nThe White House (2022) Roadmap for Researchers on Priorities Related to Information Integrity \nResearch and Development. https://www.whitehouse.gov/wp-content/uploads/2022/12/Roadmap-\nInformation-Integrity-RD-2022.pdf? \nThiel, D. (2023) Investigation Finds AI Image Generation Models Trained on Child Abuse. Stanford Cyber \nPolicy Center. https://cyber.fsi.stanford.edu/news/investigation-\ufb01nds-ai-image-generation-models-\ntrained-child-abuse \n \n59 \nTirrell, L. (2017) Toxic Speech: Toward an Epidemiology of Discursive Harm. Philosophical Topics, 45(2), \n139-162. https://www.jstor.org/stable/26529441", "49d93069-13cf-4f24-b8fc-f79478c143de": "139-162. https://www.jstor.org/stable/26529441  \nTufekci, Z. (2015) Algorithmic Harms Beyond Facebook and Google: Emergent Challenges of \nComputational Agency. Colorado Technology Law Journal. https://ctlj.colorado.edu/wp-\ncontent/uploads/2015/08/Tufekci-\ufb01nal.pdf \nTurri, V. et al. (2023) Why We Need to Know More: Exploring the State of AI Incident Documentation \nPractices. AAAI/ACM Conference on AI, Ethics, and Society. \nhttps://dl.acm.org/doi/fullHtml/10.1145/3600211.3604700 \nUrbina, F. et al. (2022) Dual use of arti\ufb01cial-intelligence-powered drug discovery. Nature Machine \nIntelligence. https://www.nature.com/articles/s42256-022-00465-9 \nWang, X. et al. (2023) Energy and Carbon Considerations of Fine-Tuning BERT. ACL Anthology. \nhttps://aclanthology.org/2023.\ufb01ndings-emnlp.607.pdf \nWang, Y. et al. (2023) Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs. arXiv.", "4d57797c-0be6-4155-92cd-e7c0a320b899": "Wang, Y. et al. (2023) Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs. arXiv. \nhttps://arxiv.org/pdf/2308.13387 \nWardle, C. et al. (2017) Information Disorder: Toward an interdisciplinary framework for research and \npolicy making. Council of Europe. https://rm.coe.int/information-disorder-toward-an-interdisciplinary-\nframework-for-researc/168076277c \nWeatherbed, J. (2024) Trolls have \ufb02ooded X with graphic Taylor Swift AI fakes. The Verge. \nhttps://www.theverge.com/2024/1/25/24050334/x-twitter-taylor-swift-ai-fake-images-trending \nWei, J. et al. (2024) Long Form Factuality in Large Language Models. arXiv. \nhttps://arxiv.org/pdf/2403.18802 \nWeidinger, L. et al. (2021) Ethical and social risks of harm from Language Models. arXiv. \nhttps://arxiv.org/pdf/2112.04359 \nWeidinger, L. et al. (2023) Sociotechnical Safety Evaluation of Generative AI Systems. arXiv.", "b0244231-06bf-4a3e-8c72-7d8599241910": "https://arxiv.org/pdf/2112.04359 \nWeidinger, L. et al. (2023) Sociotechnical Safety Evaluation of Generative AI Systems. arXiv. \nhttps://arxiv.org/pdf/2310.11986 \nWeidinger, L. et al. (2022) Taxonomy of Risks posed by Language Models. FAccT \u201922. \nhttps://dl.acm.org/doi/pdf/10.1145/3531146.3533088 \nWest, D. (2023) AI poses disproportionate risks to women. Brookings. \nhttps://www.brookings.edu/articles/ai-poses-disproportionate-risks-to-women/ \nWu, K. et al. (2024) How well do LLMs cite relevant medical references? An evaluation framework and \nanalyses. arXiv. https://arxiv.org/pdf/2402.02008 \nYin, L. et al. (2024) OpenAI\u2019s GPT Is A Recruiter\u2019s Dream Tool. Tests Show There\u2019s Racial Bias. Bloomberg. \nhttps://www.bloomberg.com/graphics/2024-openai-gpt-hiring-racial-discrimination/ \nYu, Z. et al. (March 2024) Don\u2019t Listen To Me: Understanding and Exploring Jailbreak Prompts of Large", "78026411-a3c4-411d-83ca-d9889dfb7adb": "Yu, Z. et al. (March 2024) Don\u2019t Listen To Me: Understanding and Exploring Jailbreak Prompts of Large \nLanguage Models. arXiv. https://arxiv.org/html/2403.17336v1 \nZaugg, I. et al. (2022) Digitally-disadvantaged languages. Policy Review. \nhttps://policyreview.info/pdf/policyreview-2022-2-1654.pdf \n \n60 \nZhang, Y. et al. (2023) Human favoritism, not AI aversion: People\u2019s perceptions (and bias) toward \ngenerative AI, human experts, and human\u2013GAI collaboration in persuasive content generation. Judgment \nand Decision Making. https://www.cambridge.org/core/journals/judgment-and-decision-\nmaking/article/human-favoritism-not-ai-aversion-peoples-perceptions-and-bias-toward-generative-ai-\nhuman-experts-and-humangai-collaboration-in-persuasive-content-\ngeneration/419C4BD9CE82673EAF1D8F6C350C4FA8 \nZhang, Y. et al. (2023) Siren\u2019s Song in the AI Ocean: A Survey on Hallucination in Large Language Models. \narXiv. https://arxiv.org/pdf/2309.01219", "6e590e51-727a-442b-b43b-24e90e0f935c": "arXiv. https://arxiv.org/pdf/2309.01219 \nZhao, X. et al. (2023) Provable Robust Watermarking for AI-Generated Text. Semantic Scholar. \nhttps://www.semanticscholar.org/paper/Provable-Robust-Watermarking-for-AI-Generated-Text-Zhao-\nAnanth/75b68d0903af9d9f6e47ce3cf7e1a7d27ec811dc"}}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook creates the data files for the assignment\n",
    "#### I have found no way to make the original notebook generate the data files after running the first breakout section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip -q install langchain==0.2.16\n",
    "# ! pip -q install langchain-cohere==0.3.0\n",
    "# ! pip -q install langchain-community==0.2.17\n",
    "# ! pip -q install langchain-core==0.2.41\n",
    "# ! pip -q install langchain-experimental==0.3.2\n",
    "# ! pip -q install langchain-huggingface==0.1.0\n",
    "# ! pip -q install langchain-openai==0.1.25\n",
    "# ! pip -q install langchain-qdrant==0.1.3\n",
    "# ! pip -q install langchain-text-splitters==0.2.4\n",
    "# ! pip -q install langgraph==0.2.16\n",
    "# ! pip -q install langgraph-checkpoint==1.0.6\n",
    "# ! pip -q install langsmith==0.1.129\n",
    "# ! pip -q install ragas==0.1.20\n",
    "# ! pip -q install rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "LANGCHAIN_API_KEY = os.environ.get(\"LANGCHAIN_API_KEY\")\n",
    "LANGCHAIN_TRACING_V2 = True\n",
    "LANGCHAIN_PROJECT=\"Advanced RAG\"\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "COHERE_API_KEY = os.environ.get(\"COHERE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "documents = []\n",
    "\n",
    "for i in range(1, 5):\n",
    "  loader = CSVLoader(\n",
    "      file_path=f\"john_wick_{i}.csv\",\n",
    "      metadata_columns=[\"Review_Date\", \"Review_Title\", \"Review_Url\", \"Author\", \"Rating\"]\n",
    "  )\n",
    "\n",
    "  movie_docs = loader.load()\n",
    "  for doc in movie_docs:\n",
    "\n",
    "    # Add the \"Movie Title\" (John Wick 1, 2, ...)\n",
    "    doc.metadata[\"Movie_Title\"] = f\"John Wick {i}\"\n",
    "\n",
    "    # convert \"Rating\" to an `int`, if no rating is provided - assume 0 rating\n",
    "    doc.metadata[\"Rating\"] = int(doc.metadata[\"Rating\"]) if doc.metadata[\"Rating\"] else 0\n",
    "\n",
    "    # newer movies have a more recent \"last_accessed_at\"\n",
    "    doc.metadata[\"last_accessed_at\"] = datetime.now() - timedelta(days=4-i)\n",
    "\n",
    "  documents.extend(movie_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'john_wick_1.csv', 'row': 0, 'Review_Date': '6 May 2015', 'Review_Title': ' Kinetic, concise, and stylish; John Wick kicks ass.\\n', 'Review_Url': '/review/rw3233896/?ref_=tt_urv', 'Author': 'lnvicta', 'Rating': 8, 'Movie_Title': 'John Wick 1', 'last_accessed_at': datetime.datetime(2024, 9, 26, 18, 8, 53, 337040)}, page_content=\": 0\\nReview: The best way I can describe John Wick is to picture Taken but instead of Liam Neeson it's Keanu Reeves and instead of his daughter it's his dog. That's essentially the plot of the movie. John Wick (Reeves) is out to seek revenge on the people who took something he loved from him. It's a beautifully simple premise for an action movie - when action movies get convoluted, they get bad i.e. A Good Day to Die Hard. John Wick gives the viewers what they want: Awesome action, stylish stunts, kinetic chaos, and a relatable hero to tie it all together. John Wick succeeds in its simplicity.\")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores  import Qdrant\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "\n",
    "vectorstore = Qdrant.from_documents(\n",
    "    documents,\n",
    "    embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"JohnWick\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_retriever = vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "RAG_TEMPLATE = \"\"\"\\\n",
    "You are a helpful and kind assistant. Use the context provided below to answer the question.\n",
    "\n",
    "If you do not know the answer, or are unsure, say you don't know.\n",
    "\n",
    "Query:\n",
    "{question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat_model = ChatOpenAI(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "naive_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | naive_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following code generated the testset, which I saved to CSV.\n",
    "### It is commented out because I do not want to run it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ragas.testset.generator import TestsetGenerator\n",
    "# from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "# from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# generator_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "# critic_llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# generator = TestsetGenerator.from_langchain(\n",
    "#     generator_llm,\n",
    "#     critic_llm,\n",
    "#     embeddings\n",
    "# )\n",
    "\n",
    "# distributions = {\n",
    "#     simple: 0.5,\n",
    "#     multi_context: 0.4,\n",
    "#     reasoning: 0.1\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testset = generator.generate_with_langchain_docs(documents, 20, distributions, with_debugging_logs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data_row in testset.test_data:\n",
    "#     question = data_row.question\n",
    "#     contexts = data_row.contexts\n",
    "#     ground_truth = data_row.ground_truth\n",
    "#     evolution_type = data_row.evolution_type\n",
    "#     metadata = data_row.metadata\n",
    "    \n",
    "#     # Process each element as needed\n",
    "#     print(f\"Question: {question}\")\n",
    "#     print(f\"Contexts: {contexts}\")\n",
    "#     print(f\"Ground Truth: {ground_truth}\")\n",
    "#     print(f\"Evolution Type: {evolution_type}\")\n",
    "#     print(f\"Metadata: {metadata}\")\n",
    "#     print(\"\\n\")  # For better readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "## commented the following code because has already run once and will mess up Langsmith\n",
    "\n",
    "# dataset_name = \"John Wicks Dataset\"\n",
    "\n",
    "# dataset = client.create_dataset(\n",
    "#     dataset_name=dataset_name,\n",
    "#     description=\"Questions about John Wicks movie reviews\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test in testset.to_pandas().iterrows():\n",
    "#   client.create_example(\n",
    "#       inputs={\n",
    "#           \"question\": test[1][\"question\"]\n",
    "#       },\n",
    "#       outputs={\n",
    "#           \"answer\": test[1][\"ground_truth\"]\n",
    "#       },\n",
    "#       metadata={\n",
    "#           \"context\": test[0]\n",
    "#       },\n",
    "#       dataset_id=dataset.id\n",
    "#   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testset.test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testset_df = testset.to_pandas()\n",
    "# testset_df.to_csv(\"testset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_df = pd.read_csv(\"testset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions = test_df[\"question\"].values.tolist()\n",
    "test_groundtruths = test_df[\"ground_truth\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []\n",
    "contexts = []\n",
    "\n",
    "for question in test_questions:\n",
    "  response = naive_retrieval_chain.invoke({\"question\" : question})\n",
    "  answers.append(response[\"response\"].content)\n",
    "  contexts.append([context.page_content for context in response[\"context\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "response_dataset = Dataset.from_dict({\n",
    "    \"question\" : test_questions,\n",
    "    \"answer\" : answers,\n",
    "    \"contexts\" : contexts,\n",
    "    \"ground_truth\" : test_groundtruths\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"What role does the Russian mafia play in John Wick's story?\",\n",
       " 'answer': \"The Russian mafia plays a significant role in John Wick's story. In the first movie, John Wick is a retired assassin who seeks vengeance against the son of a Russian mobster who stole his car, killed his dog, and destroyed his peaceful life. This leads to a violent conflict between John Wick and the Russian mafia. Additionally, in the second movie, John Wick is forced back into the world of assassins when an Italian crime lord calls in a favor, illustrating the ongoing involvement of criminal organizations in John Wick's life.\",\n",
       " 'contexts': [\": 20\\nReview: After resolving his issues with the Russian mafia, John Wick (Keanu Reeves) returns home. But soon the mobster Santino D'Antonio (Riccardo Scamarcio) visits him to show Wick's marker and tells that he needs to help. John Wicks refuses since he is retired and Santino blows-up his house. John Wick meets the owner of the Continental hotel in New York City, Winston (Ian McShane), and he tells that Wick cannot violate the Mafia rules and shall honor the marker. Santino asks John Wick to kill his sister Gianna D'Antonio (Claudia Gerini) in Rome so that he can sit on the High Table of the criminal organizations. When John Wicks accomplishes his assignment, Santino puts a seven-million dollar contract on him attracting professional killers from everywhere. But Wick promises to kill Santino that is not protected by his marker anymore.\",\n",
       "  \": 18\\nReview: When the story begins, John (Keanu Reeves) has just lost his wife. After her death, he's a bit lost but tries to rebuild his life. One day, he's getting gas and a young Russian-American punk notices Wick's classic car...and tries to buy it off him. But Wick isn't interested and declines. Soon, the punk arrives at Wick's home with some goons where they surprise him--beating him senseless, destroying his stuff, killing his dog and stealing his car! This is when you then learn that Wick is a super-assassin....and the punk chose the wrong guy to attack. The jerk's father is a big-time Russian mobster....and it's a contest to see who will win...the mobster and his gang or Wick on his own. Considering there are (so far) two MORE John Wick films, it pretty much seems certain who will win this battle.\",\n",
       "  \": 19\\nReview: If you've seen the first John Wick movie, you know that Keanu Reeves is John Wick, a retired assassin who comes out of retirement when someone kills his dog. In this one, which begins a week later, matters are still reverberating, and some one has stolen his car, which calls for a lot of carnage. That settled, John is called on to pay off an old debt by helping Ian McShane take over the Assassin's Guild by flying around to Italy, Canada and Manhattan and killing what seems like hundreds of assassins.\",\n",
       "  \": 5\\nReview: Iosef's uncle still has John Wick's car. Wick comes after it and the uncle accepts his offer of peace. He hopes to return to his peaceful retirement but crime lord Santino D'Antonio calls in his Marker. He faces deadly assassins, numerous killers, and countless thugs as Santino uses him to gain power even offering a $7 million contract.\",\n",
       "  \": 5\\nReview: Ultra-violent first entry with lots of killings, thrills , noisy action , suspense , and crossfire . In this original John Wick (2014) , an ex-hit-man comes out of retirement to track down the gangsters that killed his dog and took everything from him . With the untimely death of his beloved wife still bitter in his mouth he seeks for vengeance . But when an arrogant Russian mob prince and hoodlums steal his car and kill his dog , they are fully aware of his lethal capacity. The Bogeyman will find himself dragged into an impossible task as every killer in the business dreams of cornering the legendary Wick who now has an enormous price on his head . In this first installment John Wick , blind with revenge, and for his salvation John will immediately unleash a carefully orchestrated maelstrom of destruction against those attempt to chase him and with a price tag on his head, as he is the target of hit men : an army of bounty-hunting killers on his trail and a murderer woman everywhere . The legendary hitman will be forced to unearth his meticulously concealed identity and to carry out a relentless vendetta . Now, only blood can quench the boogeyman's thirst for retribution . Don't Set Him Off! . John Wick isn't the Boogeyman... He's the guy you send to kill the doomed Boogeyman. Revenge is all he has left. You want peace, prepare for war . Don't Hunt What You Can't Kill. Tick Tock, Mr. Wick. Everyone Is Waiting. For John Wick . Every Action Has Consequences. This Friday, Wick is Back . Its the World Vs. Wick. Every Action Has Consequences.\",\n",
       "  \": 6\\nReview: Assassin John Wick is referred to as Baba Yaga in this third chapter, suggesting that he is, in fact, a supernatural being; his ability to evade death, appear and disappear like a ninja from a Godfrey Ho film, and move with lightning fast reflexes despite being in his mid-fifties confirms this fact, in my opinion. Unfortunately, Wick's invincibility means that the many action scenes in Parabellum are largely devoid of excitement, the only outcome being that Wick is still alive and all of his enemies are dead.\",\n",
       "  ': 4\\nReview: \"John Wick: Chapter 2\" (2017 release; 122 min.) continues the \\'adventures\\' of former (?) hit man John Wick. As the movie opens, we are immediately thrown in the middle of a car vs. bike chase, and next thing we know, we find our man retrieving his beloved Mustang from a chop shop in NYC, but not without cars flying about, and dozens of dead or wounded bodies. And that\\'s all in the pre-opening credits! As the story unfolds, Wick, who wants \"out, is nevertheless forced back \"in\" when an Italian baddie calls in a favor and Wick has no choice but to accept. To tell you more of the plot would spoil your viewing experience, you\\'ll just have to see for yourself how it all plays out.',\n",
       "  ': 1\\nReview: The Table, the international crminal brotherhood which has condemned John Wick empowers The Marquis to deal with him (and the Manager of the New York Continentale Hotel). But maybe, just maybe, there is a way out.',\n",
       "  \": 20\\nReview: John Wick is something special. It takes as much time setting up elaborate action sequences as it does the world with which it all takes place in. And what a world it is. It reminds me of Millers Crossing and it is cooler than any other recent attempt at noir. We are shown a criminal underworld where, if you are connected, many powerful people know who you are and show you respect. John Wick was connected but he got out. He is the rare killer who has found peace, and he is grateful for that peace. Some young kids steal that from him and he does what he does best, he wages a one man war against the Russian Mafia. It might sound like the film takes quite a leap but it all makes sense. The motives of John and the people who get in the way of his bullets are all very clear, even if it does come across as rather simple. That's the plot at it's most basic. Then there's the action. The film is directed by Reeve's stuntman from The Matrix, so this guy knows action. There are sequences that flow so smoothly it puts other action films and their quick cuts to shame. Keaunu moves so fluidly throughout the film and comes across as such a natural that the only disappointment is that we have not seen him like this before. Along the way are plenty of character actors, fans of The Matrix and The Wire will recognize a few people then there are more obvious ones like Ian McShane and Willem Dafoe. Everyone seems to be having a good time. That is another plus for this movie. It get's dark at times but overall it is quite fun, not very chipper, but fun. I cannot recommend this movie enough. I believe it is a must see for action fans and for anyone looking for something a bit different from the usual fare.\",\n",
       "  ': 17\\nReview: Picking up where the first movie left off, John Wick still wants his car back. Also, he now discovers that there is a bounty on his head. Rejoining his former profession, he heads to Rome to complete an assassination assignment.'],\n",
       " 'ground_truth': \"The Russian mafia plays a significant role in John Wick's story as they are the reason for his return to violence after retirement. After resolving issues with them, John Wick is confronted by Santino D'Antonio, who uses a marker to compel Wick to assist him, leading to further conflict and ultimately a contract being placed on Wick's life.\"}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    answer_correctness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "\n",
    "metrics = [\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    answer_correctness,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(retrieval_chain):\n",
    "    answers = []\n",
    "    contexts = []\n",
    "\n",
    "    for question in test_questions:\n",
    "        response = retrieval_chain.invoke({\"question\" : question})\n",
    "        answers.append(response[\"response\"].content)\n",
    "        contexts.append([context.page_content for context in response[\"context\"]])\n",
    "    response_dataset = Dataset.from_dict({\n",
    "        \"question\" : test_questions,\n",
    "        \"answer\" : answers,\n",
    "        \"contexts\" : contexts,\n",
    "        \"ground_truth\" : test_groundtruths\n",
    "    })\n",
    "    return evaluate(response_dataset, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 100/100 [00:46<00:00,  2.17it/s]\n"
     ]
    }
   ],
   "source": [
    "naive_results = run_evaluation(naive_retrieval_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"Advanced Rag - {uuid4().hex[0:8]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  !pip install rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | bm25_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Opinions on John Wick seem to be divided. Some people really enjoyed the movie, praising its action sequences, world-building, and Keanu Reeves' performance, while others found it to be lacking in plot and substance. Overall, it seems that there are both fans and critics of the John Wick series.\""
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 100/100 [00:34<00:00,  2.87it/s]\n"
     ]
    }
   ],
   "source": [
    "bm25_results = run_evaluation(bm25_retrieval_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'faithfulness': 0.6707, 'answer_relevancy': 0.8206, 'context_recall': 0.7958, 'context_precision': 0.6861, 'answer_correctness': 0.6426}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contextual Compression Retrieval\n",
    "This would NOT run in this notebook even after setting API key.\n",
    "This runs in the original notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import getpass\n",
    "# os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "# from langchain_cohere import CohereRerank\n",
    "\n",
    "# compressor = CohereRerank(model=\"rerank-english-v3.0\")\n",
    "# compression_retriever = ContextualCompressionRetriever(\n",
    "#     base_compressor=compressor, base_retriever=naive_retriever\n",
    "# )\n",
    "\n",
    "# contextual_compression_retrieval_chain = (\n",
    "#     {\"context\": itemgetter(\"question\") | compression_retriever, \"question\": itemgetter(\"question\")}\n",
    "#     | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "#     | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=naive_retriever, llm=chat_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_query_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | multi_query_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 100/100 [00:56<00:00,  1.76it/s]\n"
     ]
    }
   ],
   "source": [
    "multi_query_results = run_evaluation(multi_query_retrieval_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "parent_docs = documents\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(location=\":memory:\")\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"full_documents\",\n",
    "    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
    ")\n",
    "\n",
    "parent_document_vectorstore = Qdrant(\n",
    "    collection_name=\"full_documents\", \n",
    "    embeddings=OpenAIEmbeddings(model=\"text-embedding-3-small\"), \n",
    "    client=client\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = InMemoryStore()\n",
    "\n",
    "parent_document_retriever = ParentDocumentRetriever(\n",
    "    vectorstore = parent_document_vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_document_retriever.add_documents(parent_docs, ids=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_document_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | parent_document_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 100/100 [00:32<00:00,  3.06it/s]\n"
     ]
    }
   ],
   "source": [
    "parent_results = run_evaluation(parent_document_retrieval_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "retriever_list = [bm25_retriever, naive_retriever, parent_document_retriever, multi_query_retriever]\n",
    "equal_weighting = [1/len(retriever_list)] * len(retriever_list)\n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=retriever_list, weights=equal_weighting\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | ensemble_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   1%|          | 1/100 [00:01<01:58,  1.20s/it]No statements were generated from the answer.\n",
      "Evaluating: 100%|██████████| 100/100 [00:59<00:00,  1.67it/s]\n"
     ]
    }
   ],
   "source": [
    "ensemble_results = run_evaluation(ensemble_retrieval_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "semantic_chunker = SemanticChunker(\n",
    "    embeddings,\n",
    "    breakpoint_threshold_type=\"percentile\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_documents = semantic_chunker.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_vectorstore = Qdrant.from_documents(\n",
    "    semantic_documents,\n",
    "    embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"JohnWickSemantic\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_retriever = semantic_vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | semantic_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 100/100 [00:50<00:00,  1.99it/s]\n"
     ]
    }
   ],
   "source": [
    "semantic_results = run_evaluation(semantic_retrieval_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'faithfulness': 0.8604, 'answer_relevancy': 0.9179, 'context_recall': 0.9750, 'context_precision': 0.7605, 'answer_correctness': 0.7486}\n",
      "{'faithfulness': 0.6707, 'answer_relevancy': 0.8206, 'context_recall': 0.7958, 'context_precision': 0.6861, 'answer_correctness': 0.6426}\n",
      "Contextual Compression Results Not Available\n",
      "{'faithfulness': 0.7868, 'answer_relevancy': 0.8284, 'context_recall': 0.9750, 'context_precision': 0.6560, 'answer_correctness': 0.6776}\n",
      "{'faithfulness': 0.8105, 'answer_relevancy': 0.9141, 'context_recall': 0.8750, 'context_precision': 0.8333, 'answer_correctness': 0.6585}\n",
      "{'faithfulness': 0.8741, 'answer_relevancy': 0.8743, 'context_recall': 0.9750, 'context_precision': 0.7560, 'answer_correctness': 0.7285}\n",
      "{'faithfulness': 0.8595, 'answer_relevancy': 0.7905, 'context_recall': 0.8750, 'context_precision': 0.7859, 'answer_correctness': 0.5975}\n"
     ]
    }
   ],
   "source": [
    "print(naive_results)\n",
    "print(bm25_results)\n",
    "print(\"Contextual Compression Results Not Available\")\n",
    "print(multi_query_results)\n",
    "print(parent_results)\n",
    "print(ensemble_results)\n",
    "print(semantic_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Naive</th>\n",
       "      <th>BM25</th>\n",
       "      <th>MultiQ</th>\n",
       "      <th>Parent</th>\n",
       "      <th>Ensemble</th>\n",
       "      <th>Semantic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>faithfulness</td>\n",
       "      <td>0.860417</td>\n",
       "      <td>0.670694</td>\n",
       "      <td>0.786845</td>\n",
       "      <td>0.810471</td>\n",
       "      <td>0.874123</td>\n",
       "      <td>0.859524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>answer_relevancy</td>\n",
       "      <td>0.917881</td>\n",
       "      <td>0.820600</td>\n",
       "      <td>0.828396</td>\n",
       "      <td>0.914089</td>\n",
       "      <td>0.874294</td>\n",
       "      <td>0.790533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>context_recall</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.795833</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>context_precision</td>\n",
       "      <td>0.760535</td>\n",
       "      <td>0.686111</td>\n",
       "      <td>0.656045</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.755983</td>\n",
       "      <td>0.785861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>answer_correctness</td>\n",
       "      <td>0.748569</td>\n",
       "      <td>0.642640</td>\n",
       "      <td>0.677619</td>\n",
       "      <td>0.658513</td>\n",
       "      <td>0.728547</td>\n",
       "      <td>0.597534</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Metric     Naive      BM25    MultiQ    Parent  Ensemble  \\\n",
       "0        faithfulness  0.860417  0.670694  0.786845  0.810471  0.874123   \n",
       "1    answer_relevancy  0.917881  0.820600  0.828396  0.914089  0.874294   \n",
       "2      context_recall  0.975000  0.795833  0.975000  0.875000  0.975000   \n",
       "3   context_precision  0.760535  0.686111  0.656045  0.833333  0.755983   \n",
       "4  answer_correctness  0.748569  0.642640  0.677619  0.658513  0.728547   \n",
       "\n",
       "   Semantic  \n",
       "0  0.859524  \n",
       "1  0.790533  \n",
       "2  0.875000  \n",
       "3  0.785861  \n",
       "4  0.597534  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_naive = pd.DataFrame(list(naive_results.items()), columns=['Metric', 'Naive'])\n",
    "df_bm25 = pd.DataFrame(list(bm25_results.items()), columns=['Metric', 'BM25'])\n",
    "df_multiQ = pd.DataFrame(list(multi_query_results.items()), columns=['Metric', 'MultiQ'])\n",
    "df_parent = pd.DataFrame(list(parent_results.items()), columns=['Metric', 'Parent'])\n",
    "df_ensemble = pd.DataFrame(list(ensemble_results.items()), columns=['Metric', 'Ensemble'])\n",
    "df_semantic = pd.DataFrame(list(semantic_results.items()), columns=['Metric', 'Semantic'])\n",
    "df_merged = df_naive.merge(df_bm25, on='Metric').merge(df_multiQ, on='Metric').merge(df_parent, on='Metric')\n",
    "df_merged = df_merged.merge(df_ensemble, on='Metric').merge(df_semantic, on='Metric')\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
